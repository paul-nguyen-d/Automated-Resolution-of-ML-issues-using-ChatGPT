,Project Name,Issue URL,PR number that fix the issue,Body,Solution SHA,Before solution SHA,Files changed
0,aiqm/torchani,https://github.com/aiqm/torchani/issues/499,500,"Hello,
torchani.models.ANI2x() downloads and unzips parameters into site-packages/torchani/resources, but this means that you cannot cleanly pip3 uninstall torchani, since site-packages/torchani is then left on disk. As a consequence, after ""uninstalling"" torchani, you can still import torchani without error but not use it for anything.
It would be quite nice if one could specify a path from which to read the parameters (and download parameters to if the path does not exist), e.g. torchani.models.ANI2x(parameter_path='/my/preferred_path/ani2x').",267f4bc05dba30d38fb6bcddab4631fc742809e7,42442af67486a6d06f31c2c5f81cae4f9ee3d0bb,"['.gitignore', 'torchani/models.py', 'torchani/resources/ani-1ccx_8x.info', 'torchani/resources/ani-1ccx_8x/rHCNO-5.2R_16-3.5A_a4-8.params', 'torchani/resources/ani-1ccx_8x/sae_linfit.dat', 'torchani/resources/ani-1ccx_8x/train', 'torchani/resources/ani-1ccx_8x/train0/cost.dat', 'torchani/resources/ani-1ccx_8x/train0/inputtrain.ipt', 'torchani/resources/ani-1ccx_8x/train0/networks/ANN-C.nnf', 'torchani/resources/ani-1ccx_8x/train0/networks/ANN-C.nnf-l1.bparam', 'torchani/resources/ani-1ccx_8x/train0/networks/ANN-C.nnf-l1.wparam', 'torchani/resources/ani-1ccx_8x/train0/networks/ANN-C.nnf-l2.bparam', 'torchani/resources/ani-1ccx_8x/train0/networks/ANN-C.nnf-l2.wparam', 'torchani/resources/ani-1ccx_8x/train0/networks/ANN-C.nnf-l3.bparam', 'torchani/resources/ani-1ccx_8x/train0/networks/ANN-C.nnf-l3.wparam', 'torchani/resources/ani-1ccx_8x/train0/networks/ANN-C.nnf-l4.bparam', 'torchani/resources/ani-1ccx_8x/train0/networks/ANN-C.nnf-l4.wparam', 'torchani/resources/ani-1ccx_8x/train0/networks/ANN-H.nnf', 'torchani/resources/ani-1ccx_8x/train0/networks/ANN-H.nnf-l1.bparam', 'torchani/resources/ani-1ccx_8x/train0/networks/ANN-H.nnf-l1.wparam', 'torchani/resources/ani-1ccx_8x/train0/networks/ANN-H.nnf-l2.bparam', 'torchani/resources/ani-1ccx_8x/train0/networks/ANN-H.nnf-l2.wparam', 'torchani/resources/ani-1ccx_8x/train0/networks/ANN-H.nnf-l3.bparam', 'torchani/resources/ani-1ccx_8x/train0/networks/ANN-H.nnf-l3.wparam', 'torchani/resources/ani-1ccx_8x/train0/networks/ANN-H.nnf-l4.bparam', 'torchani/resources/ani-1ccx_8x/train0/networks/ANN-H.nnf-l4.wparam', 'torchani/resources/ani-1ccx_8x/train0/networks/ANN-N.nnf', 'torchani/resources/ani-1ccx_8x/train0/networks/ANN-N.nnf-l1.bparam', 'torchani/resources/ani-1ccx_8x/train0/networks/ANN-N.nnf-l1.wparam', 'torchani/resources/ani-1ccx_8x/train0/networks/ANN-N.nnf-l2.bparam', 'torchani/resources/ani-1ccx_8x/train0/networks/ANN-N.nnf-l2.wparam', 'torchani/resources/ani-1ccx_8x/train0/networks/ANN-N.nnf-l3.bparam', 'torchani/resources/ani-1ccx_8x/train0/networks/ANN-N.nnf-l3.wparam', 'torchani/resources/ani-1ccx_8x/train0/networks/ANN-N.nnf-l4.bparam', 'torchani/resources/ani-1ccx_8x/train0/networks/ANN-N.nnf-l4.wparam', 'torchani/resources/ani-1ccx_8x/train0/networks/ANN-O.nnf', 'torchani/resources/ani-1ccx_8x/train0/networks/ANN-O.nnf-l1.bparam', 'torchani/resources/ani-1ccx_8x/train0/networks/ANN-O.nnf-l1.wparam', 'torchani/resources/ani-1ccx_8x/train0/networks/ANN-O.nnf-l2.bparam', 'torchani/resources/ani-1ccx_8x/train0/networks/ANN-O.nnf-l2.wparam', 'torchani/resources/ani-1ccx_8x/train0/networks/ANN-O.nnf-l3.bparam', 'torchani/resources/ani-1ccx_8x/train0/networks/ANN-O.nnf-l3.wparam', 'torchani/resources/ani-1ccx_8x/train0/networks/ANN-O.nnf-l4.bparam', 'torchani/resources/ani-1ccx_8x/train0/networks/ANN-O.nnf-l4.wparam', 'torchani/resources/ani-1ccx_8x/train0/output.opt', 'torchani/resources/ani-1ccx_8x/train0/rHCNO-5.2R_16-3.5A_a4-8.params', 'torchani/resources/ani-1ccx_8x/train0/sae_linfit.dat', 'torchani/resources/ani-1ccx_8x/train0/tolerance.dat', 'torchani/resources/ani-1ccx_8x/train1/cost.dat', 'torchani/resources/ani-1ccx_8x/train1/inputtrain.ipt', 'torchani/resources/ani-1ccx_8x/train1/networks/ANN-C.nnf', 'torchani/resources/ani-1ccx_8x/train1/networks/ANN-C.nnf-l1.bparam', 'torchani/resources/ani-1ccx_8x/train1/networks/ANN-C.nnf-l1.wparam', 'torchani/resources/ani-1ccx_8x/train1/networks/ANN-C.nnf-l2.bparam', 'torchani/resources/ani-1ccx_8x/train1/networks/ANN-C.nnf-l2.wparam', 'torchani/resources/ani-1ccx_8x/train1/networks/ANN-C.nnf-l3.bparam', 'torchani/resources/ani-1ccx_8x/train1/networks/ANN-C.nnf-l3.wparam', 'torchani/resources/ani-1ccx_8x/train1/networks/ANN-C.nnf-l4.bparam', 'torchani/resources/ani-1ccx_8x/train1/networks/ANN-C.nnf-l4.wparam', 'torchani/resources/ani-1ccx_8x/train1/networks/ANN-H.nnf', 'torchani/resources/ani-1ccx_8x/train1/networks/ANN-H.nnf-l1.bparam', 'torchani/resources/ani-1ccx_8x/train1/networks/ANN-H.nnf-l1.wparam', 'torchani/resources/ani-1ccx_8x/train1/networks/ANN-H.nnf-l2.bparam', 'torchani/resources/ani-1ccx_8x/train1/networks/ANN-H.nnf-l2.wparam', 'torchani/resources/ani-1ccx_8x/train1/networks/ANN-H.nnf-l3.bparam', 'torchani/resources/ani-1ccx_8x/train1/networks/ANN-H.nnf-l3.wparam', 'torchani/resources/ani-1ccx_8x/train1/networks/ANN-H.nnf-l4.bparam', 'torchani/resources/ani-1ccx_8x/train1/networks/ANN-H.nnf-l4.wparam', 'torchani/resources/ani-1ccx_8x/train1/networks/ANN-N.nnf', 'torchani/resources/ani-1ccx_8x/train1/networks/ANN-N.nnf-l1.bparam', 'torchani/resources/ani-1ccx_8x/train1/networks/ANN-N.nnf-l1.wparam', 'torchani/resources/ani-1ccx_8x/train1/networks/ANN-N.nnf-l2.bparam', 'torchani/resources/ani-1ccx_8x/train1/networks/ANN-N.nnf-l2.wparam', 'torchani/resources/ani-1ccx_8x/train1/networks/ANN-N.nnf-l3.bparam', 'torchani/resources/ani-1ccx_8x/train1/networks/ANN-N.nnf-l3.wparam', 'torchani/resources/ani-1ccx_8x/train1/networks/ANN-N.nnf-l4.bparam', 'torchani/resources/ani-1ccx_8x/train1/networks/ANN-N.nnf-l4.wparam', 'torchani/resources/ani-1ccx_8x/train1/networks/ANN-O.nnf', 'torchani/resources/ani-1ccx_8x/train1/networks/ANN-O.nnf-l1.bparam', 'torchani/resources/ani-1ccx_8x/train1/networks/ANN-O.nnf-l1.wparam', 'torchani/resources/ani-1ccx_8x/train1/networks/ANN-O.nnf-l2.bparam', 'torchani/resources/ani-1ccx_8x/train1/networks/ANN-O.nnf-l2.wparam', 'torchani/resources/ani-1ccx_8x/train1/networks/ANN-O.nnf-l3.bparam', 'torchani/resources/ani-1ccx_8x/train1/networks/ANN-O.nnf-l3.wparam', 'torchani/resources/ani-1ccx_8x/train1/networks/ANN-O.nnf-l4.bparam', 'torchani/resources/ani-1ccx_8x/train1/networks/ANN-O.nnf-l4.wparam', 'torchani/resources/ani-1ccx_8x/train1/output.opt', 'torchani/resources/ani-1ccx_8x/train1/rHCNO-5.2R_16-3.5A_a4-8.params', 'torchani/resources/ani-1ccx_8x/train1/sae_linfit.dat', 'torchani/resources/ani-1ccx_8x/train1/tolerance.dat', 'torchani/resources/ani-1ccx_8x/train2/cost.dat', 'torchani/resources/ani-1ccx_8x/train2/inputtrain.ipt', 'torchani/resources/ani-1ccx_8x/train2/networks/ANN-C.nnf', 'torchani/resources/ani-1ccx_8x/train2/networks/ANN-C.nnf-l1.bparam', 'torchani/resources/ani-1ccx_8x/train2/networks/ANN-C.nnf-l1.wparam', 'torchani/resources/ani-1ccx_8x/train2/networks/ANN-C.nnf-l2.bparam', 'torchani/resources/ani-1ccx_8x/train2/networks/ANN-C.nnf-l2.wparam', 'torchani/resources/ani-1ccx_8x/train2/networks/ANN-C.nnf-l3.bparam', 'torchani/resources/ani-1ccx_8x/train2/networks/ANN-C.nnf-l3.wparam', 'torchani/resources/ani-1ccx_8x/train2/networks/ANN-C.nnf-l4.bparam', 'torchani/resources/ani-1ccx_8x/train2/networks/ANN-C.nnf-l4.wparam', 'torchani/resources/ani-1ccx_8x/train2/networks/ANN-H.nnf', 'torchani/resources/ani-1ccx_8x/train2/networks/ANN-H.nnf-l1.bparam', 'torchani/resources/ani-1ccx_8x/train2/networks/ANN-H.nnf-l1.wparam', 'torchani/resources/ani-1ccx_8x/train2/networks/ANN-H.nnf-l2.bparam', 'torchani/resources/ani-1ccx_8x/train2/networks/ANN-H.nnf-l2.wparam', 'torchani/resources/ani-1ccx_8x/train2/networks/ANN-H.nnf-l3.bparam', 'torchani/resources/ani-1ccx_8x/train2/networks/ANN-H.nnf-l3.wparam', 'torchani/resources/ani-1ccx_8x/train2/networks/ANN-H.nnf-l4.bparam', 'torchani/resources/ani-1ccx_8x/train2/networks/ANN-H.nnf-l4.wparam', 'torchani/resources/ani-1ccx_8x/train2/networks/ANN-N.nnf', 'torchani/resources/ani-1ccx_8x/train2/networks/ANN-N.nnf-l1.bparam', 'torchani/resources/ani-1ccx_8x/train2/networks/ANN-N.nnf-l1.wparam', 'torchani/resources/ani-1ccx_8x/train2/networks/ANN-N.nnf-l2.bparam', 'torchani/resources/ani-1ccx_8x/train2/networks/ANN-N.nnf-l2.wparam', 'torchani/resources/ani-1ccx_8x/train2/networks/ANN-N.nnf-l3.bparam', 'torchani/resources/ani-1ccx_8x/train2/networks/ANN-N.nnf-l3.wparam', 'torchani/resources/ani-1ccx_8x/train2/networks/ANN-N.nnf-l4.bparam', 'torchani/resources/ani-1ccx_8x/train2/networks/ANN-N.nnf-l4.wparam', 'torchani/resources/ani-1ccx_8x/train2/networks/ANN-O.nnf', 'torchani/resources/ani-1ccx_8x/train2/networks/ANN-O.nnf-l1.bparam', 'torchani/resources/ani-1ccx_8x/train2/networks/ANN-O.nnf-l1.wparam', 'torchani/resources/ani-1ccx_8x/train2/networks/ANN-O.nnf-l2.bparam', 'torchani/resources/ani-1ccx_8x/train2/networks/ANN-O.nnf-l2.wparam', 'torchani/resources/ani-1ccx_8x/train2/networks/ANN-O.nnf-l3.bparam', 'torchani/resources/ani-1ccx_8x/train2/networks/ANN-O.nnf-l3.wparam', 'torchani/resources/ani-1ccx_8x/train2/networks/ANN-O.nnf-l4.bparam', 'torchani/resources/ani-1ccx_8x/train2/networks/ANN-O.nnf-l4.wparam', 'torchani/resources/ani-1ccx_8x/train2/output.opt', 'torchani/resources/ani-1ccx_8x/train2/rHCNO-5.2R_16-3.5A_a4-8.params', 'torchani/resources/ani-1ccx_8x/train2/sae_linfit.dat', 'torchani/resources/ani-1ccx_8x/train2/tolerance.dat', 'torchani/resources/ani-1ccx_8x/train3/cost.dat', 'torchani/resources/ani-1ccx_8x/train3/inputtrain.ipt', 'torchani/resources/ani-1ccx_8x/train3/networks/ANN-C.nnf', 'torchani/resources/ani-1ccx_8x/train3/networks/ANN-C.nnf-l1.bparam', 'torchani/resources/ani-1ccx_8x/train3/networks/ANN-C.nnf-l1.wparam', 'torchani/resources/ani-1ccx_8x/train3/networks/ANN-C.nnf-l2.bparam', 'torchani/resources/ani-1ccx_8x/train3/networks/ANN-C.nnf-l2.wparam', 'torchani/resources/ani-1ccx_8x/train3/networks/ANN-C.nnf-l3.bparam', 'torchani/resources/ani-1ccx_8x/train3/networks/ANN-C.nnf-l3.wparam', 'torchani/resources/ani-1ccx_8x/train3/networks/ANN-C.nnf-l4.bparam', 'torchani/resources/ani-1ccx_8x/train3/networks/ANN-C.nnf-l4.wparam', 'torchani/resources/ani-1ccx_8x/train3/networks/ANN-H.nnf', 'torchani/resources/ani-1ccx_8x/train3/networks/ANN-H.nnf-l1.bparam', 'torchani/resources/ani-1ccx_8x/train3/networks/ANN-H.nnf-l1.wparam', 'torchani/resources/ani-1ccx_8x/train3/networks/ANN-H.nnf-l2.bparam', 'torchani/resources/ani-1ccx_8x/train3/networks/ANN-H.nnf-l2.wparam', 'torchani/resources/ani-1ccx_8x/train3/networks/ANN-H.nnf-l3.bparam', 'torchani/resources/ani-1ccx_8x/train3/networks/ANN-H.nnf-l3.wparam', 'torchani/resources/ani-1ccx_8x/train3/networks/ANN-H.nnf-l4.bparam', 'torchani/resources/ani-1ccx_8x/train3/networks/ANN-H.nnf-l4.wparam', 'torchani/resources/ani-1ccx_8x/train3/networks/ANN-N.nnf', 'torchani/resources/ani-1ccx_8x/train3/networks/ANN-N.nnf-l1.bparam', 'torchani/resources/ani-1ccx_8x/train3/networks/ANN-N.nnf-l1.wparam', 'torchani/resources/ani-1ccx_8x/train3/networks/ANN-N.nnf-l2.bparam', 'torchani/resources/ani-1ccx_8x/train3/networks/ANN-N.nnf-l2.wparam', 'torchani/resources/ani-1ccx_8x/train3/networks/ANN-N.nnf-l3.bparam', 'torchani/resources/ani-1ccx_8x/train3/networks/ANN-N.nnf-l3.wparam', 'torchani/resources/ani-1ccx_8x/train3/networks/ANN-N.nnf-l4.bparam', 'torchani/resources/ani-1ccx_8x/train3/networks/ANN-N.nnf-l4.wparam', 'torchani/resources/ani-1ccx_8x/train3/networks/ANN-O.nnf', 'torchani/resources/ani-1ccx_8x/train3/networks/ANN-O.nnf-l1.bparam', 'torchani/resources/ani-1ccx_8x/train3/networks/ANN-O.nnf-l1.wparam', 'torchani/resources/ani-1ccx_8x/train3/networks/ANN-O.nnf-l2.bparam', 'torchani/resources/ani-1ccx_8x/train3/networks/ANN-O.nnf-l2.wparam', 'torchani/resources/ani-1ccx_8x/train3/networks/ANN-O.nnf-l3.bparam', 'torchani/resources/ani-1ccx_8x/train3/networks/ANN-O.nnf-l3.wparam', 'torchani/resources/ani-1ccx_8x/train3/networks/ANN-O.nnf-l4.bparam', 'torchani/resources/ani-1ccx_8x/train3/networks/ANN-O.nnf-l4.wparam', 'torchani/resources/ani-1ccx_8x/train3/output.opt', 'torchani/resources/ani-1ccx_8x/train3/rHCNO-5.2R_16-3.5A_a4-8.params', 'torchani/resources/ani-1ccx_8x/train3/sae_linfit.dat', 'torchani/resources/ani-1ccx_8x/train3/tolerance.dat', 'torchani/resources/ani-1ccx_8x/train4/cost.dat', 'torchani/resources/ani-1ccx_8x/train4/inputtrain.ipt', 'torchani/resources/ani-1ccx_8x/train4/networks/ANN-C.nnf', 'torchani/resources/ani-1ccx_8x/train4/networks/ANN-C.nnf-l1.bparam', 'torchani/resources/ani-1ccx_8x/train4/networks/ANN-C.nnf-l1.wparam', 'torchani/resources/ani-1ccx_8x/train4/networks/ANN-C.nnf-l2.bparam', 'torchani/resources/ani-1ccx_8x/train4/networks/ANN-C.nnf-l2.wparam', 'torchani/resources/ani-1ccx_8x/train4/networks/ANN-C.nnf-l3.bparam', 'torchani/resources/ani-1ccx_8x/train4/networks/ANN-C.nnf-l3.wparam', 'torchani/resources/ani-1ccx_8x/train4/networks/ANN-C.nnf-l4.bparam', 'torchani/resources/ani-1ccx_8x/train4/networks/ANN-C.nnf-l4.wparam', 'torchani/resources/ani-1ccx_8x/train4/networks/ANN-H.nnf', 'torchani/resources/ani-1ccx_8x/train4/networks/ANN-H.nnf-l1.bparam', 'torchani/resources/ani-1ccx_8x/train4/networks/ANN-H.nnf-l1.wparam', 'torchani/resources/ani-1ccx_8x/train4/networks/ANN-H.nnf-l2.bparam', 'torchani/resources/ani-1ccx_8x/train4/networks/ANN-H.nnf-l2.wparam', 'torchani/resources/ani-1ccx_8x/train4/networks/ANN-H.nnf-l3.bparam', 'torchani/resources/ani-1ccx_8x/train4/networks/ANN-H.nnf-l3.wparam', 'torchani/resources/ani-1ccx_8x/train4/networks/ANN-H.nnf-l4.bparam', 'torchani/resources/ani-1ccx_8x/train4/networks/ANN-H.nnf-l4.wparam', 'torchani/resources/ani-1ccx_8x/train4/networks/ANN-N.nnf', 'torchani/resources/ani-1ccx_8x/train4/networks/ANN-N.nnf-l1.bparam', 'torchani/resources/ani-1ccx_8x/train4/networks/ANN-N.nnf-l1.wparam', 'torchani/resources/ani-1ccx_8x/train4/networks/ANN-N.nnf-l2.bparam', 'torchani/resources/ani-1ccx_8x/train4/networks/ANN-N.nnf-l2.wparam', 'torchani/resources/ani-1ccx_8x/train4/networks/ANN-N.nnf-l3.bparam', 'torchani/resources/ani-1ccx_8x/train4/networks/ANN-N.nnf-l3.wparam', 'torchani/resources/ani-1ccx_8x/train4/networks/ANN-N.nnf-l4.bparam', 'torchani/resources/ani-1ccx_8x/train4/networks/ANN-N.nnf-l4.wparam', 'torchani/resources/ani-1ccx_8x/train4/networks/ANN-O.nnf', 'torchani/resources/ani-1ccx_8x/train4/networks/ANN-O.nnf-l1.bparam', 'torchani/resources/ani-1ccx_8x/train4/networks/ANN-O.nnf-l1.wparam', 'torchani/resources/ani-1ccx_8x/train4/networks/ANN-O.nnf-l2.bparam', 'torchani/resources/ani-1ccx_8x/train4/networks/ANN-O.nnf-l2.wparam', 'torchani/resources/ani-1ccx_8x/train4/networks/ANN-O.nnf-l3.bparam', 'torchani/resources/ani-1ccx_8x/train4/networks/ANN-O.nnf-l3.wparam', 'torchani/resources/ani-1ccx_8x/train4/networks/ANN-O.nnf-l4.bparam', 'torchani/resources/ani-1ccx_8x/train4/networks/ANN-O.nnf-l4.wparam', 'torchani/resources/ani-1ccx_8x/train4/output.opt', 'torchani/resources/ani-1ccx_8x/train4/rHCNO-5.2R_16-3.5A_a4-8.params', 'torchani/resources/ani-1ccx_8x/train4/sae_linfit.dat', 'torchani/resources/ani-1ccx_8x/train4/tolerance.dat', 'torchani/resources/ani-1ccx_8x/train5/cost.dat', 'torchani/resources/ani-1ccx_8x/train5/inputtrain.ipt', 'torchani/resources/ani-1ccx_8x/train5/networks/ANN-C.nnf', 'torchani/resources/ani-1ccx_8x/train5/networks/ANN-C.nnf-l1.bparam', 'torchani/resources/ani-1ccx_8x/train5/networks/ANN-C.nnf-l1.wparam', 'torchani/resources/ani-1ccx_8x/train5/networks/ANN-C.nnf-l2.bparam', 'torchani/resources/ani-1ccx_8x/train5/networks/ANN-C.nnf-l2.wparam', 'torchani/resources/ani-1ccx_8x/train5/networks/ANN-C.nnf-l3.bparam', 'torchani/resources/ani-1ccx_8x/train5/networks/ANN-C.nnf-l3.wparam', 'torchani/resources/ani-1ccx_8x/train5/networks/ANN-C.nnf-l4.bparam', 'torchani/resources/ani-1ccx_8x/train5/networks/ANN-C.nnf-l4.wparam', 'torchani/resources/ani-1ccx_8x/train5/networks/ANN-H.nnf', 'torchani/resources/ani-1ccx_8x/train5/networks/ANN-H.nnf-l1.bparam', 'torchani/resources/ani-1ccx_8x/train5/networks/ANN-H.nnf-l1.wparam', 'torchani/resources/ani-1ccx_8x/train5/networks/ANN-H.nnf-l2.bparam', 'torchani/resources/ani-1ccx_8x/train5/networks/ANN-H.nnf-l2.wparam', 'torchani/resources/ani-1ccx_8x/train5/networks/ANN-H.nnf-l3.bparam', 'torchani/resources/ani-1ccx_8x/train5/networks/ANN-H.nnf-l3.wparam', 'torchani/resources/ani-1ccx_8x/train5/networks/ANN-H.nnf-l4.bparam', 'torchani/resources/ani-1ccx_8x/train5/networks/ANN-H.nnf-l4.wparam', 'torchani/resources/ani-1ccx_8x/train5/networks/ANN-N.nnf', 'torchani/resources/ani-1ccx_8x/train5/networks/ANN-N.nnf-l1.bparam', 'torchani/resources/ani-1ccx_8x/train5/networks/ANN-N.nnf-l1.wparam', 'torchani/resources/ani-1ccx_8x/train5/networks/ANN-N.nnf-l2.bparam', 'torchani/resources/ani-1ccx_8x/train5/networks/ANN-N.nnf-l2.wparam', 'torchani/resources/ani-1ccx_8x/train5/networks/ANN-N.nnf-l3.bparam', 'torchani/resources/ani-1ccx_8x/train5/networks/ANN-N.nnf-l3.wparam', 'torchani/resources/ani-1ccx_8x/train5/networks/ANN-N.nnf-l4.bparam', 'torchani/resources/ani-1ccx_8x/train5/networks/ANN-N.nnf-l4.wparam', 'torchani/resources/ani-1ccx_8x/train5/networks/ANN-O.nnf', 'torchani/resources/ani-1ccx_8x/train5/networks/ANN-O.nnf-l1.bparam', 'torchani/resources/ani-1ccx_8x/train5/networks/ANN-O.nnf-l1.wparam', 'torchani/resources/ani-1ccx_8x/train5/networks/ANN-O.nnf-l2.bparam', 'torchani/resources/ani-1ccx_8x/train5/networks/ANN-O.nnf-l2.wparam', 'torchani/resources/ani-1ccx_8x/train5/networks/ANN-O.nnf-l3.bparam', 'torchani/resources/ani-1ccx_8x/train5/networks/ANN-O.nnf-l3.wparam', 'torchani/resources/ani-1ccx_8x/train5/networks/ANN-O.nnf-l4.bparam', 'torchani/resources/ani-1ccx_8x/train5/networks/ANN-O.nnf-l4.wparam', 'torchani/resources/ani-1ccx_8x/train5/output.opt', 'torchani/resources/ani-1ccx_8x/train5/rHCNO-5.2R_16-3.5A_a4-8.params', 'torchani/resources/ani-1ccx_8x/train5/sae_linfit.dat', 'torchani/resources/ani-1ccx_8x/train5/tolerance.dat', 'torchani/resources/ani-1ccx_8x/train6/cost.dat', 'torchani/resources/ani-1ccx_8x/train6/inputtrain.ipt', 'torchani/resources/ani-1ccx_8x/train6/networks/ANN-C.nnf', 'torchani/resources/ani-1ccx_8x/train6/networks/ANN-C.nnf-l1.bparam', 'torchani/resources/ani-1ccx_8x/train6/networks/ANN-C.nnf-l1.wparam', 'torchani/resources/ani-1ccx_8x/train6/networks/ANN-C.nnf-l2.bparam', 'torchani/resources/ani-1ccx_8x/train6/networks/ANN-C.nnf-l2.wparam', 'torchani/resources/ani-1ccx_8x/train6/networks/ANN-C.nnf-l3.bparam', 'torchani/resources/ani-1ccx_8x/train6/networks/ANN-C.nnf-l3.wparam', 'torchani/resources/ani-1ccx_8x/train6/networks/ANN-C.nnf-l4.bparam', 'torchani/resources/ani-1ccx_8x/train6/networks/ANN-C.nnf-l4.wparam', 'torchani/resources/ani-1ccx_8x/train6/networks/ANN-H.nnf', 'torchani/resources/ani-1ccx_8x/train6/networks/ANN-H.nnf-l1.bparam', 'torchani/resources/ani-1ccx_8x/train6/networks/ANN-H.nnf-l1.wparam', 'torchani/resources/ani-1ccx_8x/train6/networks/ANN-H.nnf-l2.bparam', 'torchani/resources/ani-1ccx_8x/train6/networks/ANN-H.nnf-l2.wparam', 'torchani/resources/ani-1ccx_8x/train6/networks/ANN-H.nnf-l3.bparam', 'torchani/resources/ani-1ccx_8x/train6/networks/ANN-H.nnf-l3.wparam', 'torchani/resources/ani-1ccx_8x/train6/networks/ANN-H.nnf-l4.bparam', 'torchani/resources/ani-1ccx_8x/train6/networks/ANN-H.nnf-l4.wparam', 'torchani/resources/ani-1ccx_8x/train6/networks/ANN-N.nnf', 'torchani/resources/ani-1ccx_8x/train6/networks/ANN-N.nnf-l1.bparam', 'torchani/resources/ani-1ccx_8x/train6/networks/ANN-N.nnf-l1.wparam', 'torchani/resources/ani-1ccx_8x/train6/networks/ANN-N.nnf-l2.bparam', 'torchani/resources/ani-1ccx_8x/train6/networks/ANN-N.nnf-l2.wparam', 'torchani/resources/ani-1ccx_8x/train6/networks/ANN-N.nnf-l3.bparam', 'torchani/resources/ani-1ccx_8x/train6/networks/ANN-N.nnf-l3.wparam', 'torchani/resources/ani-1ccx_8x/train6/networks/ANN-N.nnf-l4.bparam', 'torchani/resources/ani-1ccx_8x/train6/networks/ANN-N.nnf-l4.wparam', 'torchani/resources/ani-1ccx_8x/train6/networks/ANN-O.nnf', 'torchani/resources/ani-1ccx_8x/train6/networks/ANN-O.nnf-l1.bparam', 'torchani/resources/ani-1ccx_8x/train6/networks/ANN-O.nnf-l1.wparam', 'torchani/resources/ani-1ccx_8x/train6/networks/ANN-O.nnf-l2.bparam', 'torchani/resources/ani-1ccx_8x/train6/networks/ANN-O.nnf-l2.wparam', 'torchani/resources/ani-1ccx_8x/train6/networks/ANN-O.nnf-l3.bparam', 'torchani/resources/ani-1ccx_8x/train6/networks/ANN-O.nnf-l3.wparam', 'torchani/resources/ani-1ccx_8x/train6/networks/ANN-O.nnf-l4.bparam', 'torchani/resources/ani-1ccx_8x/train6/networks/ANN-O.nnf-l4.wparam', 'torchani/resources/ani-1ccx_8x/train6/output.opt', 'torchani/resources/ani-1ccx_8x/train6/rHCNO-5.2R_16-3.5A_a4-8.params', 'torchani/resources/ani-1ccx_8x/train6/sae_linfit.dat', 'torchani/resources/ani-1ccx_8x/train6/tolerance.dat']"
1,aiqm/torchani,https://github.com/aiqm/torchani/issues/511,512,"TorchANI is not raising an error during the inference of systems with unknown species and basically ignores those species. As an example, ANI2x supports H C N O S F CL but for a molecule that has Zn, it won't complain!",31bf913d1c5d6b0c0f40d960bf4c834c176703ef,5fbd9edd7d73104a75e5a124f48bd3f69ade87ed,"['torchani/aev.py', 'torchani/models.py', 'torchani/nn.py']"
2,DeepLabCut/DeepLabCut,https://github.com/DeepLabCut/DeepLabCut/issues/1792,1911,"Is your feature request related to a problem? Please describe.
When I try to run inference of a folder of videos, they get analyzed in a random order. This is because it seems that they get shuffled during the collection/discovery step.
However, when you look at your folder, the results start to appear in a random order. I would like to look down my folder list in my file explorer and see the videos get analyzed in some kind of order.
Describe the solution you'd like
I would like a call to videos.sort() to sort the list of videos found so they get
Describe alternatives you've considered
Patching deeplabcut myself, I would rather collobarate and submit the patch to you to help other users.
Attached is one non-invasive suggestion. You could patch get_list_of_videos but I feel like that might get used in training, where yo uwould want the videos to be sorted.
0001-Ensure-videos-are-sorted-during-inference-steps-to-h.txt",322ea8ab7e1b325bc3ea812fc276f89552fdc643,d80db136c704dcf6e8c7e3ab934d3610618eea9c,"['deeplabcut/pose_estimation_tensorflow/predict_videos.py', 'deeplabcut/utils/auxiliaryfunctions.py']"
3,DeepLabCut/DeepLabCut,https://github.com/DeepLabCut/DeepLabCut/issues/1981,1982,"Is there an existing issue for this?

 I have searched the existing issues

Bug description
Conda environment created with numpy>=1.23 will fail with exception:
AttributeError: module 'numpy' has no attribute 'asscalar'
Reported in this StackOverflow question.
Operating System
Not my system, but StackOverflow report shows Windows. However, this is platform-agnostic issue.
DeepLabCut version
2.2.2
DeepLabCut mode
single animal
Device type
gpu
Steps To Reproduce

Follow Install instructions.
Check that numpy >= 1.23 is installed.
Run example (python testscript.py)

Relevant log output
No response
Anything else?
NumPy has had numpy.asscalar() deprecated since v1.16 and finally removed in v1.23 (see Release Notes).
Quick fix is to set upper bound in YAML environment definitions (numpy <1.23). Better would be to follow NumPy guidance and replace numpy.asscalar() calls with numpy.ndarray.item().
Code of Conduct

 I agree to follow this project's Code of Conduct",446fc5485767a54f5f4319b32be0ff7670837f9b,7099e1a2f4f64f6158f05395835f7b55fddba190,"['deeplabcut/pose_estimation_tensorflow/datasets/pose_deterministic.py', 'deeplabcut/pose_estimation_tensorflow/datasets/pose_imgaug.py', 'deeplabcut/pose_estimation_tensorflow/datasets/pose_tensorpack.py']"
4,marl/openl3,https://github.com/marl/openl3/issues/73,75,"skimage uses lazy imports, so we need to import each submodule explictly (e.g. import skimage.transform; skimage.transform.rescale(X, s) instead of import skimage; skimage.transform.rescale(X, s)).",d46bc264484fc81f28f57df78d403f8579bcf4e7,14aa684ff3d85052743fbc2ce3b81070d63c9d7c,['openl3/core.py']
5,HazyResearch/fonduer,https://github.com/HazyResearch/fonduer/issues/56,64,"Currently, Fonduer is kind of a monolitic package, where all database tables are created on init. In order to make development easier, we want to split Fonduer into sobmodules, each of which a single task in the pipeline:

parser
candidates
featurization
supervision
learning
utils

Database tables will only be created in the initialization of each of these independent modules.
TODO:

 Reorganize files as is, just fixing imports and setting up new directories
 Use all absolute imports
 Split up the initialization performed by Meta into the respective pipeline phases
 Define each phase's API. Expose submodules, rather than all functions, in the root fonduer package
 Simplify the source files where possible",1cb59f256832b7274d111f98953503778ca03d6b,d3bc6cc193996a713fa195b97400ecbe47af3db4,"['.travis.yml', 'CHANGELOG.rst', 'Makefile', 'fonduer/__init__.py', 'fonduer/parser/__init__.py', 'fonduer/parser/doc_preprocessors.py', 'fonduer/parser/models/context.py', 'fonduer/parser/models/table.py', 'fonduer/parser/parser.py', 'fonduer/parser/preprocessors/__init__.py', 'fonduer/parser/preprocessors/csv_paths_preprocessor.py', 'fonduer/parser/preprocessors/doc_preprocessor.py', 'fonduer/parser/preprocessors/html_doc_preprocessor.py', 'fonduer/parser/preprocessors/text_doc_preprocessor.py', 'fonduer/parser/preprocessors/tsv_doc_preprocessor.py', 'fonduer/parser/preprocessors/xml_multidoc_preprocessor.py', 'fonduer/parser/rule_parser.py', 'fonduer/parser/spacy_parser.py', 'requirements-dev.txt', 'tests/e2e/test_e2e.py', 'tests/parser/test_parser.py']"
6,stared/livelossplot,https://github.com/stared/livelossplot/issues/75,77,"When using Tensorboard logging, it will try to create a directory with time_str = datetime.now().isoformat()[:-7].replace(""T"", "" "").
However, the isoformat includes the "":"" character which cannot be used in a file path.
The line should be updated to replace a blacklist of characters. This may vary from one OS to another, but on Windows 10, it's not possible to use Tensorboard logging without tweaking the time_str variable.",8570823e11ad33eae4b294f0de63ac6e5d80c040,bf594af275b87a3bfdb0dc7380207abc2b75f2ba,['livelossplot/tensorboard.py']
7,mozilla/bugbug,https://github.com/mozilla/bugbug/issues/18,449,"Needed for mozilla/relman-auto-nag#221.
To do this, we'll need to collect some labels.
We can automatically create positive labels by getting bugs that have the regression keyword and the regression-wanted keyword.
We can't automatically create negative labels because we can't be sure that the regression range is available, when we have the regression keyword but no regression-wanted.",24c805e64ef1d99743a6f5d418e8a09b4412ece0,33f6ae83ce3c22a1a1a8fbaa3880cb838b228a3e,"['bugbug/models/__init__.py', 'bugbug/models/regressionrange.py', 'run.py']"
8,brendanhasz/probflow,https://github.com/brendanhasz/probflow/issues/21,39,"Currently ProbFlow uses just a single MC sample from variational posteriors per batch.  Fitting will be much more stable if we can use more.  In fact I'm pretty sure it's impossible to use mixture distributions as variational posteriors with just 1 MC sample...?
This'll require some expand_dims-ing of the input tensors/numpy arrays/pandas d... Hmm won't work with pandas dataframes ðŸ¤” .
Also means user slicing code in __call__ methods could cause problems.... Maybe just have the default be 1 and tell users to handle it if they want >1 (but handle it in applications and modules like Dense / DenseNetwork).

 add ability to use >1 MC sample ber batch (n_mc_samples kwarg to Model.fit?)
 tests
 update applications and modules to be compatible w/ >1 MC sample/batch
 tests
 add section to user guide about it",1e69413c53143ca22105d79bd0ca0241c1633dc5,9cddfeedc60abafe2a7f090dd80031148f8a363c,"['.bumpversion.cfg', 'docs/user_guide/fitting.rst', 'setup.py', 'src/probflow/__init__.py', 'src/probflow/models/categorical_model.py', 'src/probflow/models/continuous_model.py', 'src/probflow/models/model.py', 'src/probflow/modules/dense.py', 'tests/unit/pytorch/applications/test_dense_classifier.py', 'tests/unit/pytorch/applications/test_dense_regression.py', 'tests/unit/pytorch/applications/test_linear_regression.py', 'tests/unit/pytorch/applications/test_logistic_regression.py', 'tests/unit/pytorch/applications/test_poisson_regression.py', 'tests/unit/pytorch/models/test_model.py', 'tests/unit/tensorflow/applications/test_dense_classifier.py', 'tests/unit/tensorflow/applications/test_dense_regression.py', 'tests/unit/tensorflow/applications/test_linear_regression.py', 'tests/unit/tensorflow/applications/test_logistic_regression.py', 'tests/unit/tensorflow/applications/test_poisson_regression.py', 'tests/unit/tensorflow/models/test_categorical_model.py', 'tests/unit/tensorflow/models/test_continuous_model.py', 'tests/unit/tensorflow/models/test_model.py']"
9,brendanhasz/probflow,https://github.com/brendanhasz/probflow/issues/10,53,"Currently you can't change the learning rate mid-training using ProbFlow w/ the PyTorch backend.  But could implement that pretty easily using torch.optim.lr_scheduler.LambdaLR.
The only trick would be that LambdaLR takes a function which should return a multiplicative factor, not the actual LR value.  So would need to pass it the desired LR divided by the original LR.",fc67721b9611e662bf298b951dd2a2a9da51f8cb,16c2a57820366452bc499127d5c264366da95682,"['src/probflow/models/model.py', 'tests/unit/pytorch/callbacks/test_learning_rate_scheduler.py']"
10,brendanhasz/probflow,https://github.com/brendanhasz/probflow/issues/28,29,"doing a:
import probflow as pf

raises:
---------------------------------------------------------------------------
ModuleNotFoundError                       Traceback (most recent call last)
<ipython-input-7-aa87442b1141> in <module>
      1 import torch
----> 2 import probflow as pf

~/github/probflow/src/probflow/__init__.py in <module>
----> 1 from probflow.applications import *
      2 from probflow.callbacks import *
      3 from probflow.data import *
      4 from probflow.distributions import *
      5 from probflow.models import *

~/github/probflow/src/probflow/applications/__init__.py in <module>
     22 
     23 
---> 24 from .dense_classifier import DenseClassifier
     25 from .dense_regression import DenseRegression
     26 from .linear_regression import LinearRegression

~/github/probflow/src/probflow/applications/dense_classifier.py in <module>
      3 import probflow.utils.ops as O
      4 from probflow.distributions import Categorical
----> 5 from probflow.models import CategoricalModel
      6 from probflow.modules import DenseNetwork
      7 from probflow.utils.casting import to_tensor

~/github/probflow/src/probflow/models/__init__.py in <module>
     23 
     24 
---> 25 from .categorical_model import CategoricalModel
     26 from .continuous_model import ContinuousModel
     27 from .discrete_model import DiscreteModel

~/github/probflow/src/probflow/models/categorical_model.py in <module>
      4 from probflow.utils.plotting import plot_categorical_dist
      5 
----> 6 from .model import Model
      7 
      8 

~/github/probflow/src/probflow/models/model.py in <module>
      7 import probflow.utils.ops as O
      8 from probflow.data import make_generator
----> 9 from probflow.modules import Module
     10 from probflow.utils.base import BaseCallback
     11 from probflow.utils.casting import to_numpy

~/github/probflow/src/probflow/modules/__init__.py in <module>
     28 
     29 
---> 30 from .batch_normalization import BatchNormalization
     31 from .dense import Dense
     32 from .dense_network import DenseNetwork

~/github/probflow/src/probflow/modules/batch_normalization.py in <module>
      3 import probflow.utils.ops as O
      4 from probflow.distributions import Deterministic, Normal
----> 5 from probflow.parameters import Parameter
      6 from probflow.utils.base import BaseDistribution
      7 from probflow.utils.initializers import xavier

~/github/probflow/src/probflow/parameters/__init__.py in <module>
     51 
     52 
---> 53 from .bounded_parameter import BoundedParameter
     54 from .categorical_parameter import CategoricalParameter
     55 from .deterministic_parameter import DeterministicParameter

~/github/probflow/src/probflow/parameters/bounded_parameter.py in <module>
      4 from probflow.utils.initializers import scale_xavier, xavier
      5 
----> 6 from .parameter import Parameter
      7 
      8 

~/github/probflow/src/probflow/parameters/parameter.py in <module>
     13 
     14 
---> 15 class Parameter(BaseParameter):
     16     r""""""Probabilistic parameter(s).
     17 

~/github/probflow/src/probflow/parameters/parameter.py in Parameter()
     87         shape: Union[int, List[int]] = 1,
     88         posterior: Type[BaseDistribution] = Normal,
---> 89         prior: BaseDistribution = Normal(0, 1),
     90         transform: Callable = None,
     91         initializer: Dict[str, Callable] = {

~/github/probflow/src/probflow/distributions/normal.py in __init__(self, loc, scale)
     48 
     49         # Check input
---> 50         ensure_tensor_like(loc, ""loc"")
     51         ensure_tensor_like(scale, ""scale"")
     52 

~/github/probflow/src/probflow/utils/validation.py in ensure_tensor_like(obj, name)
     23         tensor_types = (torch.Tensor, BaseParameter)
     24     else:
---> 25         import tensorflow as tf
     26 
     27         tensor_types = (tf.Tensor, tf.Variable, BaseParameter)

ModuleNotFoundError: No module named 'tensorflow'

both from the pip version and a clone of this repo.",9da5b64e97dac1efcbccc0e0f80b75772168a99d,4f45617bd7913fe3b62510dc15762f89b543472a,"['setup.py', 'src/probflow/utils/validation.py']"
11,brendanhasz/probflow,https://github.com/brendanhasz/probflow/issues/18,31,"Add a probabilistic kwarg (True or False) to Dense, DenseNetwork, Embedding, and BatchNormalization modules.
That way you can pretty easily do, say, a non-probabilistic net with a probabilistic linear layer on top (see Snoek et al., 2015 and Riquelme et al. 2018):
class NeuralLinear(pf.ContinuousModel):

    def __init__(self, units):
        self.net = pf.DenseNetwork(units, probabilistic=False)
        self.linear = pf.Dense(units[-1], 2, probabilistic=True)

    def __call__(self, x):
        a = self.linear(tf.math.relu(self.net(x)))
        return pf.Normal(a[..., 0], tf.exp(a[..., 1]))
And, set whether you want your embeddings to be probabilistic or not (for Embedding).

 probabilistic kwarg for Dense
 tests
 probabilistic kwarg for DenseNetwork
 tests
 probabilistic kwarg for Embedding
 tests
 Add a â€œNeural Linear Modelâ€ section to the examples with the NeuralLinear example above.",56b468d37299b414943ce1e0e1ca82154554e595,be870b1db768e85ffad020c0cd9a0c515cba6b49,"['.bumpversion.cfg', 'Makefile', 'README.rst', 'docs/examples/examples.rst', 'docs/examples/img/neural_linear/calibration.svg', 'docs/examples/img/neural_linear/mae.svg', 'docs/examples/neural_linear.rst', 'docs/index.rst', 'docs/user_guide/fitting.rst', 'setup.py', 'src/probflow/__init__.py', 'src/probflow/callbacks/kl_weight_scheduler.py', 'src/probflow/callbacks/learning_rate_scheduler.py', 'src/probflow/callbacks/monitor_elbo.py', 'src/probflow/callbacks/monitor_metric.py', 'src/probflow/callbacks/time_out.py', 'src/probflow/data/array_data_generator.py', 'src/probflow/models/categorical_model.py', 'src/probflow/models/continuous_model.py', 'src/probflow/models/model.py', 'src/probflow/modules/dense.py', 'src/probflow/modules/dense_network.py', 'src/probflow/modules/embedding.py', 'src/probflow/parameters/multivariate_normal_parameter.py', 'src/probflow/parameters/parameter.py', 'src/probflow/utils/ops.py', 'tests/unit/pytorch/test_parameters_pytorch.py', 'tests/unit/tensorflow/test_callbacks.py', 'tests/unit/tensorflow/test_models.py', 'tests/unit/tensorflow/test_modules.py', 'tests/unit/tensorflow/test_parameters.py', 'tests/unit/tensorflow/test_utils_ops.py']"
12,brendanhasz/probflow,https://github.com/brendanhasz/probflow/issues/19,46,"Add a CenteredParameter, which should use a QR decomposition reparameterization for a length-N vector of parameters centered at zero using N-1 underlying variables.
Have a center_by kwarg (one of 'all', 'column', 'row') which determines how they're centered.  'all' (the default) means the sum of all elements, regardless of shape, is 0.  'column' means the sum of each column is 0, and 'row' means the sum of each row is 0.  For 'all', get a prod(shape)-length vector via the QR decomposition, then reshape into the correct shape.  For 'column' and 'row', only allow 2d shape, and can matrix multiply the A from the QR decomp by a matrix, then transpose for row. Can do that in the transform function, and have appropriate priors such that resulting parameters have prior ~ normal(0, 1).  And make sure to mention that the prior is fixed in the docs.

 Add CenteredParameter
 tests
 docs
 add section to parameters page of user guide",c9ef4800a63400277e83501d732d56934634df06,a845eec2a1300bc4778b3ddfd4a2b29e0bb51143,"['.bumpversion.cfg', 'docs/dev_guide/dev_guide.rst', 'docs/img/coverage_light.svg', 'docs/img/dual_headed_net_light.svg', 'docs/img/posteriors_light.svg', 'docs/img/pred_dist_light.svg', 'docs/index.rst', 'docs/user_guide/parameters.rst', 'setup.py', 'src/probflow/__init__.py', 'src/probflow/parameters/__init__.py', 'src/probflow/parameters/centered_parameter.py', 'src/probflow/utils/casting.py', 'src/probflow/utils/ops.py', 'tests/unit/pytorch/parameters/test_centered_parameter.py', 'tests/unit/tensorflow/parameters/test_centered_parameter.py']"
13,hackingmaterials/automatminer,https://github.com/hackingmaterials/automatminer/issues/241,246,"Saving matpipe with tpot backend saves the best pipeline under backend but not best_pipeline. The best_models are also lost. They should save in an intuitive way and throw warning on load to ensure users know the full backend is not tpot
...Alternatively, find a way to serialize TPOT backend.",101f1b850b944432b28e933964a1631fd811fc12,63dfaff40cf8578ceb3b180a4fc449a91926da63,"['.circleci/config.yml', '.circleci/config_old.yml', 'MANIFEST.in', 'README.md', 'automatminer/__init__.py', 'automatminer/automl/adaptors.py', 'automatminer/automl/base.py', 'automatminer/automl/tests/test_base.py', 'automatminer/base.py', 'automatminer/featurization/core.py', 'automatminer/featurization/sets.py', 'automatminer/pipeline.py', 'automatminer/preprocessing/core.py', 'automatminer/preprocessing/tests/test_core.py', 'automatminer/presets.py', 'automatminer/tests/test_base.py', 'automatminer/tests/test_pipeline.py', 'automatminer/utils/log.py', 'automatminer/utils/pkg.py', 'automatminer/utils/tests/test_log.py', 'automatminer/utils/tests/test_pkg.py', 'benchdev/graphnet/megnet.py', 'benchdev/tasks.py', 'benchdev/workflows.py', 'docs/_images/cv_nested.png', 'docs/_images/forum.png', 'docs/_images/logo.png', 'docs/_images/matbench_pie_charts.png', 'docs/_images/matminer_examples.png', 'docs/_sources/advanced.rst.txt', 'docs/_sources/automatminer.automl.config.rst.txt', 'docs/_sources/automatminer.automl.rst.txt', 'docs/_sources/automatminer.automl.tests.rst.txt', 'docs/_sources/automatminer.featurization.rst.txt', 'docs/_sources/automatminer.featurization.tests.rst.txt', 'docs/_sources/automatminer.preprocessing.rst.txt', 'docs/_sources/automatminer.preprocessing.tests.rst.txt', 'docs/_sources/automatminer.rst.txt', 'docs/_sources/automatminer.tests.rst.txt', 'docs/_sources/automatminer.utils.rst.txt', 'docs/_sources/automatminer.utils.tests.rst.txt', 'docs/_sources/basic.rst.txt', 'docs/_sources/datasets.rst.txt', 'docs/_sources/index.rst.txt', 'docs/_sources/modules.rst.txt', 'docs/_sources/tutorials.rst.txt', 'docs/_static/alabaster.css', 'docs/_static/cv_nested.png', 'docs/_static/documentation_options.js', 'docs/_static/forum.png', 'docs/_static/logo.png', 'docs/_static/logo.svg', 'docs/_static/logo_header.png', 'docs/_static/matbench_pie_charts.png', 'docs/_static/matminer_examples.png', 'docs/_static/nature.css-e', 'docs/advanced.html', 'docs/automatminer.automl.config.html', 'docs/automatminer.automl.html', 'docs/automatminer.automl.tests.html', 'docs/automatminer.featurization.html', 'docs/automatminer.featurization.tests.html', 'docs/automatminer.html', 'docs/automatminer.preprocessing.html', 'docs/automatminer.preprocessing.tests.html', 'docs/automatminer.tests.html', 'docs/automatminer.utils.html', 'docs/automatminer.utils.tests.html', 'docs/basic.html', 'docs/datasets.html', 'docs/genindex.html', 'docs/index.html', 'docs/installation.html', 'docs/license.html', 'docs/modules.html', 'docs/modules.rst', 'docs/objects.inv', 'docs/py-modindex.html', 'docs/search.html', 'docs/searchindex.js', 'docs/source/_static/cv_nested.png', 'docs/source/_static/forum.png', 'docs/source/_static/logo.png', 'docs/source/_static/logo_header.png', 'docs/source/_static/matbench_pie_charts.png', 'docs/source/_static/matminer_examples.png', 'docs/source/_static/nature.css-e', 'docs/source/_templates/layout.html', 'docs/source/advanced.rst', 'docs/source/automatminer.automl.config.rst', 'docs/source/automatminer.automl.rst', 'docs/source/automatminer.automl.tests.rst', 'docs/source/automatminer.featurization.rst', 'docs/source/automatminer.featurization.tests.rst', 'docs/source/automatminer.preprocessing.rst', 'docs/source/automatminer.preprocessing.tests.rst', 'docs/source/automatminer.rst', 'docs/source/automatminer.tests.rst', 'docs/source/automatminer.utils.rst', 'docs/source/automatminer.utils.tests.rst', 'docs/source/basic.rst', 'docs/source/conf.py', 'docs/source/datasets.rst', 'docs/source/index.rst', 'docs/source/modules.rst', 'docs/source/tutorials.rst', 'docs/tutorials.html', 'requirements.txt', 'setup.py', 'tasks.py']"
14,hackingmaterials/automatminer,https://github.com/hackingmaterials/automatminer/issues/239,246,"Besides #238, I think the diagnostics into a fitted pipe could be further improved. In particular, it's too difficult to determine which model actually performed best.",101f1b850b944432b28e933964a1631fd811fc12,63dfaff40cf8578ceb3b180a4fc449a91926da63,"['.circleci/config.yml', '.circleci/config_old.yml', 'MANIFEST.in', 'README.md', 'automatminer/__init__.py', 'automatminer/automl/adaptors.py', 'automatminer/automl/base.py', 'automatminer/automl/tests/test_base.py', 'automatminer/base.py', 'automatminer/featurization/core.py', 'automatminer/featurization/sets.py', 'automatminer/pipeline.py', 'automatminer/preprocessing/core.py', 'automatminer/preprocessing/tests/test_core.py', 'automatminer/presets.py', 'automatminer/tests/test_base.py', 'automatminer/tests/test_pipeline.py', 'automatminer/utils/log.py', 'automatminer/utils/pkg.py', 'automatminer/utils/tests/test_log.py', 'automatminer/utils/tests/test_pkg.py', 'benchdev/graphnet/megnet.py', 'benchdev/tasks.py', 'benchdev/workflows.py', 'docs/_images/cv_nested.png', 'docs/_images/forum.png', 'docs/_images/logo.png', 'docs/_images/matbench_pie_charts.png', 'docs/_images/matminer_examples.png', 'docs/_sources/advanced.rst.txt', 'docs/_sources/automatminer.automl.config.rst.txt', 'docs/_sources/automatminer.automl.rst.txt', 'docs/_sources/automatminer.automl.tests.rst.txt', 'docs/_sources/automatminer.featurization.rst.txt', 'docs/_sources/automatminer.featurization.tests.rst.txt', 'docs/_sources/automatminer.preprocessing.rst.txt', 'docs/_sources/automatminer.preprocessing.tests.rst.txt', 'docs/_sources/automatminer.rst.txt', 'docs/_sources/automatminer.tests.rst.txt', 'docs/_sources/automatminer.utils.rst.txt', 'docs/_sources/automatminer.utils.tests.rst.txt', 'docs/_sources/basic.rst.txt', 'docs/_sources/datasets.rst.txt', 'docs/_sources/index.rst.txt', 'docs/_sources/modules.rst.txt', 'docs/_sources/tutorials.rst.txt', 'docs/_static/alabaster.css', 'docs/_static/cv_nested.png', 'docs/_static/documentation_options.js', 'docs/_static/forum.png', 'docs/_static/logo.png', 'docs/_static/logo.svg', 'docs/_static/logo_header.png', 'docs/_static/matbench_pie_charts.png', 'docs/_static/matminer_examples.png', 'docs/_static/nature.css-e', 'docs/advanced.html', 'docs/automatminer.automl.config.html', 'docs/automatminer.automl.html', 'docs/automatminer.automl.tests.html', 'docs/automatminer.featurization.html', 'docs/automatminer.featurization.tests.html', 'docs/automatminer.html', 'docs/automatminer.preprocessing.html', 'docs/automatminer.preprocessing.tests.html', 'docs/automatminer.tests.html', 'docs/automatminer.utils.html', 'docs/automatminer.utils.tests.html', 'docs/basic.html', 'docs/datasets.html', 'docs/genindex.html', 'docs/index.html', 'docs/installation.html', 'docs/license.html', 'docs/modules.html', 'docs/modules.rst', 'docs/objects.inv', 'docs/py-modindex.html', 'docs/search.html', 'docs/searchindex.js', 'docs/source/_static/cv_nested.png', 'docs/source/_static/forum.png', 'docs/source/_static/logo.png', 'docs/source/_static/logo_header.png', 'docs/source/_static/matbench_pie_charts.png', 'docs/source/_static/matminer_examples.png', 'docs/source/_static/nature.css-e', 'docs/source/_templates/layout.html', 'docs/source/advanced.rst', 'docs/source/automatminer.automl.config.rst', 'docs/source/automatminer.automl.rst', 'docs/source/automatminer.automl.tests.rst', 'docs/source/automatminer.featurization.rst', 'docs/source/automatminer.featurization.tests.rst', 'docs/source/automatminer.preprocessing.rst', 'docs/source/automatminer.preprocessing.tests.rst', 'docs/source/automatminer.rst', 'docs/source/automatminer.tests.rst', 'docs/source/automatminer.utils.rst', 'docs/source/automatminer.utils.tests.rst', 'docs/source/basic.rst', 'docs/source/conf.py', 'docs/source/datasets.rst', 'docs/source/index.rst', 'docs/source/modules.rst', 'docs/source/tutorials.rst', 'docs/tutorials.html', 'requirements.txt', 'setup.py', 'tasks.py']"
15,HunterMcGushion/hyperparameter_hunter,https://github.com/HunterMcGushion/hyperparameter_hunter/issues/88,89,"HI
ther's a problem when You try to rerun the optimizer with 'exactly' the same
hyperparameters's range of values;
for example, if I declare:
'max_depth'=Integer(2, 15)
and any other Real hyperpar., for ex.:
'min_split_loss'=(...)
the  hyper_space_size will be: np.inf;
then it make sense to rerun optimizer  also with high interations(in the same space);
but if I try to do that, the following Uncaught exception occur:
""Traceback (most recent call last):
File ""D:/MachinLrn/Esperiments_Prjct/scripts/one.py"", line 135, in 
execute()
File ""D:/MachinLrn/Esperiments_Prjct/scripts/one.py"", line 109, in execute
optimizer.go()  # Now, we go
File ""D:\software\Anaconda3\envs\RBM\lib\site-packages\hyperparameter_hunter\optimization.py"", line 87, in go
super().go()
File ""D:\software\Anaconda3\envs\RBM\lib\site-packages\hyperparameter_hunter\optimization_core.py"", line 340, in go
self._optimization_loop()
File ""D:\software\Anaconda3\envs\RBM\lib\site-packages\hyperparameter_hunter\optimization_core.py"", line 364, in _optimization_loop
if len(self.similar_experiments) + len(self.tested_keys) >= self.search_space_size:
File ""D:\software\Anaconda3\envs\RBM\lib\site-packages\hyperparameter_hunter\optimization_core.py"", line 802, in search_space_size
self._search_space_size = len(self.hyperparameter_space)
TypeError: 'float' object cannot be interpreted as an integer
then if I change the max_depth range , like (2, 14) and rerun optimizer, all works well, again...",6aca27b3f7a09a88810a50ea171d8938eaa83c13,d8d9390c2453302c5cf6ee26b0f681baa97bb7f2,"['CHANGELOG.md', 'hyperparameter_hunter/sentinels.py', 'hyperparameter_hunter/space.py', 'hyperparameter_hunter/utils/optimization_utils.py']"
16,HunterMcGushion/hyperparameter_hunter,https://github.com/HunterMcGushion/hyperparameter_hunter/issues/111,118,"HI Hunter!
seems to me that ther's a issue with Keras optimization..
when I define my custom ""_build_fn_optimization"" function,
i need to pass some parameters for a functional model initialization;
but seems that these parameters are not available in the scope of function;
then name error issue raises, for example:
""NameError: name 'maxlen' is not defined...
or am I wrong something?..
thanks
embedding_matrix = # load_glove.....
def _build_fn_optimization(embedding_matrix, maxlen=100, max_features=10000, embed_size=200):

    inp = Input(shape=(maxlen,))
    x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=False)(inp)
    x = SpatialDropout1D(rate=Real(0.2, 0.5))(x)
    x = Bidirectional(CuDNNGRU(Integer(50, 150), return_sequences=True,
                               kernel_initializer=glorot_normal(seed=22),
                               recurrent_initializer=orthogonal(gain=1.0, seed=10)))(x)

    x = Flatten()(x)
    x = Dense(Integer(100, 200), activation=""relu"", kernel_initializer=glorot_normal(seed=1))(x)
    x = Dropout(0.1)(x)
    x = BatchNormalization()(x)

    x = Dense(1, activation=""sigmoid"")(x)
    model = Model(inputs=inp, outputs=x)
    model.compile(loss='binary_crossentropy', optimizer=Categorical([""adam"", ""rmsprop""]), metrics=[""accuracy""])
    return model

def execute():
   
    env = Environment(
        train_dataset=x_train,
        target_column='target',
        root_results_path=Keras_path,
        do_predict_proba=True,  # True / 1
        metrics_map=['roc_auc_score'],
        # metrics_map=dict(f1=lambda y_true, y_pred: f1_score(y_true, y_pred, average=""micro"")),
        cross_validation_type=StratifiedKFold,
        cross_validation_params=dict(n_splits=5,  shuffle=True, random_state=32),
        runs=1
    )
   

    # Optimization ####################
    optimizer = BayesianOptimization(iterations=5,
                                     acquisition_optimizer_kwargs=dict(
                                                                     n_points=10000,
                                                                     n_restarts_optimizer=5,
                                                                     n_jobs=8),
                                    )

    optimizer.set_experiment_guidelines(
        model_initializer=KerasClassifier,
        model_init_params=dict(build_fn=_build_fn_optimization),
        model_extra_params=dict(
            fit=dict(
                    eval_set=[(env.train_input, env.train_target),
                              (env.validation_input, env.validation_target)],
                    early_stopping_rounds=5),
            # callbacks=[EarlyStopping(patience=5, monitor='val_acc', mode='max')],
            batch_size=Categorical([32, 64], transform=""onehot""),
            epochs=500,
            verbose=0,
        ),
    )",08890dc27f4f5e22f4c4ce89a3210ea965804bae,e0a67ed1f7eedeee35bb158b1012c6d236ec7969,"['.coveragerc', '.gitignore', 'CHANGELOG.md', 'hyperparameter_hunter/__init__.py', 'hyperparameter_hunter/experiments.py', 'hyperparameter_hunter/importer.py', 'hyperparameter_hunter/key_handler.py', 'hyperparameter_hunter/library_helpers/keras_helper.py', 'hyperparameter_hunter/library_helpers/keras_optimization_helper.py', 'hyperparameter_hunter/models.py', 'hyperparameter_hunter/optimization_core.py', 'hyperparameter_hunter/result_reader.py', 'hyperparameter_hunter/settings.py', 'hyperparameter_hunter/tracers.py', 'hyperparameter_hunter/utils/general_utils.py', 'hyperparameter_hunter/utils/optimization_utils.py', 'hyperparameter_hunter/utils/parsing_utils.py', 'scripts/clean.sh', 'scripts/docstr_coverage.sh', 'scripts/test.sh', 'tests/smoke_tests/test_keras.py', 'tests/smoke_tests/test_lightgbm.py', 'tests/test_experiments.py', 'tests/test_keras_helper.py', 'tests/test_keras_optimization_helper.py', 'tests/test_optimization_utils.py']"
17,HunterMcGushion/hyperparameter_hunter,https://github.com/HunterMcGushion/hyperparameter_hunter/issues/34,91,"Update the following methods of optimization_core.InformedOptimizationProtocol:

_execute_experiment
_find_similar_experiments


The two aforementioned methods are the two locations at which optimization_core.InformedOptimizationProtocol.optimizer is ""tell-ed"" the utility value of a set of hyperparameters
Currently, a negative utility value is provided to optimizer, which will cause problems if target_metric should be minimized

This is the case when target_metric is some loss measure


Need to add a means of specifying that positive utility values should be used, instead of negative, or of detecting that target_metric measures loss",0d4a1b2bd363620b234893fc60768c7379f87165,9204f299cb6cf679c686c998873a85d2a337fff4,"['hyperparameter_hunter/environment.py', 'hyperparameter_hunter/experiments.py', 'hyperparameter_hunter/key_handler.py', 'hyperparameter_hunter/leaderboards.py', 'hyperparameter_hunter/metrics.py', 'hyperparameter_hunter/optimization_core.py', 'hyperparameter_hunter/recorders.py', 'hyperparameter_hunter/reporting.py', 'hyperparameter_hunter/result_reader.py', 'hyperparameter_hunter/utils/optimization_utils.py']"
18,qubvel/efficientnet,https://github.com/qubvel/efficientnet/issues/135,130,"I just verified with the reference tf.keras implementation, and here are the results. Below is the output for B5
This implementation's drop connect rate
(index, name, rate)
0 block1b_drop 0.9875
1 block1c_drop 0.975
2 block2b_drop 0.95
3 block2c_drop 0.9375
4 block2d_drop 0.925
5 block2e_drop 0.9125
6 block3b_drop 0.8875
7 block3c_drop 0.875
8 block3d_drop 0.8625
9 block3e_drop 0.85
10 block4b_drop 0.825
11 block4c_drop 0.8125
12 block4d_drop 0.8
13 block4e_drop 0.7875
14 block4f_drop 0.775
15 block4g_drop 0.7625
16 block5b_drop 0.7375
17 block5c_drop 0.725
18 block5d_drop 0.7124999999999999
19 block5e_drop 0.7
20 block5f_drop 0.6875
21 block5g_drop 0.675
22 block6b_drop 0.6499999999999999
23 block6c_drop 0.6375
24 block6d_drop 0.625
25 block6e_drop 0.6125
26 block6f_drop 0.6
27 block6g_drop 0.5874999999999999
28 block6h_drop 0.575
29 block6i_drop 0.5625
30 block7b_drop 0.5375
31 block7c_drop 0.5249999999999999
32 top_dropout 0.6

Tensorflow's drop connect rate
0 block1b_drop 0.9948717948717949
1 block1c_drop 0.9897435897435898
2 block2b_drop 0.9794871794871794
3 block2c_drop 0.9743589743589743
4 block2d_drop 0.9692307692307692
5 block2e_drop 0.9641025641025641
6 block3b_drop 0.9538461538461538
7 block3c_drop 0.9487179487179487
8 block3d_drop 0.9435897435897436
9 block3e_drop 0.9384615384615385
10 block4b_drop 0.9282051282051282
11 block4c_drop 0.9230769230769231
12 block4d_drop 0.9179487179487179
13 block4e_drop 0.9128205128205128
14 block4f_drop 0.9076923076923077
15 block4g_drop 0.9025641025641026
16 block5b_drop 0.8923076923076922
17 block5c_drop 0.8871794871794871
18 block5d_drop 0.882051282051282
19 block5e_drop 0.8769230769230769
20 block5f_drop 0.8717948717948718
21 block5g_drop 0.8666666666666667
22 block6b_drop 0.8564102564102564
23 block6c_drop 0.8512820512820513
24 block6d_drop 0.8461538461538461
25 block6e_drop 0.841025641025641
26 block6f_drop 0.8358974358974359
27 block6g_drop 0.8307692307692307
28 block6h_drop 0.8256410256410256
29 block6i_drop 0.8205128205128205
30 block7b_drop 0.8102564102564103
31 block7c_drop 0.8051282051282052
32 top_dropout 0.6

At index 31 it's off by a significant amount",f7f3e736c113b872caf53dae9fbbda996a8eb87d,40b6afe90a21e3cc414f4cd31fb58fb6c1e550fe,['efficientnet/model.py']
19,qubvel/efficientnet,https://github.com/qubvel/efficientnet/issues/60,130,"Thank you so much for your great work on this and the segmentation_models!
Wenn I use the efficientnetB7 with tensorflow 2.0 (2.0.0-beta1) it prints:

Large dropout rate: 0.5625 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended

Is this already handled?",f7f3e736c113b872caf53dae9fbbda996a8eb87d,40b6afe90a21e3cc414f4cd31fb58fb6c1e550fe,['efficientnet/model.py']
20,qubvel/efficientnet,https://github.com/qubvel/efficientnet/issues/51,52,"I cannot load models with the v1.0.0b1 version of your package. In model.py of your package you are calling backend.backend(): https://github.com/qubvel/efficientnet/blob/master/efficientnet/model.py#L246 .
I cannot see anywhere where this is supposed to be automatically initialized to the user's backend. The same file contains backend = None: https://github.com/qubvel/efficientnet/blob/master/efficientnet/model.py#L43
When trying to load a model this leads to the following error message:
Found 3662 validated image filenames.
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-13-3f84b7149375> in <module>
      8                       model_dir = ""../"",
      9                       folds = 1,
---> 10                       batch_size = 16)
     11 ("""")

<ipython-input-12-eda66ccb6232> in pred_cv_test(df, model_dir, folds, nr_samples, batch_size)
     31                                     batch_size = batch_size)
     32 
---> 33         model = get_model(model_dir = model_dir, fold_nr = fold)
     34         preds = model.predict_generator(
     35             generator = test_generator,

<ipython-input-12-eda66ccb6232> in get_model(model_dir, fold_nr)
      9     best_epoch_model = load_model(
     10         model_dir + ""effnet_reg_fold"" + str(fold_nr) + "".h5"",
---> 11         custom_objects = {""root_mse"": root_mse})
     12     return(best_epoch_model)
     13 

~\Anaconda3\envs\r-tensorflow\lib\site-packages\tensorflow\python\keras\saving\save.py in load_model(filepath, custom_objects, compile)
    144       h5py is not None and (
    145           isinstance(filepath, h5py.File) or h5py.is_hdf5(filepath))):
--> 146     return hdf5_format.load_model_from_hdf5(filepath, custom_objects, compile)
    147 
    148   if isinstance(filepath, six.string_types):

~\Anaconda3\envs\r-tensorflow\lib\site-packages\tensorflow\python\keras\saving\hdf5_format.py in load_model_from_hdf5(filepath, custom_objects, compile)
    210     model_config = json.loads(model_config.decode('utf-8'))
    211     model = model_config_lib.model_from_config(model_config,
--> 212                                                custom_objects=custom_objects)
    213 
    214     # set weights

~\Anaconda3\envs\r-tensorflow\lib\site-packages\tensorflow\python\keras\saving\model_config.py in model_from_config(config, custom_objects)
     53                     '`Sequential.from_config(config)`?')
     54   from tensorflow.python.keras.layers import deserialize  # pylint: disable=g-import-not-at-top
---> 55   return deserialize(config, custom_objects=custom_objects)
     56 
     57 

~\Anaconda3\envs\r-tensorflow\lib\site-packages\tensorflow\python\keras\layers\serialization.py in deserialize(config, custom_objects)
     87       module_objects=globs,
     88       custom_objects=custom_objects,
---> 89       printable_module_name='layer')

~\Anaconda3\envs\r-tensorflow\lib\site-packages\tensorflow\python\keras\utils\generic_utils.py in deserialize_keras_object(identifier, module_objects, custom_objects, printable_module_name)
    190             custom_objects=dict(
    191                 list(_GLOBAL_CUSTOM_OBJECTS.items()) +
--> 192                 list(custom_objects.items())))
    193       with CustomObjectScope(custom_objects):
    194         return cls.from_config(cls_config)

~\Anaconda3\envs\r-tensorflow\lib\site-packages\tensorflow\python\keras\engine\network.py in from_config(cls, config, custom_objects)
   1129         if layer in unprocessed_nodes:
   1130           for node_data in unprocessed_nodes.pop(layer):
-> 1131             process_node(layer, node_data)
   1132 
   1133     name = config.get('name')

~\Anaconda3\envs\r-tensorflow\lib\site-packages\tensorflow\python\keras\engine\network.py in process_node(layer, node_data)
   1085         flat_input_tensors = nest.flatten(input_tensors)
   1086         if len(flat_input_tensors) == 1:
-> 1087           layer(flat_input_tensors[0], **kwargs)
   1088         else:
   1089           layer(input_tensors, **kwargs)

~\Anaconda3\envs\r-tensorflow\lib\site-packages\tensorflow\python\keras\engine\base_layer.py in __call__(self, inputs, *args, **kwargs)
    632                     outputs = base_layer_utils.mark_as_return(outputs, acd)
    633                 else:
--> 634                   outputs = call_fn(inputs, *args, **kwargs)
    635 
    636             except TypeError as e:

~\Anaconda3\envs\r-tensorflow\lib\site-packages\tensorflow\python\keras\layers\core.py in call(self, inputs)
    358 
    359   def call(self, inputs):
--> 360     return self.activation(inputs)
    361 
    362   def compute_output_shape(self, input_shape):

~\Anaconda3\envs\r-tensorflow\lib\site-packages\efficientnet\model.py in swish(x)
    134     """"""
    135 
--> 136     if backend.backend() == 'tensorflow':
    137         try:
    138             # The native TF implementation has a more

AttributeError: 'NoneType' object has no attribute 'backend'

I can get it to work if I hardcode my backend by editing model.py.
In your inference notebook you are not loading a saved model from disk. Have you guys tested and successfully loaded in a model from disk with the new version?",5a3f95740e443b81c3827cceea78fd883af90845,3ff1824465b6931dbf0a8d61c62e159b58602589,"['efficientnet/__init__.py', 'efficientnet/model.py']"
21,netrack/keras-metrics,https://github.com/netrack/keras-metrics/issues/3,4,"Hi, I've seen the similar issue that was about evaluation after training on multi-class classification, mine is about the training. If one has only one unit at the end, everything is correct, however if dense layer with two units is used for the last layer with a softmax activation, true positive is always equals to true negative, and false positive is always equals to false negative in every batch during the training. And when you sum all of them you obtain a number that is exactly equals to twice of batch size. Please kindly check my code below.
Note: I'm aware of the fact that these metrics shouldn't be checked per batch basis, but seeing something wrong pushed me to create this issue. By the way, thanks for the library and effort.
https://colab.research.google.com/drive/1lmQ-hWcN4tsGMicd4dKnSjeTD-BdgJuE",56042dfdd4f6cec23628c012e178897c6f21bd40,1c47fe748b7018249ecf2e6a189e1d3d55c271e5,['keras_metrics/metrics.py']
22,netrack/keras-metrics,https://github.com/netrack/keras-metrics/issues/22,23,"I'm trying to solve a binary segmentation problem with a highly imbalanced dataset, so tracking the recall and precision is useful. I'm using a keras implementation of a CNN.
imgs_train, imgs_mask_train, imgs_test, imgs_validation, mask_validation = self.load_data()
model = self.get_unet()
weighted_loss=weighted_pixelwise_crossentropy(np.array([1,self.loss_weight]))

adam = optimizers.Adam()
precision = keras_metrics.precision() 
recall = keras_metrics.recall() model.compile(loss=weighted_loss,optimizer=adam, metrics=['accuracy', precision, recall])
model_checkpoint = ModelCheckpoint(self.experiment_id+'.hdf5', monitor='loss',verbose=1, save_best_only=True, save_weights_only= True)
csv_logger = CSVLogger(self.augmented_datapath+""_""+self.experiment_id+'.csv', append=True, separator=';')
model.fit(imgs_train, imgs_mask_train, validation_data=[imgs_validation, mask_validation], epochs=20, verbose=1, callbacks=[model_checkpoint, csv_logger])

Here's a snippet of the code I'm using. load_data() returns numpy arrays for the training, testing, and validation data, and the model returns a (512, 512, 2) array, with 1s in one of the two columns for each pixel.
Traceback (most recent call last):
  File ""job_array_nnet.py"", line 74, in <module>
    job_array(args.array_id)
  File ""job_array_nnet.py"", line 66, in job_array
    nnet.train_neuralnet()
  File ""/mainfs/home/jw22g14/DeepSVM/neural_net_model.py"", line 172, in train_neuralnet
    model.compile(loss=weighted_loss,optimizer=adam, metrics=['accuracy', precision, recall])
  File ""/home/jw22g14/.conda/envs/python3/lib/python3.6/site-packages/keras/engine/training.py"", line 440, in compile
    handle_metrics(output_metrics)
  File ""/home/jw22g14/.conda/envs/python3/lib/python3.6/site-packages/keras/engine/training.py"", line 409, in handle_metrics
    mask=masks[i])
  File ""/home/jw22g14/.conda/envs/python3/lib/python3.6/site-packages/keras/engine/training_utils.py"", line 403, in weighted
    score_array = fn(y_true, y_pred)
  File ""/home/jw22g14/.conda/envs/python3/lib/python3.6/site-packages/keras_metrics/metrics.py"", line 218, in __call__
    tp = self.tp(y_true, y_pred)
  File ""/home/jw22g14/.conda/envs/python3/lib/python3.6/site-packages/keras_metrics/metrics.py"", line 72, in __call__
    y_true, y_pred = self.cast(y_true, y_pred)
  File ""/home/jw22g14/.conda/envs/python3/lib/python3.6/site-packages/keras_metrics/metrics.py"", line 26, in cast
    return self.cast_strategy(y_true, y_pred, dtype=dtype)
  File ""/home/jw22g14/.conda/envs/python3/lib/python3.6/site-packages/keras_metrics/metrics.py"", line 44, in _categorical
    _, labels = y_pred.shape

And here's the stack trace.",883ce21a76292b09fc8bbd920532aea6efc189f6,f12f3358a458357d5abf3eeb2d05aae46aa554e2,['keras_metrics/metrics.py']
23,NRCan/geo-deep-learning,https://github.com/NRCan/geo-deep-learning/issues/292,294,"Model_choice.py has gotten messy and cluttered over the years and needs a bit of refactoring. This refactoring should be done before addressing #246 and #152.
Current state of things

set_hyperparamers() function has very vague prupose of ""set[ting] hyperparameters based on values provided in yaml config file"";
net() function is supposed to ""Define the neural net"", but in reality it's an all-in-one vague function that does the following:


Defines net architecture;
Reads a checkpoint to memory with load_checkpoint() (from a .pth.tar file as created by torch.save);
Returns if net() is called from inference or continues with the following if net() is called from train mode:
If more than one gpu is requested, determines which gpus are available based on user-inputted threshold for GPU's available RAM and usage %;
Sets model to DataParallel if more than one gpu is requested and available;
Sets main device with set_device() function;
Pushes model to main device;
Calls set_hyperparamters() (see above);
Pushes loss to device;
Returns 7 (!!) objects: model, model_name, loss, etc.

Suggested solution (high level)
All these steps could be better separated in small, dedicated functions of their own:

read_checkpoint(): renamed version of load_checkpoint (prevents confusion with load_state_dict function). Although it derives from torch.load(checkpoint)'s function, this function really just reads a checkpoint in memory from a .pth.tar file to a Python dict containing weights, optimizer, etc.
define_net_architecture(): define the model architecture from config parameters (i.e. create model with randomly initialized weights)
adapt_checkpoint_to_dp_model(): for use at test loop during training only, adapts a generic checkpoint to be loaded to a DataParallel model as is done in load_from_checkpoint (if model is DataParallel object)
define_loss(): calls verify_weights() and instantiates a loss criterion
define_optimizer(): instantiates optimizer with learning rate, weight decay, etc.

These functions would be called only when necessary in 3 main places:

Beginning of train_segmentation:


read checkpoint to be loaded checkpoint for model weights and optimizer;
define net architecture;
load model weights with pytorch's [model_object].load_state_dict() method;
define loss;
define optimizer;
load optimizer from checkpoint;


Test loop in train_segmentation:


Load best checkpoint to model (adapt checkpoint keys if model is a nn.DataParallel instance using dedicated function);


Beginning of inference:


override architecture, input bands, output classes from checkpoint's params;
define net architecture
load weights from provided checkpoint to model using pytorch's [model_object].load_state_dict() method",ef96eb6c77dd031c01f981790970bcd93456286b,a1ee851c79b46f9a79a84d62e24a12909b412bfd,"['config/gdl_config_template.yaml', 'config/model/deeplabv3_resnet101_dualhead.yaml', 'config/training/default_training.yaml', 'inference_segmentation.py', 'models/model_choice.py', 'tests/loss/test_losses.py', 'tests/model/test_models.py', 'train_segmentation.py', 'utils/loss.py', 'utils/utils.py']"
24,NRCan/geo-deep-learning,https://github.com/NRCan/geo-deep-learning/issues/57,64,"In train_model.py, there isn't any verification that the number of classes used for training in classification tasks is the same as the parameter given in the config file. The model is built based on the the config file's value for num_classes
If the wrong number of classes is used for training, it results in meaningless outputs from the model and IndexError: list index out of range  during classification.
We need to add a check to avoid running meaningless trainings.",26fa5599030c7aa346836ba61bc22471971385a5,40b1ac8d62a3c2722c829cb07d96488927a4e851,"['inference.py', 'models/model_choice.py', 'train_model.py']"
25,NRCan/geo-deep-learning,https://github.com/NRCan/geo-deep-learning/issues/6,30,"#Objectives
The intent of this project is to provide a basic system for deep learning uses in remote sensing (RS). As of today, the application has the capacity to perform semantic segmentation of images. In order to provide a larger capacity for RS tools, there is (at least) one task we need to add to the system : classification.
The definition of both these tasks and the difference between the two can be found in the lexicon.",b0cff2995640b83fed6f0a44e6a32cc6447fa0a2,1a357dd067a39e1629b8317cff7379d98c662f83,"['.travis.yml', 'README.md', 'conf/config.yaml', 'conf/config_ci_tst.yaml', 'conf/config_ci_tst2.yaml', 'data/DATA_INFO.txt', 'data/classification_data.zip', 'image_classification.py', 'images_to_samples.py', 'models/inception.py', 'models/model_choice.py', 'train_model.py', 'utils.py']"
26,NRCan/geo-deep-learning,https://github.com/NRCan/geo-deep-learning/issues/293,295,"As was done with losses in #255  and as will be done with model architectures in #246, optimizers should be instantiated with hydra's instantiate() function and config parameters should be adapted accordingly.",a1e322d8341ad9588b8836ea2727bac4619e1281,ef96eb6c77dd031c01f981790970bcd93456286b,"['config/gdl_config_template.yaml', 'config/model/checkpoint_unet.yaml', 'config/model/deeplabv3+_pretrained.yaml', 'config/model/deeplabv3_pretrained.yaml', 'config/model/deeplabv3_resnet101_dualhead.yaml', 'config/model/gdl_deeplabv3-dualhead.yaml', 'config/model/gdl_unet.yaml', 'config/model/gdl_unetsmall.yaml', 'config/model/smp_deeplabv3.yaml', 'config/model/smp_deeplabv3plus.yaml', 'config/model/smp_manet.yaml', 'config/model/smp_pan.yaml', 'config/model/smp_unet-resnext101.yaml', 'config/model/smp_unet-spacenet-baseline.yaml', 'config/model/smp_unet-spacenet-efficientnetb5.yaml', 'config/model/smp_unet-spacenet-senet154.yaml', 'config/model/smp_unet.yaml', 'config/model/smp_unetplusplus.yaml', 'config/model/unet.yaml', 'config/model/unet_pretrained.yaml', 'config/model/unet_small.yaml', 'config/old_development/config_test_4channels_implementation.yaml', 'config/optimizer/adabound.yaml', 'config/optimizer/adaboundw.yaml', 'config/optimizer/adam.yaml', 'config/optimizer/adamw.yaml', 'config/optimizer/sgd.yaml', 'inference_segmentation.py', 'models/checkpointed_unet.py', 'models/model_choice.py', 'models/unet.py', 'tests/loss/test_losses.py', 'tests/model/test_models.py', 'tests/optimizer/test_optimizers.py', 'train_segmentation.py', 'utils/optimizer.py', 'utils/utils.py']"
27,NRCan/geo-deep-learning,https://github.com/NRCan/geo-deep-learning/issues/84,89,"The receptive field of a model is currently calculated from the number of max pooling layers in the model's architecture. There are some limitations to this approach:

Max pooling layers can be replaced with strides in conv layers, therefore augmenting the model's receptive field.
Max pooling of size > 2x2 are not currently handled.

Depending of the architecture, calculating the receptive field of a model can be tricky/impossible. Therefore, it would be best to remove the function to calculate the receptive field and simply replace it with:
a) a constant value, parametrizable from the config file; OR
b) a constant value deduced from the size of the input patch (e.g. 10% in height/width)",9d5b7637b2406be885a0ffd2e922326b11d957fb,4cca1dc519c954a885d5a541adf0ee2e0d431124,['inference.py']
28,NRCan/geo-deep-learning,https://github.com/NRCan/geo-deep-learning/issues/305,308,"A recent team poll has shown that support for boto3 features in the code is largely considered irrelevant.
Some places where boto is used:

tiling (aka sampling): download data from AWS bucket
inference: download imagery from AWS bucket",ae922a3e698b54d229d2d3b7538da22bd4540d24,badc9790aa9552f98ad32c37bc4a29375a54c104,"['README.md', 'config/README.md', 'config/gdl_config_template.yaml', 'inference_segmentation.py', 'sampling_segmentation.py', 'train_segmentation.py', 'utils/logger.py', 'utils/utils.py']"
29,NRCan/geo-deep-learning,https://github.com/NRCan/geo-deep-learning/issues/260,263,"In current version of GDL, the only task available is segmentation
PR #208 removed train_classification and classification at inference remains implemented in inference_segmentation.py, but is unreachable due to the current usage of GDL.py using mode and task to execute a specific script.",6b037e12c099731e913cff72d5bd6a6cecd9d208,31d429c0704daef88ccce33ece960aaf2e6005f0,"['.github/workflows/github-actions-ci.yml', 'GDL.py', 'config/gdl_config_template.yaml', 'config/task/segmentation.yaml', 'data/images_to_samples_ci_csv.csv', 'inference_segmentation.py', 'sampling_segmentation.py', 'train_segmentation.py', 'utils/metrics.py', 'utils/utils.py']"
30,NRCan/geo-deep-learning,https://github.com/NRCan/geo-deep-learning/issues/189,208,"Config_template should reflect a ""generally-good-to-start-with"" configuration, that is, trying to cover common use cases of GDL and output an ""ok"" model to start with. For example, we could set batch size=4, num_gpus=1, num_epoch=50, architecture to a simple pretrained unet that has been giving good results for us (@victorlazio109 which one?), etc.
This issue has been raised by @bstdenis. Don't hesitate to comment Mr. St-Denis!",f7e60334a1da6547ba6174f70b0873493991ff76,d00c443d0554b44008c95fa5d1df88ecbaf8a4a6,"['.travis.yml', 'GDL.py', 'README.md', 'config/README.md', 'config/augmentation/basic_augmentation_segmentation.yaml', 'config/callbacks/default_callbacks.yaml', 'config/config_template.yaml', 'config/config_template_minimum.yaml', 'config/dataset/test_ci_segmentation_dataset.yaml', 'config/gdl_config_template.yaml', 'config/hydra/default.yaml', 'config/inference/default_inference.yaml', 'config/model/checkpoint_unet.yaml', 'config/model/deeplabv3+_pretrained.yaml', 'config/model/deeplabv3_resnet101.yaml', 'config/model/fastrcnn.yaml', 'config/model/fcn_resnet101.yaml', 'config/model/inception.yaml', 'config/model/ternauset.yaml', 'config/model/unet.yaml', 'config/model/unet_pretrained.yaml', 'config/model/unet_small.yaml', 'config/old_development/config_test_4channels_implementation.yaml', 'config/optimizer/adam.yaml', 'config/optimizer/adamw.yaml', 'config/optimizer/sgd.yaml', 'config/scheduler/cosine.yaml', 'config/scheduler/cosinewarm.yaml', 'config/scheduler/cyclic.yaml', 'config/scheduler/multi_step_reg.yaml', 'config/scheduler/plateau.yaml', 'config/scheduler/step.yaml', 'config/task/segmentation.yaml', 'config/tracker/mlflow.yaml', 'config/tracker/tensorflow.yaml', 'config/training/default_training.yaml', 'config/travis_CI/config_ci_classification_local.yaml', 'config/travis_CI/config_ci_segmentation_local.yaml', 'config/travis_CI/environment.yml', 'config/v1.1.0/config_canvecaux.yaml', 'config/v1.1.0/config_coordconv.yaml', 'config/v1.1.0/config_metasegm.yaml', 'config/visualization/default_visualization.yaml', 'environment.yml', 'inference_segmentation.py', 'models/model_choice.py', 'sampling_segmentation.py', 'train_classification.py', 'train_segmentation.py', 'utils/augmentation.py', 'utils/create_dataset.py', 'utils/data_analysis.py', 'utils/logger.py', 'utils/optimizer.py', 'utils/utils.py', 'utils/verifications.py']"
31,NRCan/geo-deep-learning,https://github.com/NRCan/geo-deep-learning/issues/222,335,"Soon, GDL will internally be used to read images from an S3 bucket, using STAC items and Franklin's API to submit requests based on spatial, temporal or other filters from metadata.
GDL's readers should therefore transition from csv as input (#221) to a json containing a list of STAC items (as json). Here's an example of a response from a request passed to Franklin that could directly be used by GDL in the future:
{ ""type"": ""FeatureCollection"", ""context"": { ""returned"": 1, ""matched"": 1 }, ""features"": [ { ""id"": ""QC15_052686187030_01_QuickBird2"", ""stac_version"": ""1.0.0-beta.2"", ""stac_extensions"": [ ""https://stac-extensions.github.io/projection/v1.0.0/schema.json"" ], ""type"": ""Feature"", ""geometry"": { ""type"": ""Polygon"", ""coordinates"": ... ], [ (2nd STAC item) ], [ (3rd STAC item) ], ""links"": [], ""stac_version"": ""1.0.0"" }
Here are 5 examples of STAC Items for IA-Ready images as served by Franklin's API:
https://datacube-stage.services.geo.ca/api/collections/worldview-3-ortho-pansharp/items/QC22_055128844110_01_WV3
https://datacube-stage.services.geo.ca/api/collections/worldview-4-ortho-pansharp/items/MB8_059380728010_01_P001_WV4
https://datacube-stage.services.geo.ca/api/collections/worldview-2-ortho-pansharp/items/VancouverP003_054230029070_01_P003_WV2
https://datacube-stage.services.geo.ca/api/collections/geoeye-1-ortho-pansharp/items/AB13_059294491080_01_GeoEye1
https://datacube-stage.services.geo.ca/api/collections/quickbird-2-ortho-pansharp/items/QC15_052686187030_01_QuickBird2
An issue remains: how to link the ground truth to this list? A separate json? This needs further analysis.",79af18767ad311c58f253215fe1fd9923d7c75a4,71bc916917cd798db64d144cab4dc822374fd4fb,"['config/inference/default_binary.yaml', 'config/inference/default_multiclass.yaml', 'dataset/aoi.py', 'evaluate_segmentation.py', 'inference_segmentation.py', 'tests/inference/inference_segmentation_binary.csv', 'tests/inference/inference_segmentation_multiclass.csv', 'utils/utils.py', 'utils/verifications.py']"
32,NRCan/geo-deep-learning,https://github.com/NRCan/geo-deep-learning/issues/370,373,"PR #309 introduced a bug:
parameter in config file switched from ""modalities"" to ""bands"":
https://github.com/NRCan/geo-deep-learning/pull/309/files#diff-3ca7ff76e0e023a04889c907451feaf113bff3830e23e3707976e2e4d6a91177L13

But this change was not applied to train_segmentation.py script:
https://github.com/NRCan/geo-deep-learning/blob/develop/train_segmentation.py#L468",a33c57deb150dd08c8aa53688e7b1c6df5cc7f57,8df2b1277c17b3ba8fa7753b489c1e9ed8babbf9,['train_segmentation.py']
33,NRCan/geo-deep-learning,https://github.com/NRCan/geo-deep-learning/issues/183,298,"If #154 is covered, then we can easily adapt inference.py to run without a yaml. We could think of 2 mandatory arguments:

image url (local path or from web)
model url (local path or from web to checkpoint.pth.tar)

There could also be a third, optional argument:

number of gpus.

If this third argument is optional though, we'd have to determine what the default behavior is:

if multiple gpus are available, run on one gpu only?
if multiple gpus are available, run on all available gpus? This implies that Inference: ability to parallelize (multi-gpu) inference on a single imageÂ #182 would be solved.",aafaa1772bd0c8003acfe3c2b628322e184772a5,a1e322d8341ad9588b8836ea2727bac4619e1281,"['config/model/smp_unet.yaml', 'inference_segmentation.py', 'models/deeplabv3_dualhead.py', 'models/model_choice.py', 'tests/model/test_models.py', 'train_segmentation.py', 'utils/utils.py']"
34,NRCan/geo-deep-learning,https://github.com/NRCan/geo-deep-learning/issues/249,259,"inference_segmentation.py currently offers to optionally calculate metrics of a prediction on a new image, if the ground truth is inputted alongside. This feature is useful, but should belong to its own module. I suggest we remove support for ground truth at inference and plan a benchmark/evaluation module of its own.",0499dfd603d85d0bdf0017b26b64c21d637ea98b,6b037e12c099731e913cff72d5bd6a6cecd9d208,"['.github/workflows/github-actions-ci.yml', 'evaluate_segmentation.py', 'inference_segmentation.py', 'utils/verifications.py']"
35,NRCan/geo-deep-learning,https://github.com/NRCan/geo-deep-learning/issues/264,266,"Currents CI testing take 15 minutes, but 11 minutes are dedicated to inference on 2 massachusetts' dataset images.
This is outrageously long for CI tests considering it doesn't add any value to the testing: we know after a minute or so that inference works.",89f7192797b022acde122761222cb77aa7d6fe2a,0499dfd603d85d0bdf0017b26b64c21d637ea98b,"['.github/workflows/github-actions-ci.yml', 'README.md', 'config/inference/default_inference.yaml', 'data/22978945_15.tif', 'data/22978945_15_uint8_clipped.tif', 'data/23429155_15.tif', 'data/23429155_15_uint8_clipped.tif', 'data/23429155_15_uint8_inf.tif', 'data/images_to_samples_ci_csv.csv', 'data/inference_sem_seg_ci_csv.csv', 'data/massachusetts_buildings.gpkg', 'train_segmentation.py']"
36,NRCan/geo-deep-learning,https://github.com/NRCan/geo-deep-learning/issues/157,208,"At inference, our model_choice.py shouldn't load any pretrained weights (ex.: Deeplabv3 with Coco pretrained weights) if a .pth is provided.
Currently, if model is trained with pretrained weights (ie ""pretrained=True"" in training section)1, a pretrained model is loaded by torchvision before the weights from the user-specified .pth are loaded. This is useless and creates the unnecessary need to an internet connection during inference.
A solution would be to override ""pretrained"" value to ""False"" if a .pth is provided at inference. This should be document in the yaml (warn user of this behavior) and README.",f7e60334a1da6547ba6174f70b0873493991ff76,d00c443d0554b44008c95fa5d1df88ecbaf8a4a6,"['.travis.yml', 'GDL.py', 'README.md', 'config/README.md', 'config/augmentation/basic_augmentation_segmentation.yaml', 'config/callbacks/default_callbacks.yaml', 'config/config_template.yaml', 'config/config_template_minimum.yaml', 'config/dataset/test_ci_segmentation_dataset.yaml', 'config/gdl_config_template.yaml', 'config/hydra/default.yaml', 'config/inference/default_inference.yaml', 'config/model/checkpoint_unet.yaml', 'config/model/deeplabv3+_pretrained.yaml', 'config/model/deeplabv3_resnet101.yaml', 'config/model/fastrcnn.yaml', 'config/model/fcn_resnet101.yaml', 'config/model/inception.yaml', 'config/model/ternauset.yaml', 'config/model/unet.yaml', 'config/model/unet_pretrained.yaml', 'config/model/unet_small.yaml', 'config/old_development/config_test_4channels_implementation.yaml', 'config/optimizer/adam.yaml', 'config/optimizer/adamw.yaml', 'config/optimizer/sgd.yaml', 'config/scheduler/cosine.yaml', 'config/scheduler/cosinewarm.yaml', 'config/scheduler/cyclic.yaml', 'config/scheduler/multi_step_reg.yaml', 'config/scheduler/plateau.yaml', 'config/scheduler/step.yaml', 'config/task/segmentation.yaml', 'config/tracker/mlflow.yaml', 'config/tracker/tensorflow.yaml', 'config/training/default_training.yaml', 'config/travis_CI/config_ci_classification_local.yaml', 'config/travis_CI/config_ci_segmentation_local.yaml', 'config/travis_CI/environment.yml', 'config/v1.1.0/config_canvecaux.yaml', 'config/v1.1.0/config_coordconv.yaml', 'config/v1.1.0/config_metasegm.yaml', 'config/visualization/default_visualization.yaml', 'environment.yml', 'inference_segmentation.py', 'models/model_choice.py', 'sampling_segmentation.py', 'train_classification.py', 'train_segmentation.py', 'utils/augmentation.py', 'utils/create_dataset.py', 'utils/data_analysis.py', 'utils/logger.py', 'utils/optimizer.py', 'utils/utils.py', 'utils/verifications.py']"
37,justinshenk/fer,https://github.com/justinshenk/fer/issues/7,8,"Nice project! Thanks for putting it together!
I found a breaking issue today, with a simple fix.  The versioning strings ask for TensorFlow >=1.14, but there's some internal code that relies on a TF configuration object that was changed in TF2.0.  A workaround is to change the tensorflow requirement strings in setup.py and requirements.txt from ""tensorflow>=1.14"",  to  ""tensorflow>=1.14,<2.0"",
I did a pip install fer in a new virtualenv and then started following the usage instructions. Here's a session dump:
In [1]: import tensorflow    
In [2]: tensorflow.__version__  
Out[2]: '2.0.0'
In [3]: from fer import FER   
In [4]: detector = FER()      
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-2-a308cfd708c0> in <module>
----> 1 detector = FER()

~/td_venv/lib/python3.7/site-packages/fer/fer.py in __init__(self, cascade_file, mtcnn, emotion_model, scale_factor, min_face_size, min_neighbors, offsets, compile)
    110                 ""fer"", ""data/emotion_model.hdf5""
    111             )
--> 112             self.config = tf.ConfigProto(log_device_placement=False)
    113             self.config.gpu_options.allow_growth = True
    114 

AttributeError: module 'tensorflow' has no attribute 'ConfigProto'
Here's a workaround  for TF1/TF2 compatibility that I think would also resolve things.
I'll send a PR later on if I can get something working smoothly. Cheers!",eb6fad46da4406efddd67f6add54fa331ded3bfe,acd3161a1d5e943a70773dca044c26b34a376b84,"['requirements.txt', 'setup.py', 'src/fer/fer.py']"
38,onnx/onnxmltools,https://github.com/onnx/onnxmltools/issues/37,38,"For both of Sequential and Model, Keras parser works correctly with Keras 2.0.9 on Windows Anaconda3 locally. However, when running CI it failed with the following message.
tests/end2end/test_single_operator_with_cntk_backend.py:92: in _test_one_to_one_operator_core
onnx_model = onnxmltools.convert_keras(keras_model)
onnxmltools/convert/main.py:31: in convert_keras
return convert(model, name, initial_types=initial_types, doc_string=doc_string)
onnxmltools/convert/keras/convert.py:30: in convert
topology = parse_keras(model, initial_types)

model = <keras.models.Sequential object at 0x7f7b40cf4908>, initial_types = None
def parse_keras(model, initial_types=None):
raw_model_container = KerasModelContainer(model)
topology = Topology(raw_model_container, default_batch_size=1, initial_types=initial_types)
scope = topology.declare_scope('root')
for node in model.inbound_nodes:
E       AttributeError: 'Sequential' object has no attribute 'inbound_nodes'
onnxmltools/convert/keras/_parse.py:74: AttributeError",4a4b2afead3fa739858fa24d825f5f7e08734868,80e1d0aba201d45ba32542327ab1a63e074a759e,"['onnxmltools/convert/keras/_parse.py', 'requirements-dev.txt']"
39,onnx/onnxmltools,https://github.com/onnx/onnxmltools/issues/66,74,"Since Keras converters are added, we should update doc accordingly.",0ef93a4771c4d23358c6f85076699420daab8f4f,85f6d1cff864f2ae879bfff32259642a3d2e9343,"['README.md', 'README.rst', 'onnxmltools/convert/coreml/_parse.py', 'onnxmltools/convert/keras/_parse.py', 'onnxmltools/convert/sklearn/convert.py']"
40,onnx/onnxmltools,https://github.com/onnx/onnxmltools/issues/124,126,"I made a new model using keras, and saved it to an .hdf file using a callback during training.
I reloaded the model and tried to convert to an ONNX model:
model = keras.models.load_model(filename)
winml_onnx_model = onnxmltools.convert_keras(model)

convert_keras throws an error here. Any idea what I might be doing wrong? I'm on Windows 10, python 3.6.2, Tensorflow 1.8, Keras 2.1.5, onnxmltools 1.2.0.0116, winmltools 1.2.0.0725
The model uses the following operators (but the error seems to be due to the Dropout layer):
Activation, Dropout, BatchNormalization, Convolution2D, MaxPooling2D, GlobalAveragePooling2D
If I do model.summary() everything seems fine with my model. Batch size is always specified as ""None"" in the model summary, so I wonder if there's some issue with how to deal with batch size in the ONNx conversion (this is my first time making an ONNX model)
Thanks!
UPDATE: This simple model gives the same error. I also tried specifying the batch_input_shape explicitly to just allow 1 item in the batch, and it didn't change anything:
model2 = Sequential()
model2.add(Dense(50, batch_input_shape=[1, 5]))
model2.add(Dropout(0.5))
winml_onnx_model2 = onnxmltools.convert_keras(model2)


Error below:
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-15-da007af6c51d> in <module>()
----> 1 winml_onnx_model = onnxmltools.convert_keras(model)

C:\Program Files (x86)\Microsoft Visual Studio\Shared\Anaconda3_64\lib\site-packages\onnxmltools\convert\main.py in convert_keras(model, name, initial_types, doc_string, targeted_onnx, custom_conversion_functions, custom_shape_calculators)
     36     from .keras.convert import convert
     37     return convert(model, name, initial_types,
---> 38                    doc_string, targeted_onnx, custom_conversion_functions, custom_shape_calculators)

C:\Program Files (x86)\Microsoft Visual Studio\Shared\Anaconda3_64\lib\site-packages\onnxmltools\convert\keras\convert.py in convert(model, name, initial_types, doc_string, targeted_onnx, custom_conversion_functions, custom_shape_calculators)
     40     topology = parse_keras(model, initial_types, targeted_onnx, custom_conversion_functions, custom_shape_calculators)
     41 
---> 42     topology.compile()
     43 
     44     if name is None:

C:\Program Files (x86)\Microsoft Visual Studio\Shared\Anaconda3_64\lib\site-packages\onnxmltools\convert\common\_topology.py in compile(self)
    607         self._resolve_duplicates()
    608         self._fix_shapes()
--> 609         self._infer_all_types()
    610         self._check_structure()
    611 

C:\Program Files (x86)\Microsoft Visual Studio\Shared\Anaconda3_64\lib\site-packages\onnxmltools\convert\common\_topology.py in _infer_all_types(self)
    493                 pass  # in Keras converter, the shape calculator can be optional.
    494             else:
--> 495                 operator.infer_types()
    496 
    497     def _resolve_duplicates(self):

C:\Program Files (x86)\Microsoft Visual Studio\Shared\Anaconda3_64\lib\site-packages\onnxmltools\convert\common\_topology.py in infer_types(self)
     95     def infer_types(self):
     96         # Invoke a core inference function
---> 97         _registration.get_shape_calculator(self.type)(self)
     98 
     99 

C:\Program Files (x86)\Microsoft Visual Studio\Shared\Anaconda3_64\lib\site-packages\onnxmltools\convert\common\_registration.py in get_shape_calculator(operator_name)
     66     '''
     67     if operator_name not in _shape_calculator_pool:
---> 68         raise ValueError('Unsupported shape calculation for operator %s' % operator_name)
     69     return _shape_calculator_pool[operator_name]

ValueError: Unsupported shape calculation for operator <class 'keras.layers.core.Dropout'>",727cc8fadfbc17790978bb6c66db461ff62c764d,d3cc8e996d13ce58b66f0eddef3308a3cc6954e1,"['onnxmltools/convert/keras/__init__.py', 'onnxmltools/convert/keras/operator_converters/Dropout.py', 'onnxmltools/convert/keras/operator_converters/__init__.py', 'onnxmltools/convert/keras/shape_calculators/Lazy.py', 'tests/end2end/test_single_operator_with_cntk_backend.py']"
41,onnx/onnxmltools,https://github.com/onnx/onnxmltools/issues/50,70,Keras BLSTM converter should be added.,1a7cd7d745da478324c3f17f57c92e448a661c5c,c22c3cc10442f205d11028a5839e29d54b8c5204,"['onnxmltools/convert/keras/operator_converters/Bidirectional.py', 'onnxmltools/convert/keras/operator_converters/__init__.py', 'onnxmltools/convert/keras/shape_calculators/Bidirectional.py', 'onnxmltools/convert/keras/shape_calculators/LSTM.py', 'onnxmltools/convert/keras/shape_calculators/__init__.py']"
42,ProGamerGov/neural-style-pt,https://github.com/ProGamerGov/neural-style-pt/issues/43,84,"Hi, I noticed this is based on the jcjohnson code, but his has a param called -normalize_gradient tat is iplemented on lines 130-149 ish on https://github.com/jcjohnson/neural-style/blob/master/neural_style.lua
Would you consider porting his lua code of this feature and perhaps other normalization approaches?
Thanks!",cbcd023326a3487a2d75270ed1f3b3ddb4b72407,97081c3853c20684b6abc6e5417e81ad42e39101,"['README.md', 'neural_style.py']"
43,BindsNET/bindsnet,https://github.com/BindsNET/bindsnet/issues/362,364,"Hi BindsNet
I am doing a master project where I am building a neuromorphic processor. I'm am (trying) to use your platform to train a network using Bindsnet, as this is the most promising SNN simulator that stays purely in the spiking domain. Long story to say that I have been playing around supervised MNIST examples, as I would like to use this to test my hardware implementation. But I'm seeing some issues.
If you want to use GPU it crashes in the poisson function in encoding.py. where it expects cuda:0 device but gets cpu. Not sure but I think its the dataloader that needs to be sent to the right device.
Next: The accuracy measurements that I provided in the training loop can not be right. I have tested this the following way. First i train a network over 5000 inputs, using the provided training loop. This loop reports 100% accuracy after about 600 inputs. After training, I turn off training in the network. Then i test the accuacy after training with the following loop:
pbar = tqdm(enumerate(dataloader))
    for (i, datum) in pbar:
        if i > 1000:
            break
        image = datum[""encoded_image""]
        label = datum[""label""]
        inputs = {""X"": image.view(time, 1, 1, 28, 28)}
        network.run(inputs=inputs, time=time)

        out_spikes = network.monitors[""Ae_spikes""].get(""s"")
        class_spike=torch.zeros(10)
        for i in range(time):
            for j in range(n_neurons):
                if out_spikes[i,0,j]:
                    class_spike[j//10] += 1
        
        aVal, maxInd = class_spike.max(0)
        if maxInd == label[0]:
            hit += 1

    
    acc = hit/1000
    print(""\n accuracy: "" + str(acc) +""\n"")
This test returns accuracy around 65% depending on the seed. So what is the accuracy that the training loop is stating? To me, my loop should give the actual accuracy.
Sorry for the long issue! Your platform seems really cool! Therefore it is also a shame that your display piece is not running properly. I hope that you'll take the time to have a look at my complaints

Anthon",6cfcf045930522bc5aa8c69023903988fa3b37d1,2b17f617c4c1e586a5896f3ac57b9398f3f1c555,['examples/mnist/supervised_mnist.py']
44,BindsNET/bindsnet,https://github.com/BindsNET/bindsnet/issues/351,354,"In the supervised_mnist.py -example, the ""All activity accuracy"" and the ""Proportion weighting accuracy"" are calculated every update_interval iterations. Also, the ""assignments"" of labels to the neurons in the excitatory layer are calculated, which are used for plotting.
This would be very handy to keep track of how the training is progressing, were it not for the fact that this seems to not be working accurately.
E.g. if you run the example with default arguments, at the end of the training you still get print-outs like
""All activity accuracy: 2.40 (last), 5.26 (average), 93.20 (best)
Proportion weighting accuracy: 82.00 (last), 18.16 (average), 93.20 (best)"",
while I personally would have assumed it to be a lot better after 5000 training-iterations.
I think the cause of the issue is that the networks predictions are just compared to the label of the one datum for the next iteration instead of to all the labels of all the data of the last update_interval, for which they are the predictions. This also becomes clear if you look at the description of the method ""assign_labels"", which calculates the ""assignments"", ""proportions"" and ""rates"": Here, there is explicitly talk of labels as a vector of size n_samples with data labels corresponding to the spiking activity of the network.
P.S. Thank you so much for the great package, by the way :)",307ee5f36d67c24eea37367d3362567f32cf5dd3,fb8fb084b2bb6c9352ebf01bfd821e95d95be728,"['bindsnet/analysis/plotting.py', 'examples/mnist/supervised_mnist.py']"
45,BindsNET/bindsnet,https://github.com/BindsNET/bindsnet/issues/224,225,"As suggested by Fremaux et al. (2010), learning performance of reward-modulated STDP can be greatly enhanced by using 'reward prediction error' instead of actual reward in the computation of the new weights (w += gamma * dt * reward * e_trace). I tested this for a simple 1D navigation task, with the following result for non-RPE:

And for RPE:

Where blue is the predicted reward for an episode, and red the actual obtained one. The maximum reward that could've been achieved in one episode is 10000, so the RPE-variant got very close.
I implemented the predicted reward as a simple moving average of past episodes (per step, so episode_reward / steps), and fed this to the Network.run. I used Pipeline, which I had to modify in order to be able to modify reward between getting it back from the Gym environment and feeding it to Network.run.
I would like to implement this in BindsNET as well, so my questions are: have you heard of this? Has anyone tried to implement it yet, and how? Do you have a preferred way of implementing?
I think it would be cool if someone can decide for their own how they want to implement the predicted reward. So we might need the ability to pass a function to Pipeline / Network. This might also cover the reward features discussed in #217.",8ebf7b03384ad63495b485cc9fdf90c5ec9a40d3,bd5f83659b3c06484ce858b05d3e104bffbdc244,"['bindsnet/learning/__init__.py', 'bindsnet/learning/reward.py', 'bindsnet/network/__init__.py', 'bindsnet/network/topology.py', 'bindsnet/pipeline/__init__.py']"
46,blue-oil/blueoil,https://github.com/blue-oil/blueoil/issues/904,978,"The configuration for image classification in the tutorial [Training for Basic Classification]https://docs.blue-oil.org/tutorial/image_cls.html is not working.
When execute blueoil train -c config/my_config.yml either in local or Docker got this error and the training is halted at 0 epoch.
.
.
.
Exception in thread Thread-1:
multiprocessing.pool.RemoteTraceback:
""""""
Traceback (most recent call last):
  File ""/usr/local/pyenv/versions/3.6.3/lib/python3.6/multiprocessing/pool.py"", line 119, in worker
    result = (True, func(*args, **kwds))
  File ""/usr/local/pyenv/versions/3.6.3/lib/python3.6/multiprocessing/pool.py"", line 44, in mapstar
    return list(map(*args))
  File ""/home/blueoil/blueoil/datasets/dataset_iterator.py"", line 81, in _process_one_data
    return _apply_augmentations(_dataset, image, label)
  File ""/home/blueoil/blueoil/datasets/dataset_iterator.py"", line 56, in _apply_augmentations
    sample = augmentor(**sample)
  File ""/home/blueoil/blueoil/data_processor.py"", line 41, in __call__
    kwargs = processor(**kwargs)
  File ""/home/blueoil/blueoil/data_augmentor.py"", line 646, in __call__
    num_max_boxes = len(gt_boxes)
TypeError: object of type 'NoneType' has no len()
""""""

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""/usr/local/pyenv/versions/3.6.3/lib/python3.6/threading.py"", line 916, in _bootstrap_inner
    self.run()
  File ""/home/blueoil/blueoil/datasets/dataset_iterator.py"", line 168, in run
    self.loop_body()
  File ""/home/blueoil/blueoil/datasets/dataset_iterator.py"", line 145, in loop_body
    fetch_result = self.pool.map(_process_one_data, task_list)
  File ""/usr/local/pyenv/versions/3.6.3/lib/python3.6/multiprocessing/pool.py"", line 266, in map
    return self._map_async(func, iterable, mapstar, chunksize).get()
  File ""/usr/local/pyenv/versions/3.6.3/lib/python3.6/multiprocessing/pool.py"", line 644, in get
    raise self._value
TypeError: object of type 'NoneType' has no len()
.
.
.
 0/78125 [..............................] - ETA: 0s",da10cc81ceef8d7f71ce7d3e4104c9d1a1252f54,3ebc0b4f50505fe3c2666be069b9699bb74a5633,['docs/tutorial/image_cls.md']
47,blue-oil/blueoil,https://github.com/blue-oil/blueoil/issues/362,364,"Background
I've been working with the new blueoil/lmnet code and found out that the kernel_initializer is not passed in conv2D wrapper function defined in lmnet/lmnet/layers/layers.py. The kernel_initializer argument in darknet block function in lmnet/lmnet/blocks.py provided but it is not passed into the actual tf.layers.conv2d function inside the wrapper function conv2D.

lmnet/lmnet/blocks.py

# TODO(wakisaka): should be replace to conv_bn_act().
def darknet(name, inputs, filters, kernel_size,
            is_training=tf.constant(False), activation=None, data_format=""NHWC"", *args, **kwargs):
    """"""Darknet19 block.

    Do convolution, batch_norm, bias, leaky_relu activation.
    Ref: https://arxiv.org/pdf/1612.08242.pdf
         https://github.com/pjreddie/darknet/blob/3bf2f342c03b0ad22efd799d5be9990c9d792354/cfg/darknet19.cfg
         https://github.com/pjreddie/darknet/blob/8215a8864d4ad07e058acafd75b2c6ff6600b9e8/cfg/yolo.2.0.cfg
    """"""
    if data_format == ""NCHW"":
        channel_data_format = ""channels_first""
    elif data_format == ""NHWC"":
        channel_data_format = ""channels_last""
    else:
        raise ValueError(""data format must be 'NCHW' or 'NHWC'. got {}."".format(data_format))

    with tf.variable_scope(name):
        if activation is None:
            def activation(x): return tf.nn.leaky_relu(x, alpha=0.1, name=""leaky_relu"")

        conv = conv2d(""conv"", inputs, filters=filters, kernel_size=kernel_size,
                      activation=None, use_bias=False, data_format=channel_data_format,
                      kernel_initializer=tf.contrib.layers.variance_scaling_initializer(),
                      *args, **kwargs)  # he initializer

        # TODO(wakisaka): Should be the same as darknet batrch norm.
        # https://github.com/tensorflow/tensorflow/blob/r1.1/tensorflow/contrib/layers/python/layers/layers.py
        # https://github.com/pjreddie/darknet/blob/8215a8864d4ad07e058acafd75b2c6ff6600b9e8/src/batchnorm_layer.c#L135
        batch_normed = batch_norm(""bn"", conv, is_training=is_training, decay=0.99, scale=True, center=True,
                                  data_format=data_format)
        tf.summary.histogram(""batch_normed"", batch_normed)

        output = activation(batch_normed)
        tf.summary.histogram(""output"", output)

        return output


lmnet/lmnet/layers/layers.py

def conv2d(
    name,
    inputs,
    filters,
    kernel_size,
    strides=1,
    padding=""SAME"",
    activation=tf.nn.relu,
    kernel_initializer=tf.contrib.layers.xavier_initializer(),
    is_debug=False,
    *args,
    **kwargs
):

    output = tf.layers.conv2d(
        name=name,
        inputs=inputs,
        filters=filters,
        kernel_size=kernel_size,
        strides=strides,
        padding=padding,
        activation=activation,
        *args,
        **kwargs,
    )

    if is_debug:
        tf.summary.histogram(name + ""/output"", output)

    return output


In my opinion, kernel_regularizer argument in conv2D function should not be defined explicitly. If defined in the calling function it will be automatically passed via *args, **kwargs argument extension. i.e.

Solution

def conv2d(
    name,
    inputs,
    filters,
    kernel_size,
    strides=1,
    padding=""SAME"",
    activation=tf.nn.relu,
    is_debug=False,
    *args,
    **kwargs
):

    output = tf.layers.conv2d(
        name=name,
        inputs=inputs,
        filters=filters,
        kernel_size=kernel_size,
        strides=strides,
        padding=padding,
        activation=activation,
        *args,
        **kwargs,
    )

    if is_debug:
        tf.summary.histogram(name + ""/output"", output)

    return output",f1e3b43226c7fd9910d1b5ad831fa63d3660d1b3,d259f3ca2c1c620f139183c6353b6c6c1758c7f2,"['blueoil/blocks.py', 'blueoil/layers/layers.py']"
48,blue-oil/blueoil,https://github.com/blue-oil/blueoil/issues/909,957,"An error occurs when we run lm_fpga.elf many times.
The probability that the problem occurs is around 1/10000.
In training, I almost followed the blueoil tutorial and modified some parameters.
The actual config is below.
# -*- coding: utf-8 -*-
# Copyright 2018 The Blueoil Authors. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# =============================================================================
from easydict import EasyDict
import tensorflow as tf

from blueoil.common import Tasks
from blueoil.networks.classification.lmnet_v1 import LmnetV1Quantize
from blueoil.datasets.image_folder import ImageFolderBase

from blueoil.data_processor import Sequence
from blueoil.pre_processor import (
    Resize,
    DivideBy255,
    PerImageStandardization
)
from blueoil.quantizations import (
    binary_mean_scaling_quantizer,
    linear_mid_tread_half_quantizer,
)

IS_DEBUG = False

NETWORK_CLASS = LmnetV1Quantize

# TODO(wakisaka): should be hidden. generate dataset class on the fly.
DATASET_CLASS = type('DATASET_CLASS', (ImageFolderBase,), {'extend_dir': '/home/blueoil/cifar/train/', 'validation_extend_dir': '/home/blueoil/cifar/test/'})

IMAGE_SIZE = [32, 32]
BATCH_SIZE = 64
DATA_FORMAT = ""NHWC""
TASK = Tasks.CLASSIFICATION
CLASSES = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']

MAX_EPOCHS = 10
SAVE_CHECKPOINT_STEPS = 1000
KEEP_CHECKPOINT_MAX = 5
TEST_STEPS = 1000
SUMMARISE_STEPS = 100


# pretrain
IS_PRETRAIN = False
PRETRAIN_VARS = []
PRETRAIN_DIR = """"
PRETRAIN_FILE = """"

PRE_PROCESSOR = Sequence([
    Resize(size=IMAGE_SIZE),
    PerImageStandardization()
])
POST_PROCESSOR = None

NETWORK = EasyDict()

NETWORK.OPTIMIZER_CLASS = tf.train.MomentumOptimizer
NETWORK.OPTIMIZER_KWARGS = {'momentum': 0.9, 'learning_rate': 0.001}
NETWORK.LEARNING_RATE_FUNC = None
NETWORK.LEARNING_RATE_KWARGS = None

NETWORK.IMAGE_SIZE = IMAGE_SIZE
NETWORK.BATCH_SIZE = BATCH_SIZE
NETWORK.DATA_FORMAT = DATA_FORMAT
NETWORK.WEIGHT_DECAY_RATE = 0.0005

# quantize
NETWORK.ACTIVATION_QUANTIZER = linear_mid_tread_half_quantizer
NETWORK.ACTIVATION_QUANTIZER_KWARGS = {
    'bit': 2,
    'max_value': 2
}
NETWORK.WEIGHT_QUANTIZER = binary_mean_scaling_quantizer
NETWORK.WEIGHT_QUANTIZER_KWARGS = {}

# dataset
DATASET = EasyDict()
DATASET.BATCH_SIZE = BATCH_SIZE
DATASET.DATA_FORMAT = DATA_FORMAT
DATASET.PRE_PROCESSOR = PRE_PROCESSOR
DATASET.AUGMENTOR = Sequence([])
DATASET.ENABLE_PREFETCH = True

The probability of the problem was 19/100000.
There're several patterns of error.
I show the 1 succeeded and 3 failed patterns below.
-------------------------------------------------------------
Comparison: Default network test  succeeded!!!
-------------------------------------------------------------
TotalInitTime 8094,  sum:8.094ms
TotalRunTime 7283,  sum:7.283ms
..Convolution 3750,72,  sum:3.822ms
....kn2row 3656,  sum:3.656ms
......kn2row-buf 6,  sum:0.006ms
......matrix_multiplication 464,429,423,417,  sum:1.733ms
........matrix_transpose (row_major) 26,25,21,18,  sum:0.09ms
......matrix_shift_add_f 580,434,425,420,  sum:1.859ms
....kn2row-1x1 63,  sum:0.063ms
......matrix_multiplication 50,  sum:0.05ms
..BatchNorm 367,17,  sum:0.384ms
..QTZ_linear_mid_tread_half 220,  sum:0.22ms
....pack_input 50,  sum:0.05ms
..QuantizedConv2D 552,922,319,389,124,  sum:2.306ms
....Convert Tensor 26,24,15,11,14,  sum:0.09ms
....Sync UDMABuf Input 100,77,43,29,28,  sum:0.277ms
....Conv2D TCA 337,768,215,300,42,  sum:1.662ms
....Sync UDMABuf Output 58,31,23,26,20,  sum:0.158ms
..Memcpy 76,24,18,17,  sum:0.135ms
..ExtractImagePatches 69,13,13,  sum:0.095ms
..QuantizedConv2D_ApplyScalingFactor 38,  sum:0.038ms
..ReLu 18,  sum:0.018ms
..Add 11,  sum:0.011ms
..AveragePool 23,  sum:0.023ms
..SoftMax 120,  sum:0.12ms

-------------------------------------------------------------
Comparison: Default network test  failed...
Failed count: 5
First failed report
index: 2 / 10
input: 0.00456843, expected: 0.00425674

-------------------------------------------------------------
TotalInitTime 8027,  sum:8.027ms
TotalRunTime 7512,  sum:7.512ms
..Convolution 3978,72,  sum:4.05ms
....kn2row 3877,  sum:3.877ms
......kn2row-buf 6,  sum:0.006ms
......matrix_multiplication 462,431,428,424,  sum:1.745ms
........matrix_transpose (row_major) 24,27,26,24,  sum:0.101ms
......matrix_shift_add_f 627,484,487,468,  sum:2.066ms
....kn2row-1x1 63,  sum:0.063ms
......matrix_multiplication 50,  sum:0.05ms
..BatchNorm 354,17,  sum:0.371ms
..QTZ_linear_mid_tread_half 233,  sum:0.233ms
....pack_input 50,  sum:0.05ms
..QuantizedConv2D 635,922,320,331,119,  sum:2.327ms
....Convert Tensor 24,24,16,12,13,  sum:0.089ms
....Sync UDMABuf Input 194,77,44,29,24,  sum:0.368ms
....Conv2D TCA 331,768,215,254,41,  sum:1.609ms
....Sync UDMABuf Output 58,31,23,18,20,  sum:0.15ms
..Memcpy 67,24,18,12,  sum:0.121ms
..ExtractImagePatches 69,13,12,  sum:0.094ms
..QuantizedConv2D_ApplyScalingFactor 35,  sum:0.035ms
..ReLu 17,  sum:0.017ms
..Add 10,  sum:0.01ms
..AveragePool 22,  sum:0.022ms
..SoftMax 118,  sum:0.118ms

-------------------------------------------------------------
Comparison: Default network test  failed...
Failed count: 6
First failed report
index: 1 / 10
input: 8.70602e-05, expected: 7.18778e-05

-------------------------------------------------------------
TotalInitTime 8064,  sum:8.064ms
TotalRunTime 7685,  sum:7.685ms
..Convolution 4225,71,  sum:4.296ms
....kn2row 4124,  sum:4.124ms
......kn2row-buf 6,  sum:0.006ms
......matrix_multiplication 464,430,547,423,  sum:1.864ms
........matrix_transpose (row_major) 25,26,21,20,  sum:0.092ms
......matrix_shift_add_f 645,514,549,486,  sum:2.194ms
....kn2row-1x1 62,  sum:0.062ms
......matrix_multiplication 49,  sum:0.049ms
..BatchNorm 382,17,  sum:0.399ms
..QTZ_linear_mid_tread_half 230,  sum:0.23ms
....pack_input 48,  sum:0.048ms
..QuantizedConv2D 538,921,319,331,118,  sum:2.227ms
....Convert Tensor 24,24,15,11,12,  sum:0.086ms
....Sync UDMABuf Input 98,75,43,28,24,  sum:0.268ms
....Conv2D TCA 330,769,215,254,41,  sum:1.609ms
....Sync UDMABuf Output 57,31,24,18,19,  sum:0.149ms
..Memcpy 73,25,18,12,  sum:0.128ms
..ExtractImagePatches 67,14,12,  sum:0.093ms
..QuantizedConv2D_ApplyScalingFactor 37,  sum:0.037ms
..ReLu 17,  sum:0.017ms
..Add 10,  sum:0.01ms
..AveragePool 22,  sum:0.022ms
..SoftMax 114,  sum:0.114ms

-------------------------------------------------------------
Comparison: Default network test  failed...
Failed count: 6
First failed report
index: 0 / 10
input: 5.93729e-05, expected: 4.80257e-05

-------------------------------------------------------------
TotalInitTime 8064,  sum:8.064ms
TotalRunTime 7506,  sum:7.506ms
..Convolution 4082,72,  sum:4.154ms
....kn2row 3983,  sum:3.983ms
......kn2row-buf 6,  sum:0.006ms
......matrix_multiplication 463,428,422,593,  sum:1.906ms
........matrix_transpose (row_major) 25,24,19,19,  sum:0.087ms
......matrix_shift_add_f 612,457,441,498,  sum:2.008ms
....kn2row-1x1 63,  sum:0.063ms
......matrix_multiplication 50,  sum:0.05ms
..BatchNorm 384,17,  sum:0.401ms
..QTZ_linear_mid_tread_half 233,  sum:0.233ms
....pack_input 47,  sum:0.047ms
..QuantizedConv2D 539,922,317,330,117,  sum:2.225ms
....Convert Tensor 24,24,15,11,12,  sum:0.086ms
....Sync UDMABuf Input 99,77,43,29,24,  sum:0.272ms
....Conv2D TCA 330,768,215,254,41,  sum:1.608ms
....Sync UDMABuf Output 58,31,23,17,19,  sum:0.148ms
..Memcpy 70,24,18,12,  sum:0.124ms
..ExtractImagePatches 67,13,12,  sum:0.092ms
..QuantizedConv2D_ApplyScalingFactor 36,  sum:0.036ms
..ReLu 17,  sum:0.017ms
..Add 10,  sum:0.01ms
..AveragePool 22,  sum:0.022ms
..SoftMax 78,  sum:0.078ms

Of course these results should be same because I just repeated lm_fpga.elf with same condition.
What is the problem ?",8acb3c4564091e7cf7cf67afb43e0f300f0ea0b7,a013850ea52f6c31213b4a2684340b50d71f9b0d,['blueoil/converter/templates/include/dma_buffer.h']
49,blue-oil/blueoil,https://github.com/blue-oil/blueoil/issues/138,28,"Problem
Current Blueoil use Momentum optimizer. I also want to use Adam optimizer for some experiments.
How to implement
Current implementation is below. MomentumOptimizer is hard coded.
Classification
https://github.com/blue-oil/blueoil/blob/master/blueoil/templates/lmnet/classification.tpl.py#L70
Object Detection
https://github.com/blue-oil/blueoil/blob/master/blueoil/templates/lmnet/object_detection.tpl.py#L90
I want to add the choice to blueoil_init.py. And also add test to blueoil_test.sh.",4d07e4f3cd4685968bd56ec01f1cf471deae921d,ca7d228fc1ff1e536220b1c013428463dd5291f3,"['blueoil/blueoil_init.py', 'blueoil/generate_lmnet_config.py', 'blueoil/templates/blueoil-config.tpl.yml', 'blueoil/templates/lmnet/classification.tpl.py', 'blueoil/templates/lmnet/object_detection.tpl.py', 'blueoil_test.sh', 'docs/usage/init.md', 'tests/config/caltech101_classification.yml', 'tests/config/caltech101_classification_has_validation.yml', 'tests/config/delta_mark_classification.yml', 'tests/config/delta_mark_classification_has_validation.yml', 'tests/config/delta_mark_object_detection.yml', 'tests/config/delta_mark_object_detection_has_validation.yml', 'tests/config/make_yml_config.py', 'tests/config/openimagesv4_object_detection.yml', 'tests/config/openimagesv4_object_detection_has_validation.yml']"
50,blue-oil/blueoil,https://github.com/blue-oil/blueoil/issues/484,1072,"We're depending on pandas, but actually it's used for just reading CSV or json files.
To reduce disk image size, we should remove this dependency.",53dae5a1468a0d3ccfab142e16797fd35de74d7d,43bd2e13280a5496fffbdc86791cfd64786d0b23,"['blueoil/cmd/output_event.py', 'blueoil/datasets/camvid.py', 'blueoil/datasets/ilsvrc_2012.py', 'blueoil/datasets/pascalvoc_2007.py', 'blueoil/datasets/pascalvoc_base.py', 'setup.cfg']"
51,blue-oil/blueoil,https://github.com/blue-oil/blueoil/issues/884,882,"The configuration file from blueoil init is identical when
apply quantization at the first layer? -> yes
and
apply quantization at the first layer? -> no",56115101289b935b6d0e9f80a36a53fc0511ed8c,f6552dfde963bef84bc85e9d07e0c50f8087fecf,"['blueoil/cmd/init.py', 'setup.cfg']"
52,blue-oil/blueoil,https://github.com/blue-oil/blueoil/issues/687,689,"We need to update TF version.
https://github.com/tensorflow/tensorflow/releases/tag/v1.15.0
Check list

 Update numpy version
 Handle FusedBatchNormV3 error
 Update GCC version compatible with TF 1.15
 Update TF version",dbc39f34eec7c1db9fd98649e3d91bd64a23eee5,33800836fa708d2f5520757388f3e8ec983223bf,['requirements.txt']
53,blue-oil/blueoil,https://github.com/blue-oil/blueoil/issues/805,807,"We cannot set trial parameter, the position is incorrect.



blueoil/output_template/python/run.py


         Line 119
      in
      056290f






 def run_prediction(input_image, model, config_file, max_percent_incorrect_values=0.1, trial=1): 








blueoil/output_template/python/run.py


         Line 211
      in
      056290f






 run_prediction(input_image, model, config_file, trial) 





And also max_percent_incorrect_values is not used anywhere.",c8cd51bc6e5a6c06582545b695cd8b27ec180b29,a0c5572caf5db74d5ec51ba8e915849bd59da456,['output_template/python/run.py']
54,blue-oil/blueoil,https://github.com/blue-oil/blueoil/issues/711,719,"Adding cosine_decay is reasonable, I think.
https://www.tensorflow.org/api_docs/python/tf/compat/v1/train/cosine_decay",5391d75d8e1e1004d2d7bcbd4226ea6940442cc7,3139cb8fa0f2ca83f356cec36e19b0f04f002a62,"['blueoil/cmd/init.py', 'blueoil/generate_lmnet_config.py']"
55,blue-oil/blueoil,https://github.com/blue-oil/blueoil/issues/249,886,"Error comes when I ran LMBiSeNet inference test on x86 and FPGA (De10-Nano)
Run lm_x86.elf on x86 server.
$  ./models/lib/lm_x86.elf ../inference_test_data/000_images_placeholder:0.npy ../inference_test_data/731_output:0.npy
-------------------------------------------------------------
Comparison: Default network test  failed...
Failed count: 1
First failed report
index: 956 / 29040
input: -22.6264, expected: -22.6264

-------------------------------------------------------------
Add,114,  sum:0.114ms
ApplyThresholds,3717,1687,681,1554,605,1391,610,1397,700,224,583,156,760,152,1775,1323,639,285,277,255,301,253,294,623,1240,7828,5789,2925,  sum:38.024ms
AveragePool,444,440,468,  sum:1.352ms
BatchNorm,6792,169,10,10,201,1,1,  sum:7.184ms
Convolution,37422,9,7,2141,  sum:39.579ms
ExtractImagePatches,8363,479,176,124,128,77,4639,1245,742,  sum:15.973ms
MulDepthWise,139,136,184,  sum:0.459ms
QTZ_linear_mid_tread_half,15306,84,433,  sum:15.823ms
QuantizedConv2D,14459,492,5574,  sum:20.525ms
QuantizedConv2D_ApplyScalingFactor,234,2,488,  sum:0.724ms
ReLu,107,1,103,0,0,  sum:0.211ms
TotalInitTime,34190,  sum:34.19ms
TotalRunTime,650778,  sum:650.778ms
func_ConcatOnDepth,672,724,881,289,262,312,151,217,249,448,932,  sum:5.137ms
kn2row-1x1,37420,8,7,2141,  sum:39.576ms
matrix_multiplication,37417,7,6,2140,  sum:39.57ms
matrix_shift_add1,174,151,154,133,65,71,194,209,194,623,658,501,  sum:3.127ms
matrix_shift_add2,513,418,438,201,93,100,87,90,83,5883,2981,955,  sum:11.842ms
pack_input_to_qwords,10964,565,253,598,240,677,218,798,229,114,257,96,240,105,287,218,232,105,46,150,46,200,46,231,1,184,92,6098,1481,1105,833,  sum:26.709ms
quantized-kn2row,73338,4649,10166,5326,8029,6875,6595,13503,2929,3711,3468,3696,3388,3400,6802,2894,9418,2262,8196,3844,7550,3779,10506,14443,490,3884,2133,115314,101021,115130,5557,  sum:562.296ms
quantized_matrix_multiplication,58593,2388,7319,3158,5879,4793,5050,11292,1990,2967,2612,3251,2379,2934,4730,1347,8508,1859,7469,3424,6805,3305,9796,14138,413,3069,797,86078,83581,108092,4718,  sum:462.734ms
quantized_ohwi_to_hwoi,16,1,10,8,2,8,2,8,2,10,9,2,2,2,3,1,29,4,23,6,29,10,23,53,49,1,0,2,5,20,1,  sum:0.341ms

I don't know why input: -22.6264, expected: -22.6264 is fail, It looks same values...
Run on FPGA.
$ root@DE10_NANO:~/lm_bisenet_camvid/export/save.ckpt-18350/352x480/output# ./models/lib/lm_fpga.elf ../inference_test_data/000_images_placeholder\:0.npy ../inference_test_data/731_output\:0.npy
-------------------------------------------------------------
Comparison: Default network test  failed...
Failed count: 29040
First failed report
index: 0 / 29040
input: -7.08966, expected: -7.80849

-------------------------------------------------------------
Add,2436,  sum:2.436ms
AveragePool,7456,7367,6627,  sum:21.45ms
BatchNorm,44120,2242,233,261,1394,68,26,  sum:48.344ms
Convolution,61820,86,81,15997,  sum:77.984ms
ExtractImagePatches,20091,2061,557,587,561,326,26626,6741,5247,  sum:62.797ms
MulDepthWise,2249,2175,2202,  sum:6.626ms
Packing input for kn2row,21586,1349,596,1762,598,2042,595,2382,595,246,656,196,710,240,880,594,632,267,112,290,119,519,97,790,16,597,288,21544,5509,4128,3142,  sum:73.077ms
QConv2D kn2row tiling,9363,1028,2008,1361,1961,1260,1963,3126,902,842,1059,803,1111,798,1907,959,3788,1273,4335,1633,4334,1994,4336,8108,6779,1007,965,29860,21581,17550,1584,  sum:139.578ms
QTZ_linear_mid_tread_half,52166,354,2005,  sum:54.525ms
QuantizedConv2D,10901,6917,7025,  sum:24.843ms
QuantizedConv2D_ApplyScalingFactor,4113,35,3701,  sum:7.849ms
ReLu,1798,14,1772,4,5,  sum:3.593ms
TotalInitTime,278156,  sum:278.156ms
TotalRunTime,646152,  sum:646.152ms
func_ConcatOnDepth,5201,6166,7087,2178,2334,2568,1343,1786,2237,4452,9140,  sum:44.492ms
kn2row-1x1,61808,78,75,15990,  sum:77.951ms
matrix_multiplication,61795,69,68,15977,  sum:77.909ms
matrix_transpose (row_major),96,  sum:0.096ms
pack_input_to_qwords,21569,1332,582,1750,581,2030,583,2371,577,236,647,186,700,231,870,585,577,257,104,281,110,511,89,781,9,587,279,21530,5495,4116,3128,  sum:72.684ms

The results are significant difference from expected. Need to fix.
More details are in #225",507fb9f0de9110942cf2174e71472586f5afe011,56115101289b935b6d0e9f80a36a53fc0511ed8c,['blueoil/networks/segmentation/lm_bisenet.py']
56,medipixel/rl_algorithms,https://github.com/medipixel/rl_algorithms/issues/261,272,"Reference:
https://github.com/cyoon1729/RLcycle/blob/38a50a102d3f770ce99e0df7847d3199e556376e/rlcycle/common/utils/common_utils.py#L8-L12",3369e6e4efe22855d8822bb8f019035b621934c9,d3357843ff783b90c721440e904bbfaef93e0a00,"['.all-contributorsrc', 'README.md', 'configs/pong_no_frameskip_v4/apex_dqn.py', 'requirements.txt', 'rl_algorithms/a2c/agent.py', 'rl_algorithms/a2c/learner.py', 'rl_algorithms/bc/ddpg_agent.py', 'rl_algorithms/bc/ddpg_learner.py', 'rl_algorithms/bc/sac_agent.py', 'rl_algorithms/bc/sac_learner.py', 'rl_algorithms/common/abstract/agent.py', 'rl_algorithms/common/abstract/distributed_logger.py', 'rl_algorithms/common/abstract/distributed_worker.py', 'rl_algorithms/common/abstract/learner.py', 'rl_algorithms/common/apex/architecture.py', 'rl_algorithms/common/apex/learner.py', 'rl_algorithms/common/buffer/distillation_buffer.py', 'rl_algorithms/common/helper_functions.py', 'rl_algorithms/ddpg/agent.py', 'rl_algorithms/ddpg/learner.py', 'rl_algorithms/distillation/dqn_agent.py', 'rl_algorithms/dqn/agent.py', 'rl_algorithms/dqn/distributed_worker.py', 'rl_algorithms/dqn/learner.py', 'rl_algorithms/dqn/linear.py', 'rl_algorithms/dqn/networks.py', 'rl_algorithms/fd/ddpg_agent.py', 'rl_algorithms/fd/ddpg_learner.py', 'rl_algorithms/fd/dqn_agent.py', 'rl_algorithms/fd/dqn_learner.py', 'rl_algorithms/fd/sac_agent.py', 'rl_algorithms/fd/sac_learner.py', 'rl_algorithms/ppo/agent.py', 'rl_algorithms/ppo/learner.py', 'rl_algorithms/recurrent/dqn_agent.py', 'rl_algorithms/recurrent/learner.py', 'rl_algorithms/sac/agent.py', 'rl_algorithms/sac/learner.py', 'rl_algorithms/td3/agent.py', 'rl_algorithms/td3/learner.py', 'setup.py']"
57,medipixel/rl_algorithms,https://github.com/medipixel/rl_algorithms/issues/129,140,We need check performace TD3 algorithm because not as good as before.,791ebd79ab004a23d97127fce5f81de30a1a8dcb,ca729e054ce9d9fcbdb4cfc6c79f75214e223089,"['README.md', 'algorithms/common/noise.py', 'algorithms/td3/agent.py', 'examples/lunarlander_continuous_v2/td3.py', 'examples/reacher_v2/td3.py']"
58,medipixel/rl_algorithms,https://github.com/medipixel/rl_algorithms/issues/243,272,"This part is to change numpy array type to torch float tensor type:



rl_algorithms/rl_algorithms/dqn/agent.py


        Lines 137 to 140
      in
      990c78a






 def _preprocess_state(self, state: np.ndarray) -> torch.Tensor: 



 """"""Preprocess state so that actor selects an action."""""" 



 state = torch.FloatTensor(state).to(device) 



 return state 





Use this method:



rl_algorithms/rl_algorithms/common/helper_functions.py


        Lines 93 to 102
      in
      990c78a






 def numpy2floattensor(arrays: Tuple[np.ndarray]) -> Tuple[np.ndarray]: 



 """"""Convert numpy arrays to torch float tensor."""""" 



 tensors = [] 



 for array in arrays: 



 tensor = torch.FloatTensor(array).to(device) 



 if torch.cuda.is_available(): 



 tensor = tensor.cuda(non_blocking=True) 



 tensors.append(tensor) 



 



 return tuple(tensors)",3369e6e4efe22855d8822bb8f019035b621934c9,d3357843ff783b90c721440e904bbfaef93e0a00,"['.all-contributorsrc', 'README.md', 'configs/pong_no_frameskip_v4/apex_dqn.py', 'requirements.txt', 'rl_algorithms/a2c/agent.py', 'rl_algorithms/a2c/learner.py', 'rl_algorithms/bc/ddpg_agent.py', 'rl_algorithms/bc/ddpg_learner.py', 'rl_algorithms/bc/sac_agent.py', 'rl_algorithms/bc/sac_learner.py', 'rl_algorithms/common/abstract/agent.py', 'rl_algorithms/common/abstract/distributed_logger.py', 'rl_algorithms/common/abstract/distributed_worker.py', 'rl_algorithms/common/abstract/learner.py', 'rl_algorithms/common/apex/architecture.py', 'rl_algorithms/common/apex/learner.py', 'rl_algorithms/common/buffer/distillation_buffer.py', 'rl_algorithms/common/helper_functions.py', 'rl_algorithms/ddpg/agent.py', 'rl_algorithms/ddpg/learner.py', 'rl_algorithms/distillation/dqn_agent.py', 'rl_algorithms/dqn/agent.py', 'rl_algorithms/dqn/distributed_worker.py', 'rl_algorithms/dqn/learner.py', 'rl_algorithms/dqn/linear.py', 'rl_algorithms/dqn/networks.py', 'rl_algorithms/fd/ddpg_agent.py', 'rl_algorithms/fd/ddpg_learner.py', 'rl_algorithms/fd/dqn_agent.py', 'rl_algorithms/fd/dqn_learner.py', 'rl_algorithms/fd/sac_agent.py', 'rl_algorithms/fd/sac_learner.py', 'rl_algorithms/ppo/agent.py', 'rl_algorithms/ppo/learner.py', 'rl_algorithms/recurrent/dqn_agent.py', 'rl_algorithms/recurrent/learner.py', 'rl_algorithms/sac/agent.py', 'rl_algorithms/sac/learner.py', 'rl_algorithms/td3/agent.py', 'rl_algorithms/td3/learner.py', 'setup.py']"
59,medipixel/rl_algorithms,https://github.com/medipixel/rl_algorithms/issues/245,246,"Create base classes for distributed training
Implement general framework and components for Ape-X
Implement Ape-X DQN",9e897adfe93600c1db85ce1a7e064064b025c2c3,07743f64f37ff5781b547d1c89d820892615d35c,"['LICENSE.md', 'README.md', 'configs/pong_no_frameskip_v4/apex_dqn.py', 'requirements.txt', 'rl_algorithms/__init__.py', 'rl_algorithms/common/abstract/architecture.py', 'rl_algorithms/common/abstract/distributed_logger.py', 'rl_algorithms/common/abstract/learner.py', 'rl_algorithms/common/abstract/worker.py', 'rl_algorithms/common/distributed/__init__.py', 'rl_algorithms/common/distributed/apex.py', 'rl_algorithms/common/distributed/buffer.py', 'rl_algorithms/common/distributed/learner.py', 'rl_algorithms/common/distributed/worker.py', 'rl_algorithms/common/helper_functions.py', 'rl_algorithms/dqn/agent.py', 'rl_algorithms/dqn/learner.py', 'rl_algorithms/dqn/logger.py', 'rl_algorithms/dqn/worker.py', 'rl_algorithms/registry.py', 'rl_algorithms/utils/__init__.py', 'rl_algorithms/utils/registry.py', 'run_lunarlander_continuous_v2.py', 'run_lunarlander_v2.py', 'run_pong_no_frameskip_v4.py', 'run_reacher_v2.py', 'tests/integration/test_run_agent.py', 'tests/integration/test_run_apex.py']"
60,ludwig-ai/ludwig,https://github.com/ludwig-ai/ludwig/issues/1403,1410,"The eval batch size is only set once at the beginning of training. However, the train batch size can scale up during training. When this happens, we should also increase the eval batch size, if possible, as any batch size that works for training should also work for inference.
https://github.com/ludwig-ai/ludwig/blob/master/ludwig/models/trainer.py#L909
Specifically:
eval_batch_size = max(eval_batch_size, progress_tracker.batch_size)

See #1389.
cc @carlogrisetti",a06fd574bebc5b343f671576ab4d30cb3d50f4e3,5300ebb4d2d9d385ce2c3506c54c579d93c83d43,"['ludwig/api.py', 'ludwig/constants.py', 'ludwig/models/trainer.py', 'ludwig/utils/defaults.py', 'tests/integration_tests/test_trainer.py']"
61,ludwig-ai/ludwig,https://github.com/ludwig-ai/ludwig/issues/1911,1912,"Describe the bug
When using the LudwigModel.evaluate() method, number input feature normalization does not occur.  This failure manifest itself with incorrect predicted values, i.e., incorrect predicted probabilities for a binary output feature.  This code illustrates the issue:
import os
import shutil

from ludwig.datasets import adult_census_income
from ludwig.evaluate import evaluate_cli


if __name__ == ""__main__"":
    # Loads the dataset as a pandas.DataFrame
    train_df, test_df, _ = adult_census_income.load(split=True, cache_dir=""adult_census_income_data"")

    import pandas as pd
    import numpy as np

    # setup pandas output options
    pd.set_option('display.max_colwidth', 50)
    pd.set_option(""display.width"", 120)
    pd.set_option(""display.max_columns"", 10)

    # make reproducible output sample
    np.random.seed(31)

    # Print sample of trainig and test data sets.
    print(f""Training Dataset(train_df):\n{train_df.sample(n=5)}"")
    print(f""\nTest Dataset(test_df):\n{test_df.sample(n=5)}"")

    import logging
    from ludwig.api import LudwigModel

    # define model configuration
    config = {'combiner': {'dropout': 0.2,
                  'num_fc_layers': 3,
                  'output_size': 128,
                  'type': 'concat'},
     'input_features': [{'name': 'age', 'type': 'number'},
                        {'name': 'workclass', 'type': 'category'},
                        {'name': 'fnlwgt', 'type': 'number'},
                        {'name': 'education', 'type': 'category'},
                        {'name': 'education-num', 'type': 'number'},
                        {'name': 'marital-status', 'type': 'category'},
                        {'name': 'occupation', 'type': 'category'},
                        {'name': 'relationship', 'type': 'category'},
                        {'name': 'race', 'type': 'category'},
                        {'name': 'sex', 'type': 'category'},
                        {'name': 'capital-gain', 'type': 'number'},
                        {'name': 'capital-loss', 'type': 'number'},
                        {'name': 'hours-per-week', 'type': 'number'},
                        {'name': 'native-country', 'type': 'category'}],
     'output_features': [{'name': 'income',
                          'num_fc_layers': 4,
                          'output_size': 32,
                          'preprocessing': {'fallback_true_label': ' >50K'},
                          'loss': {'type': 'binary_weighted_cross_entropy'},
                          'type': 'binary'}],
     'preprocessing': {'number': {'missing_value_strategy': 'fill_with_mean',
                                  'normalization': 'zscore'}},
     'trainer': {'epochs': 3, 'optimizer': {'type': 'adam'}}}


    # instantiate Ludwig model object
    model = LudwigModel(config=config, logging_level=logging.INFO)

    shutil.rmtree('results', ignore_errors=True)
    # Trains the model. This cell might take a few minutes.
    train_stats, preprocessed_data, output_directory = model.train(training_set=train_df,
                                                                   test_set=test_df)

    np.random.seed(13)
    eval_df = test_df.sample(n=1000)

    print(eval_df.head())

    # Generates predictions and performance statistics for the test set.
    shutil.rmtree('test_results', ignore_errors=True)
    print(f""\n>>>>>torch training flag before: {model.model.training}"")
    print(f""\n>>>>PERFORMING api evalaute()<<<<<<<<<"")
    test_stats, predictions, output_directory = model.evaluate(
      eval_df,
      collect_predictions=True,   # True
      collect_overall_stats=True, # True
      skip_save_eval_stats=False,   # False
      skip_save_predictions=False,  # False
      # skip_save_unprocessed_output=True,   # not specified
      output_directory=""test_results"",
      return_type=""dict""
    )
    print(f""torch training flag after: {model.model.training}"")

    pred_df = pd.read_parquet(""test_results/predictions.parquet"")
    print(""\n>>>>> evaluate api from in-memory api trained model"")
    print(pred_df.head())


Predicted probabilities are all 1.0 or 0.0.
  income_probabilities income_predictions  income_probabilities_ <=50K  income_probabilities_ >50K  income_probability
0           [1.0, 0.0]              <=50K                          1.0                         0.0                 1.0
1           [1.0, 0.0]              <=50K                          1.0                         0.0                 1.0
2           [1.0, 0.0]              <=50K                          1.0                         0.0                 1.0
3           [1.0, 0.0]              <=50K                          1.0                         0.0                 1.0
4           [1.0, 0.0]              <=50K                          1.0                         0.0                 1.0",957efc1dc9941fb2458b70e8b5c3a77c4ba9cc22,f00d2701d93d0f64397885db68459c73c9e8120c,['ludwig/features/number_feature.py']
62,ludwig-ai/ludwig,https://github.com/ludwig-ai/ludwig/issues/1262,1269,It should be possible to use distributed training (with eg. Horovod) when running hyperparameter optimization.,8545f51470c2e2b8ef0dfb5ae5d313793728fae0,8095b112c4fbd332804f274be3aa9d897d7f57a9,"['ludwig/api.py', 'ludwig/backend/ray.py', 'ludwig/callbacks.py', 'ludwig/data/dataframe/dask_df_utils.py', 'ludwig/data/preprocessing.py', 'ludwig/hyperopt/execution.py', 'ludwig/hyperopt/run.py', 'ludwig/hyperopt/sampling.py', 'ludwig/models/trainer.py', 'ludwig/utils/fs_utils.py', 'requirements.txt', 'requirements_ray.txt', 'tests/integration_tests/test_hyperopt_ray_horovod.py', 'tests/integration_tests/test_ray.py']"
63,ludwig-ai/ludwig,https://github.com/ludwig-ai/ludwig/issues/484,1174,"Is it possible to pass 2D numpy arrays as input instead of png/jpg for image classification?
I have made some 2D numpy arrays (values between 0~1 , black and white, 1 channel) that I would like to train them as images for classification, I was wondering if it's possible to pass the arrays to Ludwig strait away instead of converting all the arrays to png then pass to Ludwig? If yes, how? Thanks for any help.",30d164e7cc3fa7d1c45286727c0183f8eefa8e39,7f4ac600077b1af44b0b58b5d78bfbe58ee9dd9c,"['ludwig/backend/dask.py', 'ludwig/data/preprocessing.py', 'ludwig/features/audio_feature.py', 'ludwig/features/bag_feature.py', 'ludwig/features/binary_feature.py', 'ludwig/features/category_feature.py', 'ludwig/features/date_feature.py', 'ludwig/features/h3_feature.py', 'ludwig/features/image_feature.py', 'ludwig/features/numerical_feature.py', 'ludwig/features/sequence_feature.py', 'ludwig/features/set_feature.py', 'ludwig/features/text_feature.py', 'ludwig/features/timeseries_feature.py', 'ludwig/features/vector_feature.py', 'ludwig/serve.py', 'ludwig/utils/audio_utils.py', 'ludwig/utils/data_utils.py', 'ludwig/utils/server_utils.py', 'tests/integration_tests/test_experiment.py', 'tests/integration_tests/test_server.py', 'tests/ludwig/utils/test_normalization.py']"
64,ludwig-ai/ludwig,https://github.com/ludwig-ai/ludwig/issues/1389,1402,"Latest master on python 3.9 on Windows with all the components updates, training with the increase_batch_size_on_plateau enabled.
When the batch size increase happens, the training total batches number is not changed. This causes the statistics\graphs output in the CLI to be mismatched with the actual training that is being performed.
Also, I expected that the training\validation\test evaluation batch size would change accordingly (not for a loss value standpoint since it should have the same result, but from a performance\speed standpoint)
Here it is after 2 plateau batch size increase 2x (so stopping at 25%)",b6703bd97b24e6d7f7b5cf931348271ff3da4c81,4de36c3f58aad6915dd8ae76327947f467bfc3cb,"['ludwig/data/batcher/base.py', 'ludwig/data/batcher/bucketed.py', 'ludwig/data/batcher/iterable.py', 'ludwig/data/batcher/random_access.py', 'ludwig/models/trainer.py']"
65,ludwig-ai/ludwig,https://github.com/ludwig-ai/ludwig/issues/1613,1663,"Train a ludwig model using concat combiner on Titanic, Forest Cover, or Mushroom Edibility.
The loss curves as well as final results are significantly different.
Desired behavior:
Given the same dataset and config, models trained with TF version of Ludwig vs. Torch may not produce identical results but they should be very similar.
For example, here are the results of running examples/titanic/multiple_model_training.py on both Ludwig 0.4 and Ludwig 0.5dev (e065e03)
Ludwig 0.4


Ludwig 0.5dev",1ebc65bf02381fca253d4d8f68ec4b690091ce26,cde8e8f28e5417888250cd29e3b9e6459679c1f8,"['ludwig/models/predictor.py', 'ludwig/models/trainer.py', 'ludwig/modules/tabnet_modules.py', 'ludwig/utils/torch_utils.py']"
66,ludwig-ai/ludwig,https://github.com/ludwig-ai/ludwig/issues/828,829,"I'm trying to use the new gpu_memory_limit parameter introduced by TF2 as opposed as the gpu_fraction one.
One thing I noticed is that it only works if --gpus is also specified, as per
https://github.com/uber/ludwig/blob/b31ce876f6cd364471f7154740485656ab4ea6c9/ludwig/utils/tf_utils.py#L87-L107
Once you both specify --gpus 0 --gpu_memory_limit 4000 (for example), allocated GPU memory is correctly set to 4GB. Without the --gpus 0 bit, all available memory is allocated as if no --gpu_memory_limit 4000 was specified.
One more thing is that Hyperopt still references the ""old"" --gpu_fraction
https://github.com/uber/ludwig/blob/b31ce876f6cd364471f7154740485656ab4ea6c9/ludwig/hyperopt_cli.py#L473-L479",75f90df4ae5d30a57a2017e6353a38c251d22e79,f2293408a38ad48e15f03a07646bd73821f68bcc,"['ludwig/hyperopt_cli.py', 'ludwig/utils/tf_utils.py']"
67,ludwig-ai/ludwig,https://github.com/ludwig-ai/ludwig/issues/1889,1893,"Describe the bug
When predicting against a dictionary that contains python float32 values the backend.cache.get_dataset_cache(config, dataset) returns an error  TypeError: Object of type float32 is not JSON serializable
Error stack:
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
Input In [11], in <cell line: 1>()
----> 1 predictions, _ = model.predict(dataset=test_dict, data_format=dict, return_type=dict)
      2 predictions[target]

File ~/projects/ludwig/ludwig/api.py:732, in LudwigModel.predict(self, dataset, data_format, split, batch_size, skip_save_unprocessed_output, skip_save_predictions, output_directory, return_type, **kwargs)
    730 # preprocessing
    731 logger.debug(""Preprocessing"")
--> 732 dataset, _ = preprocess_for_prediction(
    733     self.config,
    734     dataset=dataset,
    735     training_set_metadata=self.training_set_metadata,
    736     data_format=data_format,
    737     split=split,
    738     include_outputs=False,
    739     backend=self.backend,
    740     callbacks=self.callbacks,
    741 )
    743 logger.debug(""Predicting"")
    744 with self.backend.create_predictor(self.model, batch_size=batch_size) as predictor:

File ~/projects/ludwig/ludwig/data/preprocessing.py:1764, in preprocess_for_prediction(config, dataset, training_set_metadata, data_format, split, include_outputs, backend, callbacks)
   1761 cached = False
   1763 dataset = wrap(dataset)
-> 1764 cache = backend.cache.get_dataset_cache(config, dataset)
   1765 dataset = dataset.unwrap()
   1767 training_set = test_set = validation_set = None

File ~/projects/ludwig/ludwig/data/cache/manager.py:95, in CacheManager.get_dataset_cache(self, config, dataset, training_set, test_set, validation_set)
     86 def get_dataset_cache(
     87     self,
     88     config: dict,
   (...)
     92     validation_set: Optional[CacheableDataset] = None,
     93 ) -> DatasetCache:
     94     if dataset is not None:
---> 95         key = self.get_cache_key(dataset, config)
     96         cache_map = {
     97             META: self.get_cache_path(dataset, key, META, ""json""),
     98             TRAINING: self.get_cache_path(dataset, key, TRAINING),
     99             TEST: self.get_cache_path(dataset, key, TEST),
    100             VALIDATION: self.get_cache_path(dataset, key, VALIDATION),
    101         }
    102         return DatasetCache(config, key, cache_map, self._dataset_manager)

File ~/projects/ludwig/ludwig/data/cache/manager.py:114, in CacheManager.get_cache_key(self, dataset, config)
    113 def get_cache_key(self, dataset: CacheableDataset, config: dict) -> str:
--> 114     return calculate_checksum(dataset, config)

File ~/projects/ludwig/ludwig/data/cache/util.py:17, in calculate_checksum(original_dataset, config)
      8 features = config.get(""input_features"", []) + config.get(""output_features"", []) + config.get(""features"", [])
      9 info = {
     10     ""ludwig_version"": ludwig.globals.LUDWIG_VERSION,
     11     ""dataset_checksum"": original_dataset.checksum,
   (...)
     15     ""feature_preprocessing"": [feature.get(PREPROCESSING, {}) for feature in features],
     16 }
---> 17 return hash_dict(info, max_length=None).decode(""ascii"")

File ~/projects/ludwig/ludwig/utils/misc_utils.py:128, in hash_dict(d, max_length)
    127 def hash_dict(d: dict, max_length: Union[int, None] = 6) -> bytes:
--> 128     s = json.dumps(d, sort_keys=True, ensure_ascii=True)
    129     h = hashlib.md5(s.encode())
    130     d = h.digest()

File ~/mambaforge/envs/base38/lib/python3.8/json/__init__.py:234, in dumps(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)
    232 if cls is None:
    233     cls = JSONEncoder
--> 234 return cls(
    235     skipkeys=skipkeys, ensure_ascii=ensure_ascii,
    236     check_circular=check_circular, allow_nan=allow_nan, indent=indent,
    237     separators=separators, default=default, sort_keys=sort_keys,
    238     **kw).encode(obj)

File ~/mambaforge/envs/base38/lib/python3.8/json/encoder.py:199, in JSONEncoder.encode(self, o)
    195         return encode_basestring(o)
    196 # This doesn't pass the iterator directly to ''.join() because the
    197 # exceptions aren't as detailed.  The list call should be roughly
    198 # equivalent to the PySequence_Fast that ''.join() would do.
--> 199 chunks = self.iterencode(o, _one_shot=True)
    200 if not isinstance(chunks, (list, tuple)):
    201     chunks = list(chunks)

File ~/mambaforge/envs/base38/lib/python3.8/json/encoder.py:257, in JSONEncoder.iterencode(self, o, _one_shot)
    252 else:
    253     _iterencode = _make_iterencode(
    254         markers, self.default, _encoder, self.indent, floatstr,
    255         self.key_separator, self.item_separator, self.sort_keys,
    256         self.skipkeys, _one_shot)
--> 257 return _iterencode(o, 0)

File ~/mambaforge/envs/base38/lib/python3.8/json/encoder.py:179, in JSONEncoder.default(self, o)
    160 def default(self, o):
    161     """"""Implement this method in a subclass such that it returns
    162     a serializable object for ``o``, or calls the base implementation
    163     (to raise a ``TypeError``).
   (...)
    177 
    178     """"""
--> 179     raise TypeError(f'Object of type {o.__class__.__name__} '
    180                     f'is not JSON serializable')

TypeError: Object of type float32 is not JSON serializable

To Reproduce
Steps to reproduce the behavior:

Create a train a titanic model
Make prediction using dict format like below:

test_dict = [{'Pclass': 3,
  'Sex': 'male',
  'Age': 34.5,
  'SibSp': 0,
  'Parch': 0,
  'Fare': 7.8292,
  'Embarked': 'Q'}]
model.predict(dataset=test_dict, data_format=dict, return_type=dict)

Expected behavior
Expected that the prediction results will be returned.
Environment (please complete the following information):

OS: osx-arm64
Version: 12.2.1 (Monterey)
Python versionL: py38
Ludwig version: master Fri Apr 8 (commit b86c9ca)

Additional context
Add any other context about the problem here.",d32fd6ce1b2a345e57e89bc9be874e716c32fa6b,a7d43eed546d82413debd08d969fb49419c99fa8,"['ludwig/data/cache/util.py', 'ludwig/features/feature_utils.py', 'ludwig/hyperopt/execution.py', 'ludwig/utils/data_utils.py', 'ludwig/utils/misc_utils.py', 'tests/ludwig/utils/test_data_utils.py']"
68,ludwig-ai/ludwig,https://github.com/ludwig-ai/ludwig/issues/296,367,"Is your feature request related to a problem? Please describe.
It seems as if numerical inputs for Ludwig do not get normalized to zero mean and a standard deviation of one. I have to preprocess the data manually before and keep the statistics of the inputs to normalize unseen data.
Describe the use case
Useful for all users that deal with numerical data.
Describe the solution you'd like
I would like an option in the preprocessing setting of the model dict to normalize inputs and store the statistics in the final model file. So if I predict new data I do not have to do any manual normalization steps before.
Best
G",39081266b4df77a30324b2d898b74c602bbf3956,4a540b6d68edaff8d123fb3fdbd7ffc68f13d225,"['ludwig/features/numerical_feature.py', 'mkdocs/docs/user_guide.md', 'tests/integration_tests/test_experiment.py', 'tests/integration_tests/utils.py', 'tests/ludwig/utils/test_normalization.py']"
69,ludwig-ai/ludwig,https://github.com/ludwig-ai/ludwig/issues/1871,1872,"This leads to failure during model load for evaluation after Ray Tune job completes.
I can investigate this.",0bbee640449865d1241088b059b4e5f8409c4a07,8d4ec8f5f8aaa2676395a94a53eaae340933888c,['ludwig/models/trainer.py']
70,ludwig-ai/ludwig,https://github.com/ludwig-ai/ludwig/issues/1030,1031,"hyperopt() generates a hyperopt_statistics.json file which contains training_stats and the metric_score from each sample
I'm optimizing for validation accuracy, but I noticed that the final metric_score that gets reported in the json file uses the validation accuracy from the last epoch during training rather than the best epoch during training
Below is the output from my hyperopt_statistics.json file
""hyperopt_results"": [
        {
            ""eval_stats"": {
                ""combined"": {
                    ""loss"": 1.4781785011291504
                },
                ""label"": {
                    ""accuracy"": 0.47138965129852295,  // this is the final reported accuracy
...
            },
            ""metric_score"": 0.47138965129852295,   // this value comes from HyperoptExecutor.get_metric_score(self, eval_stats) which just copies the value above
            ""parameters"": {
                ""training.learning_rate"": 0.0006019209790229743,
                ""utterance.cell_type"": ""gru"",
                ""utterance.num_layers"": 1,
                ""utterance.state_size"": 495
            },
            ""training_stats"": {
                ""validation"": {
                    ""combined"": {
                        ""loss"": [
                            1.3932547569274902,
                            1.2642898559570312,
                            1.3837428092956543,
                            1.2704368829727173,
                            1.3504513502120972,
                            1.3695340156555176,
                            1.6437498331069946,
                            1.589107632637024,
                            1.4781785011291504
                        ]
                    },
                    ""label"": {
                        ""accuracy"": [
                            0.40962761640548706,
                            0.440508633852005,
                            0.4423251450061798,
                            0.47320616245269775,  // this is the best validation accuracy from training
                            0.47320616245269775,
                            0.440508633852005,
                            0.4523160755634308,
                            0.40690281987190247,
                            0.47138965129852295   // this value from the last epoch is what's actually reported above
                        ],
...

Is this intended because the logger output from LudwigModel.train() uses the best validation accuracy so I thought hyperopt would have similar behavior?
If this is a bug I can try to help fix it as I have a general idea of where this behavior comes from in the codebase
Environment:

Run in Google Colab
Python version 3.6.9
Ludwig version 0.3.1",8af2181d64aab2b823db204f02d63227e07b8604,ab9e7a569730f17e0d168585fe3064c3264efe8d,"['ludwig/hyperopt/execution.py', 'ludwig/preprocess.py', 'tests/integration_tests/test_hyperopt.py']"
71,ludwig-ai/ludwig,https://github.com/ludwig-ai/ludwig/issues/1991,1992,"When evaluating with Ray, the user may get the following error:
 File ""/home/ray/anaconda3/lib/python3.7/site-packages/torchmetrics/classification/accuracy.py"", line 222, in update
    mode = _mode(preds, target, self.threshold, self.top_k, self.num_classes, self.multiclass, self.ignore_index)
  File ""/home/ray/anaconda3/lib/python3.7/site-packages/torchmetrics/functional/classification/accuracy.py"", line 66, in _mode
    ignore_index=ignore_index,
  File ""/home/ray/anaconda3/lib/python3.7/site-packages/torchmetrics/utilities/checks.py"", line 265, in _check_classification_inputs
    _basic_input_validation(preds, target, threshold, multiclass, ignore_index)
  File ""/home/ray/anaconda3/lib/python3.7/site-packages/torchmetrics/utilities/checks.py"", line 44, in _basic_input_validation
    raise ValueError(""The `target` has to be an integer tensor."")
ValueError: The `target` has to be an integer tensor.

This error can be reproduced by running the following:
from ludwig.api import LudwigModel
from ludwig.backend import RAY

model = LudwigModel(config, backend=""ray"")
model.train(
    dataset=data_csv,
    skip_save_training_description=True,
    skip_save_training_statistics=True,
    skip_save_model=True,
    skip_save_progress=True,
    skip_save_log=True,
    skip_save_processed_input=True,
    output_directory=output_dir,
)
model.evaluate(
    dataset=data_csv,
    skip_save_unprocessed_output=True,
    skip_save_predictions=True,
    skip_save_eval_stats=True,
    collect_predictions=False,
    collect_overall_stats=False,
    output_directory=output_dir,
)

Alternatively, simply run a test which calls model.evaluate such as tests/integration_tests/test_api.py::test_api_skip_parameters_evaluate where all skip params are True, and set the LudwigModel created in run_api_commands to have backend=""ray"".",74354e868f2cd3f80fe12527dbc89ba8e3435fe6,88920b28676811e29f231e5b400a0af600e806af,"['ludwig/data/dataframe/dask.py', 'ludwig/features/number_feature.py', 'tests/integration_tests/test_ray.py', 'tests/integration_tests/utils.py']"
72,ludwig-ai/ludwig,https://github.com/ludwig-ai/ludwig/issues/2186,2191,"Config:
input_features:
  - name: doc
    type: text
    preprocessing:
      max_sequence_length: 4096
output_features:
  - name: cls
    type: category
    column: cls
trainer:
  epochs: 10
  early_stop: 3
  reduce_learning_rate_on_plateau: 1
  reduce_learning_rate_on_plateau_patience: 1
Error:
  File ""/home/ray/anaconda3/lib/python3.8/site-packages/ludwig/models/trainer.py"", line 851, in train
    should_break = self._train_loop(
  File ""/home/ray/anaconda3/lib/python3.8/site-packages/ludwig/models/trainer.py"", line 1019, in _train_loop
    should_break = self.run_evaluation(
  File ""/home/ray/anaconda3/lib/python3.8/site-packages/ludwig/models/trainer.py"", line 652, in run_evaluation
    should_break = self.check_progress_on_validation(
  File ""/home/ray/anaconda3/lib/python3.8/site-packages/ludwig/models/trainer.py"", line 1177, in check_progress_on_validation
    self.reduce_learning_rate(
  File ""/home/ray/anaconda3/lib/python3.8/site-packages/ludwig/models/trainer.py"", line 1305, in reduce_learning_rate
    is_improved = improved(last_metric_value, progress_tracker.best_reduce_learning_rate_eval_metric)
  File ""/home/ray/anaconda3/lib/python3.8/site-packages/ludwig/modules/metric_modules.py"", line 476, in <lambda>
    return lambda x, y: x < y
TypeError: '<' not supported between instances of 'TrainerMetric' and 'float'",36787ba741b5bcdf5fb5b610752009f6cf7fbd03,1cfdbf86c829616d07fad80aebdf424ebfb0dedc,"['ludwig/models/trainer.py', 'tests/integration_tests/test_trainer.py']"
73,ludwig-ai/ludwig,https://github.com/ludwig-ai/ludwig/issues/2169,2170,"Already removed this error in trainer.py in this commit, but need to do the same for predictor.py and in any other module that tries to operate on numpy arrays in this fashion directly.
/ludwig/ludwig/models/predictor.py:181: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /root/pytorch/torch/csrc/utils/tensor_numpy.cpp:172.)",3fca05602cc32bcb41b9f2a83702ea95afb683ea,10b04bf3cc91a20834540e3e517ba1a8cfa87d07,"['ludwig/models/predictor.py', 'ludwig/models/trainer.py', 'tests/integration_tests/test_regularizers.py']"
74,ludwig-ai/ludwig,https://github.com/ludwig-ai/ludwig/issues/1181,1326,"Describe the bug
When generating a model prediction  involving an audio feature and the data set contains only one record, a ZeroDvisionError exception is raised in ludwig.utils.audio_utils.calcuate_var() function.  The error occurs because the variable count = 1 which results in a zero in the denominator of this expression:
return (sum2 - ((sum1 * sum1) / float(count))) / float(count - 1)

To Reproduce
Steps to reproduce the behavior:
Run unit test test_server.py
Expected behavior
Successful prediction with only a single record.
Log file
Here is log and error messages:
PASSED [ 66%]FAILED [100%]Failed to run predict: float division by zero
Traceback (most recent call last):
  File ""/opt/project/ludwig/serve.py"", line 92, in predict
    dataset=[entry], data_format=dict
  File ""/opt/project/ludwig/api.py"", line 683, in predict
    backend=self.backend,
  File ""/opt/project/ludwig/data/preprocessing.py"", line 1728, in preprocess_for_prediction
    backend
  File ""/opt/project/ludwig/data/preprocessing.py"", line 162, in preprocess_for_prediction
    backend=backend
  File ""/opt/project/ludwig/data/preprocessing.py"", line 1080, in build_dataset
    skip_save_processed_input
  File ""/opt/project/ludwig/data/preprocessing.py"", line 1225, in build_data
    skip_save_processed_input
  File ""/opt/project/ludwig/features/audio_feature.py"", line 343, in add_feature_data
    backend
  File ""/opt/project/ludwig/features/audio_feature.py"", line 174, in _process_in_memory
    merged_stats['var'] = calculate_var(merged_stats['sum'], merged_stats['sum2'], merged_stats['count'])
  File ""/opt/project/ludwig/utils/audio_utils.py"", line 240, in calculate_var
    return (sum2 - ((sum1 * sum1) / float(count))) / float(count - 1)
ZeroDivisionError: float division by zero

tests/integration_tests/test_server.py:193 (test_server_integration_with_audio[True])
500 != 200

Expected :200
Actual   :500
<Click to see difference>

single_record = True, csv_filename = 'EA8A7B56DE.csv'

    @pytest.mark.parametrize('single_record', [False, True])
    def test_server_integration_with_audio(single_record, csv_filename):
        # Audio Inputs
        audio_dest_folder = os.path.join(os.getcwd(), 'generated_audio')
    
        # Resnet encoder
        input_features = [
            audio_feature(
                folder=audio_dest_folder,
            ),
            text_feature(encoder='embed', min_len=1),
            numerical_feature(normalization='zscore')
        ]
        output_features = [
            category_feature(vocab_size=2),
            numerical_feature()
        ]
    
        rel_path = generate_data(input_features, output_features, csv_filename)
        model, output_dir = train_model(input_features, output_features,
                                        data_csv=rel_path)
    
        app = server(model)
        client = TestClient(app)
        response = client.get('/')
        assert response.status_code == 200
    
        response = client.post('/predict')
        # expect the HTTP 400 error code for this situation
        assert response.status_code == 400
        assert response.json() == ALL_FEATURES_PRESENT_ERROR
    
        data_df = read_csv(rel_path)
    
        if single_record:
            # Single record prediction
            first_entry = data_df.T.to_dict()[0]
            data, files = convert_to_form(first_entry)
            server_response = client.post('/predict', data=data, files=files)
>           assert server_response.status_code == 200
E           assert 500 == 200

../../tests/integration_tests/test_server.py:233: AssertionError


Environment (please complete the following information):

OS: [e.g. iOS]  Ludwig Docker container
Version [e.g. 22]
Python version: 3.6.9
Ludwig version: 0.4-dev0

Additional context
As short-term work-around modified denominator to be max(1, count-1).",f344a2670667f2f02189626071de9f1408f6144a,8eab807c6ca71a7d1fdda30c49924e4f68bf3101,"['ludwig/features/audio_feature.py', 'ludwig/utils/audio_utils.py']"
75,ludwig-ai/ludwig,https://github.com/ludwig-ai/ludwig/issues/1093,1103,"Describe the bug
For a single output feature model, the per epoch training/validation/test output feature loss value does not equal the combined training/validation/test loss values
To Reproduce
Steps to reproduce the behavior:

Run examples/mnist/simple_model_training.py with logging_level=logging.INFO
See following extract of log file.  The output feature label's loss values are not equal to the combined loss value for every epoch.  There are 'relatively' close but not equal.

â•’â•â•â•â•â•â•â•â•â•â•â••
â”‚ TRAINING â”‚
â•˜â•â•â•â•â•â•â•â•â•â•â•›


Epoch 1
Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 469/469 [00:23<00:00, 19.83it/s]
Evaluation train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 469/469 [00:07<00:00, 62.35it/s]
Evaluation test : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 77.71it/s]
Took 32.2061s
â•’â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â••
â”‚ label   â”‚   loss â”‚   accuracy â”‚   hits_at_k â”‚
â•žâ•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•¡
â”‚ train   â”‚ 0.1175 â”‚     0.9683 â”‚      0.9966 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ test    â”‚ 0.1067 â”‚     0.9708 â”‚      0.9970 â”‚
â•˜â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•›
â•’â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â••
â”‚ combined   â”‚   loss â”‚
â•žâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•¡
â”‚ train      â”‚ 0.0960 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ test       â”‚ 0.0831 â”‚
â•˜â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•›


Epoch 2
Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 469/469 [00:20<00:00, 22.43it/s]
Evaluation train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 469/469 [00:05<00:00, 86.99it/s]
Evaluation test : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 42.91it/s]
Took 28.1646s
â•’â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â••
â”‚ label   â”‚   loss â”‚   accuracy â”‚   hits_at_k â”‚
â•žâ•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•¡
â”‚ train   â”‚ 0.0675 â”‚     0.9838 â”‚      0.9989 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ test    â”‚ 0.0662 â”‚     0.9830 â”‚      0.9983 â”‚
â•˜â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•›
â•’â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â••
â”‚ combined   â”‚   loss â”‚
â•žâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•¡
â”‚ train      â”‚ 0.0527 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ test       â”‚ 0.0476 â”‚
â•˜â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•›


Epoch 3
Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 469/469 [00:22<00:00, 21.27it/s]
Evaluation train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 469/469 [00:05<00:00, 81.35it/s]
Evaluation test : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:00<00:00, 95.46it/s]
Took 28.6584s
â•’â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â••
â”‚ label   â”‚   loss â”‚   accuracy â”‚   hits_at_k â”‚
â•žâ•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•¡
â”‚ train   â”‚ 0.0472 â”‚     0.9875 â”‚      0.9991 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ test    â”‚ 0.0472 â”‚     0.9863 â”‚      0.9993 â”‚
â•˜â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•›
â•’â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â••
â”‚ combined   â”‚   loss â”‚
â•žâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•¡
â”‚ train      â”‚ 0.0428 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ test       â”‚ 0.0391 â”‚
â•˜â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•›


Epoch 4
Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 469/469 [00:21<00:00, 22.08it/s]
Evaluation train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 469/469 [00:04<00:00, 98.50it/s] 
Evaluation test : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:00<00:00, 88.37it/s]
Took 26.9187s
â•’â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â••
â”‚ label   â”‚   loss â”‚   accuracy â”‚   hits_at_k â”‚
â•žâ•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•¡
â”‚ train   â”‚ 0.0400 â”‚     0.9894 â”‚      0.9996 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ test    â”‚ 0.0422 â”‚     0.9867 â”‚      0.9993 â”‚
â•˜â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•›
â•’â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â••
â”‚ combined   â”‚   loss â”‚
â•žâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•¡
â”‚ train      â”‚ 0.0381 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ test       â”‚ 0.0375 â”‚
â•˜â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•›


Epoch 5
Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 469/469 [00:19<00:00, 24.25it/s]
Evaluation train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 469/469 [00:04<00:00, 95.41it/s]
Evaluation test : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:00<00:00, 96.54it/s]
Took 25.9466s
â•’â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â••
â”‚ label   â”‚   loss â”‚   accuracy â”‚   hits_at_k â”‚
â•žâ•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•¡
â”‚ train   â”‚ 0.0291 â”‚     0.9931 â”‚      0.9997 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ test    â”‚ 0.0324 â”‚     0.9907 â”‚      0.9997 â”‚
â•˜â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•›
â•’â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â••
â”‚ combined   â”‚   loss â”‚
â•žâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•¡
â”‚ train      â”‚ 0.0219 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ test       â”‚ 0.0224 â”‚
â•˜â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•›


Expected behavior
With only a single output feature, I was expecting the combined loss value to equal the single output feature's loss value per epoch
Screenshots
See log extract above
Environment (please complete the following information):

OS: MacOS 10.15.7, running Docker for Mac 3.04
Python version 3.6.9
Ludwig version 0.3.2. 0.3.3, 0.4-dev0

Additional context
From what I can tell this is solely reporting issue and does not affect a model's ability for convergence.
I have possible root cause for the reporting issue.  In the context of the categorical feature, the CategoryOutputFeature._setup_loss() and CategoryOutputFeature._setup_metrics() methods may have an unintended interaction.  In CategoryOutputFeature._setup_loss() there is this code fragment:
        self.eval_loss_function = SoftmaxCrossEntropyMetric(
            num_classes=self.num_classes,
            feature_loss=self.loss,
            name='eval_loss'
        )

and in CategoryOutputFeature._setup_loss()
        self.metric_functions[LOSS] = self.eval_loss_function

At conclusion of output feature construction, both self.eval_loss_function and self.metric_functions[LOSS] are pointing to the same instance of a Keras Metric object.
So during the training loop, this metric object is called twice, once in the context of calculating the loss for the output feature and a second time to capture the combined loss.  During training the combined loss is accumulated in ECD.eval_loss_metric for reporting purposes.  This results in the epoch loss calculations using different values for the output feature and combined.
Based on testing I've done, the fix is to change CategoryOutputFeature.eval_loss_function to be a Keras Loss function and not a metric and revise metric specification to CategoryOutputFeture.metric_functions[LOSS] to not reuse CategoryOutputFeature.eval_loss_function.
Further, this reporting issue appears to be present in other output features.  Similar changes are needed in the other output features' _setup_loss() and _setup_metrics() methods.
I'll start a PR to address this issue.",cbcb601037a0c6474b93a7e5ee89e4d317822b93,5398e54bc9172c250235273cf1525ca248599a25,"['ludwig/constants.py', 'ludwig/decoders/sequence_decoders.py', 'ludwig/features/base_feature.py', 'ludwig/features/binary_feature.py', 'ludwig/features/category_feature.py', 'ludwig/features/numerical_feature.py', 'ludwig/features/sequence_feature.py', 'ludwig/models/predictor.py', 'ludwig/modules/loss_modules.py', 'ludwig/modules/metric_modules.py', 'ludwig/modules/recurrent_modules.py', 'tests/integration_tests/test_sequence_sampled_softmax.py']"
76,ludwig-ai/ludwig,https://github.com/ludwig-ai/ludwig/issues/2851,2852,"Describe the bug
Cannot use the RoBERTa encoder for input and output features.
To Reproduce
Configuration of model to reproduce the issue:
config = {
    ""input_features"": [
        {
            ""name"": ""source_code"",
            ""type"": ""text"",
            ""encoder"": {
                ""type"": ""roberta"",
                ""max_sequence_length"": 3000,
            }
        },
    ],
    ""output_features"": [
        {
            ""name"": ""test_code"",
            ""type"": ""text"",
            ""encoder"": {
                ""type"": ""roberta"",
                ""max_sequence_length"": 1000,
            }
        }
    ],
}
The training data used:
source_code,test_code
function addOne(a) { return a + 1 },expect(addOne(1)).toEqual(2)
function hello(a) { return 1+a },expect(hello(1)).toEqual(2)

Expected behavior
The model can be trained with the RoBERTa encoder and the max_sequence_length argument is passed through and can be adjusted through the features encoder configuration.
Environment:

OS: macOS 12.6
Version [e.g. 22]
Python version 3.9
Ludwig version 0.6.4

Additional context
The ""max_sequence_length"" was added to the configuration to try to fix the issue, when changing the encoder to ""bert"" the configuration worked so it seems an issue with this particular encoder.
Traceback:
Traceback (most recent call last):
  File ""/Users/rudolfo/Workspace/ludwig-code-gen/main.py"", line 27, in <module>
    model.train(
  File ""/Users/rudolfo/Workspace/ludwig-code-gen/env/lib/python3.9/site-packages/ludwig/api.py"", line 505, in train
    self.model = LudwigModel.create_model(self.config, random_seed=random_seed)
  File ""/Users/rudolfo/Workspace/ludwig-code-gen/env/lib/python3.9/site-packages/ludwig/api.py"", line 1542, in create_model
    return model_type(**config, random_seed=random_seed)
  File ""/Users/rudolfo/Workspace/ludwig-code-gen/env/lib/python3.9/site-packages/ludwig/models/ecd.py"", line 46, in __init__
    self.input_features.update(self.build_inputs(self._input_features_def))
  File ""/Users/rudolfo/Workspace/ludwig-code-gen/env/lib/python3.9/site-packages/ludwig/models/base.py"", line 55, in build_inputs
    input_features[input_feature_def[NAME]] = cls.build_single_input(input_feature_def, input_features)
  File ""/Users/rudolfo/Workspace/ludwig-code-gen/env/lib/python3.9/site-packages/ludwig/models/base.py"", line 72, in build_single_input
    input_feature_obj = input_feature_class(input_feature_def, encoder_obj=encoder_obj)
  File ""/Users/rudolfo/Workspace/ludwig-code-gen/env/lib/python3.9/site-packages/ludwig/features/text_feature.py"", line 188, in __init__
    super().__init__(input_feature_config, encoder_obj=encoder_obj, **kwargs)
  File ""/Users/rudolfo/Workspace/ludwig-code-gen/env/lib/python3.9/site-packages/ludwig/features/sequence_feature.py"", line 271, in __init__
    self.encoder_obj = self.initialize_encoder(input_feature_config.encoder)
  File ""/Users/rudolfo/Workspace/ludwig-code-gen/env/lib/python3.9/site-packages/ludwig/features/base_feature.py"", line 170, in initialize_encoder
    return encoder_cls(encoder_config=encoder_config, **encoder_params_dict)
TypeError: __init__() missing 1 required positional argument: 'max_sequence_length'",ee89c2001a75b49375836f16042bf8c05f23bee6,bb721464f0a8d00a1570ea82260ff7873fd805e5,['ludwig/schema/encoders/text_encoders.py']
77,ludwig-ai/ludwig,https://github.com/ludwig-ai/ludwig/issues/2144,2175,"Total dataset size = 570M (Including test, train, val)
Batch size = 32K
Expected steps per epoch = 570M / 32K = ~17800
Observed steps per epoch = 354 (From stdout at start of experiment training training for 1000 epochs, and 354000 steps)
Not performing distributed training
To reproduce, run hyperopt on the dataset with 8 parallel trials.",7c929d3b637e81cc20026ac90cfb25ac5ffb7576,cdec598e44735d7713cd0739ac9d5e8aa9212348,['ludwig/data/dataset/ray.py']
78,ludwig-ai/ludwig,https://github.com/ludwig-ai/ludwig/issues/268,1174,Is it possible to pass image(s) for classification prediction via API without writing them to the hard drive first?,30d164e7cc3fa7d1c45286727c0183f8eefa8e39,7f4ac600077b1af44b0b58b5d78bfbe58ee9dd9c,"['ludwig/backend/dask.py', 'ludwig/data/preprocessing.py', 'ludwig/features/audio_feature.py', 'ludwig/features/bag_feature.py', 'ludwig/features/binary_feature.py', 'ludwig/features/category_feature.py', 'ludwig/features/date_feature.py', 'ludwig/features/h3_feature.py', 'ludwig/features/image_feature.py', 'ludwig/features/numerical_feature.py', 'ludwig/features/sequence_feature.py', 'ludwig/features/set_feature.py', 'ludwig/features/text_feature.py', 'ludwig/features/timeseries_feature.py', 'ludwig/features/vector_feature.py', 'ludwig/serve.py', 'ludwig/utils/audio_utils.py', 'ludwig/utils/data_utils.py', 'ludwig/utils/server_utils.py', 'tests/integration_tests/test_experiment.py', 'tests/integration_tests/test_server.py', 'tests/ludwig/utils/test_normalization.py']"
79,philipperemy/keras-tcn,https://github.com/philipperemy/keras-tcn/issues/121,122,"The normalization layers (BatchNorm or LayerNorm) do not get unique names. Since each ResidualBlock has 2 Norm layers, you will not be able to save the model.
It is an easy fix:
In tcn.py, line 100 add the line with K.name_scope('norm_{}'.format(k)):
currently looks like:
with K.name_scope(name):  # name scope used to make sure weights get unique names
    self._add_and_activate_layer(Conv1D(filters=self.nb_filters,
                                                        kernel_size=self.kernel_size,
                                                        dilation_rate=self.dilation_rate,
                                                        padding=self.padding,
                                                        name=name,
                                                        kernel_initializer=self.kernel_initializer))

if self.use_batch_norm:
    self._add_and_activate_layer(BatchNormalization())
elif self.use_layer_norm:
    self._add_and_activate_layer(LayerNormalization())

Change to:
with K.name_scope(name):  # name scope used to make sure weights get unique names
    self._add_and_activate_layer(Conv1D(filters=self.nb_filters,
                                                        kernel_size=self.kernel_size,
                                                        dilation_rate=self.dilation_rate,
                                                        padding=self.padding,
                                                        name=name,
                                                        kernel_initializer=self.kernel_initializer))
with K.name_scope('norm_{}'.format(k)):
    if self.use_batch_norm:
        self._add_and_activate_layer(BatchNormalization())
    elif self.use_layer_norm:
        self._add_and_activate_layer(LayerNormalization())",b6dd27b18ecbe7748e2dba4e820716456c8d33c5,edb8017da6904ea7c6c5674e76b3e70f2ed5495c,['tcn/tcn.py']
80,philipperemy/keras-tcn,https://github.com/philipperemy/keras-tcn/issues/5,6,"Running
from tcn import tcn

model = tcn.dilated_tcn(output_slice_index='last',
                        num_feat=20,
                        num_classes=None,
                        nb_filters=24,
                        kernel_size=8,
                        dilatations=[1, 2, 4, 8],
                        nb_stacks=8,
                        max_len=100,
                        activation='norm_relu',
                        regression=True)

gives the Warnings
Using TensorFlow backend.
/home/hoppeta/.pyenv/versions/3.6.0/lib/python3.6/site-packages/keras/legacy/layers.py:748: UserWarning: The `AtrousConvolution1D` layer  has been deprecated. Use instead the `Conv1D` layer with the `dilation_rate` argument.
  warnings.warn('The `AtrousConvolution1D` layer '
/home/hoppeta/src/keras-tcn/tcn/tcn.py:38: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.
  res_x = Merge(mode='sum')([original_x, x])
/home/hoppeta/src/keras-tcn/tcn/tcn.py:62: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.
  x = Merge(mode='sum')(skip_connections)
x.shape= (?, 24)
model.x = (?, 100, 20)
model.y = (?, 1)",ce5b962b3034cd0737728b351ff76c00680332a7,3fd3674c73fe451e3d79e41244938ed8c62855ec,['tcn/tcn.py']
81,atomistic-machine-learning/schnetpack,https://github.com/atomistic-machine-learning/schnetpack/issues/250,261,"Hi!
I was looking into the memory usage of schnetpack to find where it can be optimized and I found that the validation batches are using much more memory than the training batches.
Is this an expected behaviour? or is there something strange going on?
My tests use the same .db file for both training and validation (for the sake of testing) and on a Nvidia GTX 1070 with 8GB of VRAM I could use a maximum training batch size of 55 while the maximum validation batch size I could use was only 30 (while keeping the other batch size at 5, respectively).
I attached an MWE where there are 5 different sets of training batch, T_batch_size, and validation batch, V_batch_size sizes for testing.
/Daniel
mem_test.zip",394afcda435d0e9c4ac48f33e28d6f1c7f8aa9f2,595b23b8bef74fc6e1b1f75ce1eaa7c75ed6d188,"['src/schnetpack/atomistic/output_modules.py', 'src/schnetpack/evaluation.py', 'src/schnetpack/md/calculators/schnet_calculator.py', 'src/schnetpack/train/trainer.py', 'src/schnetpack/utils/script_utils/evaluation.py']"
82,atomistic-machine-learning/schnetpack,https://github.com/atomistic-machine-learning/schnetpack/issues/165,216,"Dear schnetpack developers,
is it planned to also implement the calculation of the stress tensor in schnetpack? This would enable to perform NPT simulations.
Best
Johannes",d5190eae99860eb0c012013c532f3d184e0215d8,1570dc5add4efbf98c04299756128bfd2f24f1cd,['src/schnetpack/atomistic/output_modules.py']
83,catalyst-team/catalyst,https://github.com/catalyst-team/catalyst/issues/130,182,"@todo:

rewrite tensorboard logs parsing
remove tensorflow dependency",e6de5683b7707fe0a4abcb3ccf4201ef918e6045,be022997cd1a01ca58d03a09cfa6ffd895761602,"['catalyst/contrib/scripts/project_embeddings.py', 'catalyst/utils/plotly.py', 'catalyst/utils/tensorboard.py', 'catalyst/utils/tests/test_tensorboard.py', 'docker/Dockerfile-041', 'docker/Dockerfile-100', 'docker/Dockerfile-contrib-041', 'docker/Dockerfile-contrib-100', 'requirements.txt']"
84,sktime/sktime,https://github.com/sktime/sktime/issues/2444,2609,"Describe the bug
It is not possible to fit the Prophet model with growth parameter as logistc
To Reproduce
After doing
Prophet(growth=""logistic"").fit(df)
as df a Series with DateTimeIndex, the following error message is displayed
ValueError: Capacities must be supplied for logistic growth in column ""cap""
After researching in Prophet's documentation, a ""capacity"" collumn is expected.
However, adding a new ""cap"" column into the Series (now a DateTimeIndex DataFrame with my time serie and cap columns), the following message error is displayed
ValueError: y must be univariate, but found more than one variable
Even trying to use the X parameter of ""fit"" method, as
Prophet(growth=""logistic"").fit(y = df.time_serie, X = df.cap)
the following error is displayed
ValueError: Name 'cap' is reserved.
I have not found a way to fit this kind of model
Expected behavior
No error message displayed and a fitted model returned
Versions
System:
python: 3.8.8 | packaged by conda-forge | (default, Feb 20 2021, 16:22:27)  [GCC 9.3.0]
executable: /opt/conda/envs/previsao_demanda/bin/python
machine: Linux-3.10.0-1160.36.2.el7.x86_64-x86_64-with-glibc2.10
Python dependencies:
pip: 22.0.4
setuptools: 49.6.0.post20210108
sklearn: 1.0.2
sktime: 0.11.0
statsmodels: 0.13.2
numpy: 1.21.5
scipy: 1.7.3
pandas: 1.2.5
matplotlib: 3.5.1
joblib: 0.17.0
numba: 0.55.1
pmdarima: 1.8.5
tsfresh: 0.19.0",af197f1781b0d94636b5027650b220a44e57d5f5,0e9400f789ad7b574fd37841cbc0c81e818e8f5e,"['sktime/forecasting/base/adapters/_fbprophet.py', 'sktime/forecasting/fbprophet.py']"
85,sktime/sktime,https://github.com/sktime/sktime/issues/1290,1232,"Describe the bug
If you load arrow_head data with default split settings, the resulting dataframe has indices that repeat. this is because there is a concatenation of train and test data
To Reproduce
from sktime.datasets import load_arrow_head
X, y = load_arrow_head(return_X_y=True)
X.index.values
Output:
array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,
        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,
        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,   0,   1,   2,
         3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,  14,  15,
        16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,  28,
        29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
        42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,
        55,  56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,
        68,  69,  70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,
        81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,  92,  93,
        94,  95,  96,  97,  98,  99, 100, 101, 102, 103, 104, 105, 106,
       107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119,
       120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132,
       133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145,
       146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158,
       159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171,
       172, 173, 174])

Expected behavior
No repetition of indices
Versions

System:
    python: 3.7.9 (default, Aug 31 2020, 07:22:35)  [Clang 10.0.0 ]
executable: /usr/local/Caskroom/miniconda/base/envs/sktime/bin/python
   machine: Darwin-20.5.0-x86_64-i386-64bit
Python dependencies:
pip: 20.3.3
setuptools: 51.0.0.post20201207
sklearn: 0.23.0
sktime: 0.7.0
statsmodels: 0.12.1
numpy: 1.19.4
scipy: 1.5.4
Cython: 0.29.17
pandas: 1.1.5
matplotlib: 3.3.3
joblib: 1.0.0
numba: 0.52.0
pmdarima: 1.8.0
tsfresh: 0.17.0",761eb15c9b86e041a8a50443d3cf360ffaa3e65f,105ad4c4411fefbdc8ad587727f666a41a47ad1c,"['.gitignore', 'examples/AA_datatypes_and_datasets.ipynb']"
86,sktime/sktime,https://github.com/sktime/sktime/issues/1678,1944,"Aim
To start to move default parameters out of tests/_config and into the concrete estimators, by specifying the parameters inside the get_test_params method.
This would declutter the logic, and make extending sktime much easier - since it reduces the ""points of extension"" by one. Default parameters would no longer have to be entered in _config but become part of the list of things to implement, in the extension template.
Related Issue
#1098
Step
link for step
This issue is associated with point 3 of the suggested refactor sequence given in the above step.
Contributing Guidelines
If you would like to work on this: please post here and which estimator you are taking on, to avoid duplication of work.
A core dev will then add your name to the estimator under refactoring and/or link the PR.
A general recipe for refactoring is as follows:

pick up an estimator class from the below checklist of estimators.
look for parameters of the estimator in ESTIMATOR_TEST_PARAMS (in the tests._config file)
add get_test_params method to the corresponding estimator class which returns a dictionary of the above parameters
refer to extension template to add get_test_params to the corresponding estimator class
For example, in case of ColumnEnsembleForecaster , dictionary of parameters which is {""forecasters"": FORECASTER} , should be returned from get_test_params method.

Checklist of estimators:

 ColumnEnsembleForecaster
 OnlineEnsembleForecaster
 FeatureUnion
 DirectTabularRegressionForecaster
 MultioutputTabularRegressionForecaster
 RecursiveTabularRegressionForecaster
 DirRecTabularRegressionForecaster
 DirectTimeSeriesRegressionForecaster
 RecursiveTimeSeriesRegressionForecaster
 MultioutputTimeSeriesRegressionForecaster
 DirRecTimeSeriesRegressionForecaster
 TransformedTargetForecaster
 ForecastingPipeline
 EnsembleForecaster
 StackingForecaster
 AutoEnsembleForecaster
 Detrender
 ForecastingGridSearchCV
 ForecastingRandomizedSearchCV
 TabularToSeriesAdaptor
 ColumnEnsembleClassifier
 FittedParamExtractor
 SeriesToPrimitivesRowTransformer
 SeriesToSeriesRowTransformer
 ColumnTransformer
 AutoARIMA
 MultiplexForecaster
 ShapeletTransformClassifier
 ContractedShapeletTransform
 ShapeletTransform
 SignatureTransformer
 SignatureClassifier
 Catch22Classifier
 MatrixProfileClassifier
 TSFreshClassifier
 ROCKETClassifier
 Arsenal
 HIVECOTEV1
 TSFreshFeatureExtractor
 TSFreshRelevantFeatureExtractor
 TSInterpolator
 RandomIntervalSpectralForest
 SFA
 ContractableBOSS
 TemporalDictionaryEnsemble
 TimeSeriesForestClassifier
 ComposableTimeSeriesForestClassifier
 ComposableTimeSeriesForestRegressor
 SupervisedTimeSeriesForest
 CanonicalIntervalForest
 DrCIF
 HCrystalBallForecaster
 BATS
 TBATS
 Prophet
 UnobservedComponents
 PartialAutoCorrelationTransformer
 AutoCorrelationTransformer
 Imputer
 HampelFilter
 OptionalPassthrough
 FeatureSelection
 ColumnwiseTransformer
 AggrDist
 PyODAnnotator
 ClaSPSegmentation",f49618aac7a111bf6aa442090201689cefef2661,348d529379f5bb164b8c75841d8c981d843ec7a1,"['sktime/classification/feature_based/_fresh_prince.py', 'sktime/tests/_config.py']"
87,sktime/sktime,https://github.com/sktime/sktime/issues/3192,3232,"Is your feature request related to a problem? Please describe.
On the topic of migrating sktime-dl models to sktime, adding MLP model for classification from sktime-dl",33e8f3bba9e5ac492630e7758350d57378521721,3449b8b764a47e1302b1a8c8dcf839b994d2c0cf,"['sktime/classification/deep_learning/mlp.py', 'sktime/networks/mlp.py', 'sktime/registry/_base_classes.py', 'sktime/registry/tests/test_lookup.py', 'sktime/tests/_config.py']"
88,sktime/sktime,https://github.com/sktime/sktime/issues/1746,2100,"Is your feature request related to a problem? Please describe.
We are currently working on refactoring the forecasting interfaces to allow probabilistic and interval predictions as described here #1687 and a high level summary included here https://github.com/sktime/enhancement-proposals/blob/main/steps/13_proba_forecasting/step.md
Describe the solution you'd like
We would like to include a way to return the probability distributions. We briefly discussed using methods from tensorflow, but are open to other suggestions.
Potential issues
This would introduce a new dependency.",061b17021332d043cdf244fc28441d25c6b7ba34,b8170946a38d89db7a61cb553514de81ae31e031,"['docs/source/installation.rst', 'extension_templates/forecasting.py', 'pyproject.toml', 'sktime/forecasting/base/_base.py', 'sktime/tests/_config.py', 'sktime/tests/test_all_estimators.py', 'sktime/utils/_testing/estimator_checks.py', 'sktime/utils/_testing/scenarios_forecasting.py', 'sktime/utils/validation/_dependencies.py']"
89,sktime/sktime,https://github.com/sktime/sktime/issues/2799,2855,"I am opening this up as an issue for the new annotation/segmentation workstream.
My plan is to implement a Hidden Markov Model (HMM) estimator for annotation of time series data. You can read more about the algorithm here, but for a brief overview:

You want to model a series of variables over time $X$, where $x_t$ represents the variable state at time $t$.
Unfortunately, you cannot measure or observe $X$ directly, it is a hidden variable, but fortunately you can observe a series of related variables $Y$, where $y_t$ represents your observation of $Y$ at time $t$.
You assume both that $y_t$ depends on $x_t$ (and only on $x_t$), and that $x_{t+1}$ depends on $x_t$ but only on $x_t$ (this assumption that each state is only influenced by the state immediately before it is known as a Markov assumption).
One can use the Baum-Welch algorithm (a special case of EM) to help find the unknown parameters that define the probabilistic relationship that relates $x_t$ and both $y_t$ and $x_{t+1}$.
If you have defined the distributions that relate $x_t$ to $y_t$ (known as the emission distribution) and $x_t$ to $x_{t+1}$ (known as the transmission distribution) then you can calculate the most likely sequence of $X$ given the observed sequence $Y$ using a variety of algorithms (for example the Viterbi algorithm)

This is an attractive algorithm for a variety of reasons - its been around for a long time (helped with the lunar landing if I am not mistaken), its a tool that people from a variety of different fields are familiar with, and has good statistical/algorithmic tools to work with it!
To start with we will work with something that assigns non-overlapping single dimensional labels to univariate time series data (we will expand later, but getting this simple case to work would be nice). This will be a batch, not online approach, and will be an unsupervised algorithm.
There are two paths forward:

one is to integrate hmmlearn.  Its a pretty nice package, has a decent number of users, and what it does it does well.  But, its currently under limited maintenance mode (which could present issues in the future?) doesn't necessarily seem to be totally optimized for speed, which could be important for large data sets, and the multivariate case. And there doesn't seem to be a clear path forward for integrating it with other HMM-like algorithms (e.g. MEMM or CRF) other than integrating new libraries.  Could be nice to have it all under one roof.
The path I plan to try out first is to write our own HMM estimator for at least the simple case.  Can we actually make it faster than the vanilla version in hmmlearn?  How easy will it be to build on this basic version?

Will reference PRs/make comments below to update progress.
@fkiraly @lmmentel",9affe4e1e99732f39aecb115b59628e44f9e2f2b,e2817ece85376d83c91043362e8a3a123617a229,"['sktime/annotation/base/_base.py', 'sktime/annotation/hmm.py', 'sktime/annotation/tests/test_all_annotators.py', 'sktime/annotation/tests/test_hmm.py', 'sktime/utils/validation/annotation.py']"
90,sktime/sktime,https://github.com/sktime/sktime/issues/2849,2850,"Is your feature request related to a problem? Please describe.
The docstring of evaluate does not indicate how to use param scoring  with sktime's metrics. It will be more convenient to get a working example of how to apply metrics to scoring and how to benchmark point forecast, interval forecast right in the method's docstrings.
Describe the solution you'd like
A docstring code example of how to evaluate predict and/or pred_intervals.
Describe alternatives you've considered
The only way I knew how to benchmark interval prediction with evaluate was by asking people.
Additional context
Add any other context or screenshots about the feature request here.",ea937073659e7e52a1dbcbe0cba5edc3110fce24,bf0bcb90fbaf579d81148341be376bda4a5ca300,['sktime/forecasting/model_evaluation/_functions.py']
91,sktime/sktime,https://github.com/sktime/sktime/issues/4067,4075,"Describe the bug
A logical bug is present in all DL estimators and is caused by not passing custom parameters to its underlying Network since Network actually makes use of these values to create the DL model of appropriate size. Take a look at the example provided below.
To Reproduce
from pprint import pprint
from classification.deep_learning import CNNClassifier

model = CNNClassifier(
    kernel_size=3,
    n_conv_layers=1,
)

# model.summary() fails due to self.history being None, another bug

pprint(vars(model))
print(""==""*25)
pprint(vars(model._network))
Output:
{'_X_metadata': [],
 '_class_dictionary': {},
 '_estimator_type': 'classifier',
 '_is_fitted': False,
 '_network': CNNNetwork(),
 '_tags_dynamic': {},
 '_threads_to_use': 1,
 'activation': 'sigmoid',
 'avg_pool_size': 3,
 'batch_size': 16,
 'callbacks': None,
 'classes_': [],
 'fit_time_': 0,
 'history': None,
 'kernel_size': 3,
 'loss': 'mean_squared_error',
 'metrics': None,
 'model_': None,
 'n_classes_': 0,
 'n_conv_layers': 1,
 'n_epochs': 2000,
 'optimizer': None,
 'random_state': None,
 'use_bias': True,
 'verbose': False}
==================================================
{'_tags_dynamic': {},
 'activation': 'sigmoid',
 'avg_pool_size': 3,
 'filter_sizes': [6, 12],
 'kernel_size': 7,
 'n_conv_layers': 2,
 'random_state': 0}
Expected behavior
This is happening because of self._network = CNNNetwork() (or corresponding Network) in the constructor of the estimator. The values that we receive are to be passed into the Network.
Additional context
I'm speculating this is the reason why the checks at #4036 are failing despite having very small hyperparameters. In any case, I'll open a PR to resolve this bug.
Versions

D:\Anaconda\envs\sktime-dev\lib\site-packages\_distutils_hack\__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn(""Setuptools is replacing distutils."")

System:
    python: 3.8.13 | packaged by conda-forge | (default, Mar 25 2022, 05:59:00) [MSC v.1929 64 bit (AMD64)]
executable: D:\Anaconda\envs\sktime-dev\python.exe
   machine: Windows-10-10.0.22621-SP0

Python dependencies:
          pip: 22.3.1
   setuptools: 65.5.1
      sklearn: 1.1.3
       sktime: 0.15.0
  statsmodels: 0.13.5
        numpy: 1.22.4
        scipy: 1.9.3
       pandas: 1.5.1
   matplotlib: 3.6.2
       joblib: 1.2.0
        numba: 0.56.4
     pmdarima: 2.0.1
      tsfresh: 0.19.0",4f140f2aee2a6e3a75395d6faf6533db324374e8,6ac90c28498599889e6957b36d0d520f11dbb0bc,"['sktime/classification/deep_learning/base.py', 'sktime/classification/deep_learning/cnn.py', 'sktime/classification/deep_learning/fcn.py', 'sktime/classification/deep_learning/lstmfcn.py', 'sktime/classification/deep_learning/mlp.py', 'sktime/classification/deep_learning/tapnet.py', 'sktime/networks/tapnet.py', 'sktime/regression/deep_learning/cnn.py', 'sktime/regression/deep_learning/tapnet.py', 'sktime/tests/test_all_estimators.py']"
92,sktime/sktime,https://github.com/sktime/sktime/issues/1079,2609,"Describe the bug
It is not compatible with the case that the growth parameter of prophet is equal to â€˜logisticâ€˜
To Reproduce
from sktime.datasets import load_airline
y = load_airline()
y = y.to_timestamp(freq=""M"")
y_train, y_test = temporal_train_test_split(y, test_size=36)

y_train_df = pd.DataFrame(y_train,index = y_train.index,columns=['y'])
y_train_df[""cap""] = max(y_train_df[""y""])
y_train_df['floor'] = 0

forecaster = Prophet(
    growth = 'logistic'
)

forecaster.fit(y_train_df)
Expected behavior
The growth parameter compatible with prophet is equal to â€˜logisticâ€˜
Additional context
Versions

from sktime import show_versions; show_versions()
System:
python: 3.8.3 (default, Jul  2 2020, 11:26:31)  [Clang 10.0.0 ]
executable: /Users/baixiaotiao/opt/anaconda3/bin/python
machine: macOS-10.16-x86_64-i386-64bit
Python dependencies:
pip: 20.1.1
setuptools: 49.2.0.post20200714
sklearn: 0.24.2
sktime: 0.6.1
statsmodels: 0.12.2
numpy: 1.20.3
scipy: 1.5.0
Cython: 0.29.21
pandas: 1.2.5
matplotlib: 3.2.2
joblib: 0.16.0
numba: 0.50.1
pmdarima: 1.8.2
tsfresh: None",af197f1781b0d94636b5027650b220a44e57d5f5,0e9400f789ad7b574fd37841cbc0c81e818e8f5e,"['sktime/forecasting/base/adapters/_fbprophet.py', 'sktime/forecasting/fbprophet.py']"
93,sktime/sktime,https://github.com/sktime/sktime/issues/2696,2382,"Describe the issue linked to the documentation
Current Documentation  on the ""method"" parameter for Imputer is limited, especially with the default method: ""drift"". It isn't particularly clear what drift does, or the parameters that are used by the sktime.PolynomialTrendForecaster() mentioned.
Suggest a potential alternative/fix
A clearer explanation of the ""drift"" method, to highlight how it differs from ""linear"", and what parameters are used in the sktime.PolynomialTrendForecaster() for ""drift"".",88c947a240b0b92658a7723a51a3bd2b9e8efb33,f3730811a6cc525d1317f1d6ac352bee3cc739b2,"['sktime/forecasting/model_selection/_tune.py', 'sktime/transformations/series/impute.py']"
94,sktime/sktime,https://github.com/sktime/sktime/issues/2511,2512,"Describe the bug
When using the predict_quantile or predict_interval methods on a ColumnEnsembleForecaster the resulting output contains an extra irrelevant level of just ""quantile"" or ""interval"" respectively.
To Reproduce
from sktime.forecasting.theta import ThetaForecaster
from sktime.forecasting.compose import ColumnEnsembleForecaster

from sktime.utils._testing.series import _make_series
import numpy as np

y = _make_series()
f = ColumnEnsembleForecaster(ThetaForecaster())
fh = np.arange(5)
f.fit(y)
f.predict_quantiles(fh)
Expected behavior
A dataframe of two levels, top level names variables and second level names quantiles, instead a redundant middle level is included just saying ""quantile"".",d8faf8d5b53356ae1ca11b564a801dea3b6a1d1b,7366622443c25c00e532ba66d2d60600a8ab23e3,['sktime/forecasting/compose/_column_ensemble.py']
95,sktime/sktime,https://github.com/sktime/sktime/issues/3009,3129,"From the presentation of @bethrice44:
if we take a ""prediction intervals adder"" wrapper, e.g., one adding predict_intervals, to add prediction intervals to an estimator that already has a different probabilistic prediction method implemented, this may lead to unexpected behaviour.
For example, assume the inner method has predict_quantiles implemented, and predict_intervals uses the default method which calls predict_quantiles.
The wrapper overwrites predict_intervals, and it might not overwrite predict_quantiles. Then, the predictoins from predict_intervals and predict_quantiles will be inconsistent.
Instead, the wrapper perhaps should (?) overwrite predict_quantiles with the base class default?",dfdd248b4ab6c4939ebeba5115532d3c6ea27ce4,97a1b6ec5669108c3a4931ffe2346e4a452b290d,['sktime/forecasting/conformal.py']
96,sktime/sktime,https://github.com/sktime/sktime/issues/2233,2234,"After developing probabilistic metrics in #2232 need to ensure they are compatible with useful features such as grid search for model parameters. There are two key problems that need to be solved for this:


proba metrics take in the output of predict_quantile or predict_interval (or predict_proba) where normal metrics just take predict. This means we need to change what predictions are used inside the grid search.


Some probabilistic metrics have their own hyperparameters. For example the quantile used in a pinball loss. Currently this is inferred from the data inputted, however for a grid search we will need to somehow tell it what quantile to produce.


To solve 1. could either create some set_default function which determines what the forecaster implements for predict (_predict, _predict_quantile or _predict_interval) or use tags inside the grid search evaluation that retrieves the type of metric being used and calls the corresponding predict function.
To solve 2. we could do a small refactor to the probabilistic metrics, where we specify the hyperprameter(s) we want and it retrieves the correct data from the input (and raises an error if it isn't there). This will allow it to require a specific quantile but reduces flexibility as a user will have to instantiate a new metric class for each different set of quantiles they want to evaluate.",3125d0d9ba8f99bcc2bfb5e94b374a0b80ccfb98,8c1e2c965c868b4e847603dcde6f12be7c59f90b,"['sktime/forecasting/model_evaluation/_functions.py', 'sktime/forecasting/prob_metric_integration.ipynb', 'sktime/performance_metrics/base/_base.py', 'sktime/performance_metrics/forecasting/probabilistic/_classes.py', 'sktime/performance_metrics/forecasting/probabilistic/tests/test_probabilistic_metrics.py']"
97,sktime/sktime,https://github.com/sktime/sktime/issues/3474,3481,"Is your feature request related to a problem? Please describe.
Migrating TapNet Regressor model from sktime-dl to sktime.",4cd0daa3ad85b58a547869452b30e223f052b91a,23d41856e590a3db57b3be39167fa6515383153f,"['sktime/regression/deep_learning/tapnet.py', 'sktime/tests/_config.py']"
98,sktime/sktime,https://github.com/sktime/sktime/issues/1687,1874,"We're currently working on a major refactor of the forecasting core interfaces to allow probabilistic and interval predictions.
The design is here:
https://github.com/sktime/enhancement-proposals/blob/main/steps/13_proba_forecasting/step.md
Major work items:
Design:

 STEP (design) forecasting interval and probabilistic prediction interface refactor/re-designÂ #984

Interface/framework refactor:

 framework: predict_interval and predict_quantiles
 framework: predict_var
 framework: predict_proba

Refactoring or adding individual methods:

 predict_interval and predict_quantiles

 ARIMA,  AutoARIMA (inherit from same intermediate adaptor class) [ENH] Add predict_quantiles to ets, pmdarima adapterÂ #1874
 AutoETS [ENH] Add predict_quantiles to ets, pmdarima adapterÂ #1874
 BATS/TBATS - [ENH] Prediction intervals refactor: BATS/TBATS; bugfix for #1625; base class updates on predict_quantilesÂ #1842 by @k1m190r
 ThetaForecaster - Implement predict_quantiles and predict_intervals in theta forecasters, tests in base classÂ #1579
 Prophet


 predict_var

 ARIMA,  AutoARIMA (inherit from same intermediate adaptor class)
 AutoETS
 BATS/TBATS
 ThetaForecaster
 Prophet


 predict_proba

 ARIMA,  AutoARIMA (inherit from same intermediate adaptor class)
 AutoETS
 BATS/TBATS
 ThetaForecaster
 Prophet



Capstone activities:

 refactor capstone

 predict_interval and predict_quantiles
 predict_var
 predict_proba


 deprecation complete",8f5c3b03f4aca1b1d5531b885746bc4632fe1a4b,79cc513346b1257a6f3fa8e4ed855b5a2a7de716,"['sktime/forecasting/base/_base.py', 'sktime/forecasting/base/adapters/_pmdarima.py', 'sktime/forecasting/ets.py', 'sktime/forecasting/tests/test_all_forecasters.py']"
99,sktime/sktime,https://github.com/sktime/sktime/issues/3907,3915,"Numpy 1.23 has been out for almost half a year now, and numpy 1.24 is on the horizon, as it is currentlly in the RC2 stage.
But sktime 0.14.1 has the requirement numpy>=1.21.0,<1.23.
See also:

https://numpy.org/doc/stable/release.html
https://github.com/numpy/numpy/releases/tag/v1.23.0
https://github.com/numpy/numpy/releases/tag/v1.24.0rc2",0442f46d8c00940f3329d325a09628c252266b94,9e093f7c735157ac15476e06880f69c039a0f922,"['pyproject.toml', 'sktime/performance_metrics/forecasting/_classes.py', 'sktime/performance_metrics/forecasting/_functions.py', 'sktime/performance_metrics/forecasting/tests/__init__.py', 'sktime/performance_metrics/forecasting/tests/test_metrics.py']"
100,sktime/sktime,https://github.com/sktime/sktime/issues/2968,2970,"It seems that, sporadically, RandomIntervalSegmenter produces a DataFrame with duplicate column names. See this CI run (mac 3.10): https://github.com/alan-turing-institute/sktime/runs/7271784537",213770bdf83a9af3acb8dbfd07d5d6aa5347939b,3fea839023ad1ecc9ebfbc20b5bcf490ec93b021,"['sktime/transformations/panel/segment.py', 'sktime/transformations/panel/tests/test_segment.py']"
101,sktime/sktime,https://github.com/sktime/sktime/issues/3816,3820,"Describe the bug
test_deep_estimator_full[keras-adamax] is failing due to a type check in keras. See, for example
https://github.com/sktime/sktime/actions/runs/3495913658/jobs/5855642783
""ValueError: optimizer of type <class 'keras.optimizers.optimizer_v2.adamax.Adamax'> cannot be serialized, it should either be absent/None/str/tf.keras.optimizers.Optimizer object""
it could be that the wrong type of Optimizer is being used, output states
Optimizer  = <class 'keras.optimizers.optimizer_experimental.optimizer.Optimizer'>
whereas it is expecting
tf.keras.optimizers.Optimizer

I don't want to hack around in the testing suite, I'll let someone who understands it better do that,",e4fa17851ac888efacb78de3d626a70809e09c3b,e64ad1b55223e488f1b6cacb990aa933becf7930,"['sktime/classification/deep_learning/base.py', 'sktime/classification/tests/test_base.py', 'sktime/regression/deep_learning/base.py']"
102,sktime/sktime,https://github.com/sktime/sktime/issues/1364,2458,"From the Aug 27 developer meeting.
We need to ensure that the following compositors work for multivariate forecasters, in descending order of priority

 pipeline compositors, TransformedTargetForecaster, ForecastingPipeline Added multivariate TransformedTargetForecaster, ForecastingPipeline, BaseGridSearch, MultiplexForecasterÂ #1376
 tuning compositors, ForecastingGridSearchCV, ForecastingRandomizedSearchCV Added multivariate TransformedTargetForecaster, ForecastingPipeline, BaseGridSearch, MultiplexForecasterÂ #1376
 reducers, all in the _reduce module, see [ENH] multivariate compositor/reducer, that creates multivariate method by applying transformers/forecasters per-variableÂ #1038
 multiplexing, MultiplexForecaster
 ensembles, EnsembleForecaster, AutoEnsembleForecaster
 stacking, StackingForecaster

Some of these might be slightly challenging, but still good first issues.
As usual, we use this issue to track who is working on what, please indicate your pick before starting work on it.",e77249f3b8b67272ff5f65e3d82207e13ab17b0b,c427cd5180544a4c56ff721c22beb5531566d45e,"['sktime/base/_meta.py', 'sktime/classification/compose/_pipeline.py', 'sktime/forecasting/base/_meta.py', 'sktime/forecasting/compose/_multiplexer.py', 'sktime/forecasting/compose/_pipeline.py', 'sktime/transformations/compose.py']"
103,sktime/sktime,https://github.com/sktime/sktime/issues/3748,3760,"See #3743 and #2986 - soft dependency imports in non-suite tests should:
(a) happen inside tests, and
(b) be isolated via pytest.skipif
Currently, some keras imports are not properly isolated.
The idea is that the full test suite should be runnable without any soft dependencies installed.",d1c529c567b56dfeefe020e764330c24b01d7396,7780b347ea6475cab9ed651b88ef2f0d9bdca19f,"['sktime/classification/deep_learning/fcn.py', 'sktime/classification/tests/test_base.py', 'sktime/forecasting/model_evaluation/tests/test_evaluate.py', 'sktime/utils/plotting.py']"
104,sktime/sktime,https://github.com/sktime/sktime/issues/3159,3167,"Describe the bug
If I execute ARIMA().fit with numpy arrays, it will fails with a strange error message. It says that the indexes of the input Time Series does not fit to the exogenous variables Time Series. And prints the input time series values as missing index values for the exogenous variables time series.
To Reproduce
rand = np.random.random(1000)
rand_x = np.random.random(1000)
ar = ARIMA()
assert rand.shape == rand_x.shape
ar.fit(rand, X=rand_x)
Expected behavior
ARIMA should be fitted without an error.
Additional context
I suppose the problem is line 269ff in series.py. The supposed index are the values of y. I would expect that the index is something like np.arange(0, len(y). I can implement this fix, but I assume that this would have effects on lots of other transformers too.
        if isinstance(y, np.ndarray):
            y_index = pd.Index(y)
        else:
            y_index = y.index
Versions

machine: Windows-10-10.0.18362-SP0
Python dependencies:
pip: 22.0.3
setuptools: 60.8.2
sklearn: 1.0.2
sktime: 0.13.0
statsmodels: 0.13.2
numpy: 1.21.6
scipy: 1.7.3
pandas: 1.4.1
matplotlib: 3.5.1
joblib: 1.1.0
numba: 0.55.1
pmdarima: 1.8.5
tsfresh: None",7089f05c6047220d8f1c32df29662b4a7a4823d8,55570d66e002497884b6259239d968cec7987b8a,"['.all-contributorsrc', 'sktime/utils/validation/series.py', 'sktime/utils/validation/tests/test_series.py']"
105,sktime/sktime,https://github.com/sktime/sktime/issues/2573,3010,"We need to have a design decision on conflicting elements of our current interface specification.
These are:

__init__ should contain no logic for input checking, and optimally no other logic. Such logic should be placed at the start of _fit
dynamic tags need to be set in the __init__, since tags are read at the start of the fit call which precedes the _fit call
we have the fit_is_empty tag, which can mean that _fit is never called for a given estimator, as well as estimators/object types that have no fit (such as distances)

Here, 1. conflucts with 2. and with 3., but 2. and 3. do not conflict with each other (I think).
I feel we need to think about potential ways to resolve this situation.
Related: #1798",5767dbaac759ed6ac43a8977f4c46beb93d0f18c,ef0d77625aefd21ab3f9a7ad989a3ee60938a7a4,"['extension_templates/annotation.py', 'extension_templates/classification.py', 'extension_templates/clustering.py', 'extension_templates/dist_kern_panel.py', 'extension_templates/dist_kern_tab.py', 'extension_templates/early_classification.py', 'extension_templates/forecasting.py', 'extension_templates/forecasting_simple.py', 'extension_templates/transformer.py', 'extension_templates/transformer_simple.py']"
106,sktime/sktime,https://github.com/sktime/sktime/issues/2247,2248,"We can now use the new bootstrapping transformers (MovingBlockBootstrap, STLBootstrapTransformer) and the new vectorize feature  to implement a BaggingForecaster as described in:

Bergmeir, C., Hyndman, R. J., & BenÃ­tez, J. M. (2016). Bagging exponential smoothing methods using STL decomposition and Box-Cox transformation. International Journal of Forecasting, 32(2), 303-312
Hyndman, R.J., & Athanasopoulos, G. (2021) Forecasting: principles and practice, 3rd edition, OTexts: Melbourne, Australia. OTexts.com/fpp3, Chapter 12.5. Accessed on February 13th 2022.

The point prediction should be the mean of the bootstrapped time series predictions and the distributional forecast should be a different tfp.distributions.Empirical for each time step!
link to the online version of [2]: https://otexts.com/fpp3/bootstrap.html",75f85633cd67c1241231ed0f0db0d1d52fec22cf,0ac21dd5339c852c8afc52ef50e93e800eb46432,"['docs/source/api_reference/forecasting.rst', 'sktime/forecasting/compose/__init__.py', 'sktime/forecasting/compose/_bagging.py', 'sktime/forecasting/compose/tests/test_bagging.py']"
107,sktime/sktime,https://github.com/sktime/sktime/issues/3315,3340,"Default for this argument is actually True.
https://github.com/alan-turing-institute/sktime/blob/824804c785fb97e20c29045ee058181ceeb9418b/sktime/forecasting/model_selection/_split.py#L1107",86506e17539dfee3073a4eb36323909bc5789f99,b4db673be58a68d01a83fc47fda6a842b2faf113,"['sktime/forecasting/model_selection/_split.py', 'sktime/transformations/series/boxcox.py']"
108,sktime/sktime,https://github.com/sktime/sktime/issues/2441,2454,"Hey ðŸ‘‹ ! Its seems the UnobservedComponents model wrapper (see here) does not support prediction intervals yet right? I would be nice to have them as I do not think is really hard because of the statsmodels implementation.
I can work on this one in the upcoming weeks!",a5bc0930fca040177e8518d0cbe2f271e1108e8a,2b7d4b3e832f8499e290d4733c75240f879ab581,"['sktime/forecasting/structural.py', 'sktime/forecasting/tests/test_structural.py']"
109,sktime/sktime,https://github.com/sktime/sktime/issues/3084,3085,"Describe the bug

When the TransformerPipeline consists of an Imputer followed by a log transformer, predictions do not apply inverse log transform (seems to be a bug)
When the TransformerPipeline consists of only a log transformer, predictions seem to be correct (inverse transform is applied correctly).

To Reproduce
from sktime.datasets import load_airline
from sktime.transformations.series.boxcox import LogTransformer
from sktime.transformations.series.impute import Imputer
from sktime.transformations.compose import TransformerPipeline
from sktime.forecasting.compose import ForecastingPipeline, TransformedTargetForecaster
from sktime.forecasting.naive import NaiveForecaster

preprocess = TransformerPipeline(
    [(""imputer"", Imputer()), (""transform"", LogTransformer())]
)

pipeline = ForecastingPipeline(
    [
        (
            ""forecaster"",
            TransformedTargetForecaster(
                [
                    (""preprocess"", preprocess),
                    (""model"", NaiveForecaster()),
                ]
            ),
        )
    ]
)

y = load_airline()

# Fit Pipeline
pipeline.fit(y)

# Peedict
preds = pipeline.predict([1, 2, 3])

print(preds)

Expected behavior
Predictions should be around the same magnitude as the data but it looks like the inverse transform is not applied for the LogTransformer when Imputer is also present in the TransformerPipeline
Additional context
May be important. The above code works as expected if the Imputer is removed from the TransformerPipeline, i.e. change this line
preprocess = TransformerPipeline(
    [(""transform"", LogTransformer())]
)
Versions

System:
python: 3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]
executable: C:\Users\Nikhil.conda\envs\pycaret_dev_sktime_13p0\python.exe
machine: Windows-10-10.0.19044-SP0
Python dependencies:
pip: 22.1.2
setuptools: 61.2.0
sklearn: 1.1.1
sktime: 0.13.0
statsmodels: 0.13.2
numpy: 1.22.4
scipy: 1.8.1
pandas: 1.4.3
matplotlib: 3.5.2
joblib: 1.1.0
numba: 0.55.2
pmdarima: 1.8.5
tsfresh: None",028350a6372be8a1c597f0f8292d001d7cca8de9,ca4e56c0c99a359d50ac140dac3f39de1c685afe,"['sktime/transformations/base.py', 'sktime/transformations/compose.py', 'sktime/transformations/hierarchical/aggregate.py', 'sktime/transformations/series/impute.py', 'sktime/transformations/tests/test_all_transformers.py', 'sktime/transformations/tests/test_compose.py']"
110,sktime/sktime,https://github.com/sktime/sktime/issues/2935,2991,"Is your feature request related to a problem? Please describe.
As discussed in the daily-stand up on 05-07-12, I wanted to propose some enhancements / changes to CNN Classifier (and hence to Deep Learning based estimators)
Describe the solution you'd like

 Moving the convert_y_to_keras function in BaseDeepClassifier into a utils function
 Enable returning the label_encoder and one hot encoder to the user from convert_y_to_keras (it can be useful for the users in case they want to use it to encode other data / or to decode data later on)
 Add option to have multiple linear layers in output layer / different activation function. Also add the use_bias parameter to output layer. (simply as a customisation option to users, multiple linear layers are useful to create deeper networks depending on dataset / choice of activation function)
 option to choose optimiser (usually its Adam but just for customisation's sake we can give this as option as well). Also should allow specifying lr either within the optimiser or as parameter to the class itself.
 Accept parameter as input in CNNClassifier to specify random seed
 Train/fit the model on GPU depending on users choice (maybe take an input parameter for this as well)
 Option to view / return the loss of fit (either as a graph or simply as a list / array ) / A better idea is to add a summary function which can return model summary as well as fit history.

Additional context
These are some additional features / changes I wanted to add to the CNNClasifier. Any inputs / additional comments are appreciated",efec3a570bf67fe4f000fbe5f0bbdc54cf937b0b,84b32054babb2f3dc83342cde3baed3c5291cae5,"['sktime/classification/deep_learning/base.py', 'sktime/classification/deep_learning/cnn.py']"
111,sktime/sktime,https://github.com/sktime/sktime/issues/3191,3233,"Is your feature request related to a problem? Please describe.
On the topic of migrating sktime-dl models to sktime, adding FCN model for classification from sktime-dl",5e8666a2a7f5e86c300e1761d5aeaec57adf994f,468b6135b2691d3d39ec59a35a40112cca51adf5,"['sktime/classification/deep_learning/fcn.py', 'sktime/networks/fcn.py', 'sktime/tests/_config.py']"
112,sktime/sktime,https://github.com/sktime/sktime/issues/3153,3171,"Is your feature request related to a problem? Please describe.
In line with discussions and #2894 , adding CNTC model for classification from sktime-dl",ad73b4042390b4af902f82660d7bb06784b9514b,1a19706bb1b0c931e51bfc4128ce068ae45cb6ec,"['pyproject.toml', 'sktime/classification/deep_learning/cntc.py', 'sktime/networks/base.py', 'sktime/networks/cntc.py', 'sktime/tests/_config.py']"
113,sktime/sktime,https://github.com/sktime/sktime/issues/2386,2418,"registry.all_estimators shows deprecation warnings from module level in cases where estimators have been moved.
These should by default not be raised, only a direct import of the deprecated location should raise these.",7d6c8904106fdb06b9747b03f1e89f34310761a5,98fe13992312d63f82ea229aadf88f3bec2ced56,"['sktime/alignment/dtw_python.py', 'sktime/registry/_lookup.py', 'sktime/utils/validation/_dependencies.py']"
114,sktime/sktime,https://github.com/sktime/sktime/issues/3169,3231,"Describe the bug
As per the discussion in #2697, it was found that _load_provided_dataset() must throw an error if return_type is not a reference to pd.DataFrame and return_X_y=False. This happens because of code appends the response variable into predictors under the column name 'class_val'.
To Reproduce
import sktime
from sktime.datasets._data_io import _load_provided_dataset

def test_load_provided_dataset():
    name = ""UnitTest""                                    # referencing to dataset available for testing in the directory 'datasets/data/'
    X, y = _load_provided_dataset(name, return_type='np2D', return_X_y=False)
    print(X.shape, y.shape)

if __name__ == ""__main__"":
    print(""Staring program"")
    test_load_provided_dataset()
    print(""Done"")
Output of above code
PS D:\Projects\sktime\sktime\sktime\datasets\tests> D:/Anaconda/envs/sktime-dev/python temp_test_data_io.py
Staring program
Traceback (most recent call last):
  File ""temp_test_data_io.py"", line 11, in <module>
    test_load_provided_dataset()
  File ""temp_test_data_io.py"", line 6, in test_load_provided_dataset
    X, y = _load_provided_dataset(name, return_type='np2D', return_X_y=False)
  File ""d:\projects\sktime\sktime\sktime\datasets\_data_io.py"", line 224, in _load_provided_dataset
    X[""class_val""] = pd.Series(y)
IndexError: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices

Expected behavior
This clearly is an issue because we are not checking the instance type of X, which could be np.array or pd.DataFrame.
Solutions(?)


Solution 1: Check instance and append in the appropriate axis
Consider the following cases

Univariate, 2D, equal length data (X=np.random.randn(20, 24), y=np.random.randn(20,)) then output.shape=(20,25)
Univariate 3D, equal length data (X=np.random.randn(20,1,24), y=np.random.randn(20,)) then output.shape=(20,1,25)
Multivariate, equal length data (X=np.random.randn(20,6,24), y=np.random.randn(20,)) then output.shape=(20,6,25); requires tiling labels which might not be desirable.

I think it should also work for unequal length data but I need confirmation. The only drawback of this solution apart from the repetition of y in the multivariate case is that it might be less intuitive.


Solution 2: Force convert the data into pandas dataframe and give a warning about the conversion.


Final Comments

There are more solutions to this but I think these two are the most straightforward. I also have the code ready for the first solution and will send a pull request if that is chosen, otherwise I'll take a day's time to send a pull request if we move forward with any other solution.
I can also add unit tests regarding the same, I don't know if having tests for private methods are a thing or not, but I think it won't hurt having more tests.

Versions

D:\Anaconda\envs\sktime-dev\lib\site-packages\_distutils_hack\__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn(""Setuptools is replacing distutils."")

System:
    python: 3.8.13 | packaged by conda-forge | (default, Mar 25 2022, 05:59:00) [MSC v.1929 64 bit (AMD64)]
executable: D:\Anaconda\envs\sktime-dev\python.exe
   machine: Windows-10-10.0.22000-SP0

Python dependencies:
          pip: 22.1.2
   setuptools: 63.1.0
      sklearn: 1.1.1
       sktime: 0.13.0
  statsmodels: 0.13.2
        numpy: 1.22.4
        scipy: 1.8.1
       pandas: 1.4.3
   matplotlib: 3.5.2
       joblib: 1.1.0
        numba: 0.55.2
     pmdarima: None
      tsfresh: None",cda9124f531762dfa59758ca5713784ba56cc8b5,beb1dc44eedfe50f0c576f60f8f305856ee0c448,"['.all-contributorsrc', 'sktime/datasets/_data_io.py', 'sktime/datasets/_single_problem_loaders.py', 'sktime/datasets/tests/test_data_io.py']"
115,sktime/sktime,https://github.com/sktime/sktime/issues/3372,3386,"Is your feature request related to a problem? Please describe.
Migrating TapNet Classifier model from sktime-dl to sktime.",23d41856e590a3db57b3be39167fa6515383153f,56fb2ce9d9d36b84c4baa5afd5543a4d76abd93e,"['pyproject.toml', 'sktime/classification/deep_learning/tapnet.py', 'sktime/networks/tapnet.py', 'sktime/tests/_config.py']"
116,intel/dffml,https://github.com/intel/dffml/issues/254,268,"In the Integrating Machine Learning usage example between the Making a Prediction and Modifying the Legacy App headings we need to add a section where we show how to use the Trained Model Operation to make the prediction, rather than the other CLI call.
In the new section we need to go over how we construct the dataflow.
elif action == 'predict':
    today = datetime.now().strftime('%Y-%m-%d %H:%M')
    subprocess.check_call([
        ""dffml"", ""dataflow"", ""run"", ""repos"", ""set"",
        ""-keys"", query['URL'],
        ""-repo-def"", ""URL"",
        ""-dataflow"", os.path.join(os.path.dirname(__file__), ""dataflow.yaml""),
        ""-sources"", ""db=demoapp"",
        ])
    result = subprocess.check_output([
        'dffml', 'predict', 'repo',
        '-keys', query['URL'],
        '-model', 'tfdnnc',
        '-model-classification', 'maintained',
        '-model-classifications', '0', '1',
        '-sources', 'db=demoapp',
        '-features',
        'def:authors:int:10',
        'def:commits:int:10',
        'def:work:int:10',
        '-log', 'critical',
        '-update'])
    result = json.loads(result)
    cursor.execute(""REPLACE INTO status (src_url, maintained) VALUES(%s, %s)"",
                   (query['URL'], result[0]['prediction']['value'],))
    cnx.commit()
    print json.dumps(dict(success=True))
will turn into:
elif action == 'predict':
    subprocess.check_call([
        ""dffml"", ""dataflow"", ""run"", ""repos"", ""set"",
        ""-keys"", query['URL'],
        ""-repo-def"", ""URL"",
        ""-dataflow"", os.path.join(os.path.dirname(__file__), ""prediction_dataflow.yaml""),
        ""-sources"", ""db=demoapp"",
        ])
    print json.dumps(dict(success=True))
This issue is probably worth spiting into smaller issues. Since off the top of my head I'd say it involves at least these sub-tasks.

 Create a model_predict Operation (operations: models: Create operations that use trained modelsÂ #215)

This operation will do almost the same thing as the dffml predict repo CLI command.
Done: model_predict operation


 Create a run_dataflow Operation

This operation will do almost the same thing as the dffml dataflow run repos set CLI command.
This operation takes a dataflow in its configuration (See remap operation for an example of how this can be done, when calling self.octx.parent( make sure not to pass ictx. ictx is only passed in remap because we are executing an output subflow from within an output operation)
For the initial test of this we want to just paste the old dataflow into the new one under the config for this operation.
Eventually we'll want to make a ConfigLoader that takes a directory as the argument to loadb config: Directory ConfigLoaderÂ #255


 Create an operation that preforms a sql query. The config for the operation should take a class similar to a source which represents a sql database. We need to make a new base class for it the same way that all the base classes are created (reference dffml/df/base.py BaseKeyValueStore for an example of how to do this). Then create an instance of it that understands how to interact with a sqlite database (reference dffml/df/memory.py MemoryKeyValueStore for an example of how to do this). Don't use the aiosqlite module, instead use asyncio.Lock and the sqlite module from the standard library (the new source tutorial uses aiosqlite, but will probably be enough to get you going in the right direction). The goal here is to provide a minimal working interaction with the database, while keeping the dependencies to only the standard library.
 Any other operations that might be needed to tie these all together...
 Create the dataflow prediction_dataflow.yaml using these new operations and any others needed to replicated the functionality of the two command line calls (because the point here is that we're making it one command line call).
 Update the tutorial documentation
 Test that the updated documentation works, start to finish.",211f0b62e8c032b0a3bd0df8d105c2df1c0eb552,a65a0cc4daa1510c7a301245f216c0f379317874,"['CHANGELOG.md', 'dffml/operation/dataflow.py', 'setup.py', 'tests/operation/test_dataflow.py', 'tests/test_df.py']"
117,intel/dffml,https://github.com/intel/dffml/issues/537,634,"'module' object is not callable
This error shows when you run scratch or tensorflow hub models in python. But works perfectly for scikit
Steps To Reproduce
from dffml import CSVSource, Features, DefFeature
from dffml.noasync import train, accuracy, predict
from dffml_model_tensorflow_hub import text_classifier


model = text_classifier(
features = Features(
DefFeature(""Sentence"" , str , 1),
),
    predict = DefFeature('Sentiment', int ,1)

)
train(model , {'I hate you!!' , 2} , {'Okay , so what do we do now?' , 0} ,
     {'I find this dish incredibly tasty' , 1} , {'This movie is horrible' , 2} ,
     {'You look beautiful' , 1} , {'I think your project could use more collabaration' , 0}
     ,{'I cannot see any effort put here' ,2} , {'This is exactly what I wanted',  1})

print(accuracy(model,
              {'This is horrific', 2} , {'Its okay never mind' , 0} , {'This time could not have been better' , 1}))

for i, features, prediction in predict(
    model,
    {'This is horrific'} , {'Its okay never mind'} , {'This time could not have been better'}
):
    features[""Sentiment""] = prediction[""Sentiment""][""value""]
    print(features)
Packages in use and versions

dffml==0.3.5",281fcfe29fc1f80de9092ed27fbaab4b6c5680ec,1635ed60627139f3cb50d6a8e5fbb3fcd678d1e4,"['CHANGELOG.md', 'model/tensorflow_hub/dffml_model_tensorflow_hub/text_classifier.py', 'model/tensorflow_hub/examples/tfhub_text_classifier/__init__.py', 'model/tensorflow_hub/examples/tfhub_text_classifier/accuracy.sh', 'model/tensorflow_hub/examples/tfhub_text_classifier/predict.sh', 'model/tensorflow_hub/examples/tfhub_text_classifier/test_data.sh', 'model/tensorflow_hub/examples/tfhub_text_classifier/test_textclassifier.py', 'model/tensorflow_hub/examples/tfhub_text_classifier/textclassifier.py', 'model/tensorflow_hub/examples/tfhub_text_classifier/train.sh', 'model/tensorflow_hub/examples/tfhub_text_classifier/train_data.sh']"
118,intel/dffml,https://github.com/intel/dffml/issues/316,340,"To maintain consistancy with the other models, we should rename the classification property to predict in dnnc.py
Reporter: @aghinsa",ff50d71a2817b4a138d6e83ada1fdb6d162fff99,1180ba9d1008cc3fb09240925443403883d0994d,"['docs/plugins/dffml_model.rst', 'docs/usage/integration.rst', 'examples/maintained/cgi-bin/api-ml.py', 'model/tensorflow/dffml_model_tensorflow/dnnc.py', 'model/tensorflow/tests/test_dnnc.py']"
119,intel/dffml,https://github.com/intel/dffml/issues/1181,1226,"Pain Point
As of the recent merges, the high-level accuracy() function returns scores w.r.t different scorers selected. These could also be a resultant value of error.
Accuracy generally follows the phenomenon of ""greater is better"" but that is no longer the case here.
This could possibly be confusing for new users.
Proposed Solution
Change accuracy() to something like score() or evaluate()",4fff2d6c8e46bbe09c4c465ac9951fa5ff5ca064,aeff29890f58bd9b3a64f5754e61c4d57945c9dc,['CHANGELOG.md']
120,intel/dffml,https://github.com/intel/dffml/issues/435,634,"The model plugins page has command line examples but needs examples from Python as well.
Copy the format of the examples/quickstart_filenames.py file and add a .. code-block:: python section for each model, keeping the Python example consistent with the command line example for each model. Let's add the python example after the CLI example for each.
For the tensorflow hub model, the docstring of the Model needs to be updated.
Edit:
Here's a good example from when tensorflow was updated: e7aaeb1",281fcfe29fc1f80de9092ed27fbaab4b6c5680ec,1635ed60627139f3cb50d6a8e5fbb3fcd678d1e4,"['CHANGELOG.md', 'model/tensorflow_hub/dffml_model_tensorflow_hub/text_classifier.py', 'model/tensorflow_hub/examples/tfhub_text_classifier/__init__.py', 'model/tensorflow_hub/examples/tfhub_text_classifier/accuracy.sh', 'model/tensorflow_hub/examples/tfhub_text_classifier/predict.sh', 'model/tensorflow_hub/examples/tfhub_text_classifier/test_data.sh', 'model/tensorflow_hub/examples/tfhub_text_classifier/test_textclassifier.py', 'model/tensorflow_hub/examples/tfhub_text_classifier/textclassifier.py', 'model/tensorflow_hub/examples/tfhub_text_classifier/train.sh', 'model/tensorflow_hub/examples/tfhub_text_classifier/train_data.sh']"
121,autonomio/talos,https://github.com/autonomio/talos/issues/106,109,"I wonder if Talos can be used for regression problems as well. I would like to simply monitor the validation loss for each combination of hyperparameters. When I call ta.Scan, I get an error about predict_classes (becauce I have a functional model) but even when I use the dev version, Talos is not doing what I expect since it still assumes a classification problem.
Thanks in advance for your help",5e69d95000078de32b1ca8ceb600f4b7d8030c6f,98da4888981b7125ef5b94d41e780acb4c9fc673,"['requirements.txt', 'setup.py', 'talos/__init__.py', 'talos/commands/reporting.py', 'talos/metrics/keras_metrics.py', 'talos/metrics/names.py', 'talos/metrics/performance.py', 'talos/metrics/score_model.py', 'talos/scan/Scan.py', 'talos/scan/scan_finish.py', 'talos/scan/scan_prepare.py', 'talos/scan/scan_round.py', 'talos/scan/scan_run.py', 'talos/utils/logging.py', 'test/core_tests/test_scan.py']"
122,pyro-ppl/funsor,https://github.com/pyro-ppl/funsor/issues/573,589,"I'm seeing a lot of complex tests run out of precision, e.g. atol=1e=3, rtol=1e-3, where it becomes difficult to distinguish real errors from numerical imprecision. I think it might be worth switching the default dtypes for tests to float64.  This seems easy for FUNSOR_BACKEND=torch, but I'm unsure about numpy and jax.
@fehiepsi how difficult would it be to switch our CI tests to float64 for FUNSOR_BACKEND=jax?",68868e0b8eee9a1cd98548d5baa96044cee58c3e,821c1b4c099cd986dd409c807b424857bcbf9c8f,"['.github/workflows/ci.yml', '.readthedocs.yml', 'README.md', 'docs/source/conf.py', 'funsor/distribution.py', 'funsor/domains.py', 'funsor/interpreter.py', 'funsor/ops/builtin.py', 'funsor/ops/op.py', 'funsor/testing.py', 'funsor/torch/ops.py', 'funsor/util.py', 'setup.cfg', 'setup.py', 'test/conftest.py', 'test/examples/test_sensor_fusion.py', 'test/pyro/test_hmm.py', 'test/test_distribution.py', 'test/test_tensor.py', 'test/test_terms.py', 'test/torch/test_ops.py', 'test/torch/test_provenance.py']"
123,pyro-ppl/funsor,https://github.com/pyro-ppl/funsor/issues/70,161,"@eb8680 suggested adding metadata to Domain objects or .input dicts to declare which inputs are plate indices. This is similar to @jpchen's suggestion of a Cat funsor, but would also support cat-broadcasting. From a mathematical perspective, adding funsors with different plate indices would be a direct sum.
I believe the best way to implement this is to first implement correct math in funsor.minipyro.log_joint and then to refactor it into Domain or Funsor.
Tasks

 Update minipyro storyboard to correctly handle platesÂ #56 sketch plate math in funsor.minipyro.log_joint
 Fix funsor.minipyro and add testsÂ #57 fix minipyro and add tests
 Implement an Expectation funsorÂ #109 add plate-aware contract/expectation ops
 Add a funsor Conv(x, time_dim, curr_dim, next_dim, new_dim) satisfying
Conv(f, ""time"", ""curr"", ""next"", ""curr"")(curr=x)
    = f(curr=x(time=arange(""time"", f.inputs[""time""].dtype - 1)),
        next=x(time=arange(""time"", f.inputs[""time""].dtype - 1) + 1))

 Implement log-time sequential sum product functionÂ #161 Implement log(time) convolution op following this paper
 Implement generalized sequential sum-product operationÂ #292 Generalize sequential_sum_product to handle longer time lags
 Support time dims in partial_sum_product (see convolve-method branch)
 Add a state space example using Conv applied to a contiguous stacked nn output",b2e116910c9e193de86d1808e870ad382eb0fc0c,3a80b2952dbd3716b53049d731499965afd40ee7,"['funsor/domains.py', 'funsor/sum_product.py', 'test/test_sum_product.py']"
124,pyro-ppl/funsor,https://github.com/pyro-ppl/funsor/issues/19,36,"A simple discrete eager HMM without plates seems like the simplest end-to-end example we could get working that would exercise lots of funsor machinery at once.
Tasks

 Write a complete but xfailing example script that we can develop against
 Figure out what's missing/broken, especially in distributions",f7545f18fb8bc902d225b72fd93420983f0631d0,5f4d8d5f9bec44cbbbf9fe984b19b98d7b4a2ecd,"['Makefile', 'examples/discrete_hmm.py', 'funsor/distributions.py', 'funsor/torch.py']"
125,pyro-ppl/funsor,https://github.com/pyro-ppl/funsor/issues/9,47,"Currently the optimizer handles only non-plated variable elimination, but we'd like to additionally support tensor variable elimination.",c47a17f9db2f97d9cda8fc2155cafc73058977eb,4537e903b4788c6c01d2fd3884485fc7e8dc74e4,"['funsor/optimizer.py', 'test/test_einsum.py', 'test/test_optimizer.py']"
126,kymatio/kymatio,https://github.com/kymatio/kymatio/issues/637,670,"Hi everybody,
I have included in a Keras model a scattering layer using the following syntax:
import kymatio
from kymatio.keras import Scattering2D

# Model architecture
def Dual_ResNet_Atrous_Att_WST(input_shape=(HEIGHT, WIDTH, CHANNELS),
                   mra_channels=MRACHANNELS,
                   num_class=NUM_CLASS,
                   ks1=KS1, ks2=KS2, ks3=KS3, 
                   dl1=DL1, dl2=DL2, dl3=DL3,
                   filters=NF,resblock1=NR1,
                   r_filters=NFL, resblock2=NR2,
                   dil_mode=DIL_MODE, 
                   sp_dropout=DR1,re_dropout=DR2,
                   mask_float=True,
                   attention=None,
                   mask_threshold=0.5,
                   return_seq=True):

    input_1 = Input(shape=(input_shape[0],input_shape[1],mra_channels))    
    input_2 = Input(shape=input_shape)
    input_3 = tf.where(tf.greater(input_2,mask_threshold),1.0,0.0)
    
    if attention == 'wst':
        # wavelet scattering transform (WST)
        d01 = UpSampling2D(size=(4, 4), interpolation=""bilinear"")(input_1)
        d02 = Lambda(lambda x: x[:,:,:,0])(d01)
        d03 = Scattering2D(J=2,L=6)(d02)
        d0 = tf.transpose(d03,perm=[0, 2, 3, 1])
    
    for cycle in range(resblock1):
        if cycle == 0:            
            d1 = stem_split_3k(d0, filters, mode=dil_mode, 
                                kernel_size_1=ks1, kernel_size_2=ks2, kernel_size_3=ks3, 
                                dilation_1=dl1, dilation_2=dl2, dilation_3=dl3,
                                kernel_regularizer=None)                      
        else:
            d1 = residual_block_split_3k(d1, filters, mode=dil_mode, 
                                         kernel_size_1=ks1, kernel_size_2=ks2, kernel_size_3=ks3, 
                                         dilation_1=dl1, dilation_2=dl2, dilation_3=dl3,
                                         kernel_regularizer=None) 
            

        d1 = SpatialDropout2D(sp_dropout)(d1)            
                 
    d2 = residual_block_split_3k(d1, num_class, mode=""add"", 
                                    kernel_size_1=ks1, kernel_size_2=ks2, kernel_size_3=ks3, 
                                    dilation_1=dl1, dilation_2=dl2,dilation_3=dl3,
                                    kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4))
        
    d3 = LeakyReLU(alpha=0.1)(d2)
    
    if mask_float:
        d3 = Multiply()([d3, input_2])
    else:
        d3 = Multiply()([d3, input_3])
   
    d3 = GlobalAveragePooling2D()(d3)    
    
    output_1 = Activation(""softmax"", name = 'softmax')(d3)    
    
    model = Model([input_1,input_2], output_1, name='Dual_Res-CR-Net_Att_WST')    

    return model


The model compiles without problems:
# Model compilation
strategy = tf.distribute.MirroredStrategy()
with strategy.scope():
    model = Dual_ResNet_Atrous_Att_WST(input_shape=(HEIGHT, WIDTH, CHANNELS),
                   mra_channels=MRACHANNELS,                          
                   num_class=NUM_CLASS,
                   ks1=KS1, ks2=KS2, ks3=KS3, 
                   dl1=DL1, dl2=DL2, dl3=DL3,
                   filters=NF,resblock1=NR1,
                   r_filters=NFL, resblock2=NR2,
                   dil_mode=DIL_MODE, 
                   sp_dropout=DR1,re_dropout=DR2,
                   mask_float=True,
                   attention='wst',
                   mask_threshold=0.5,                                         
                   return_seq=True)
        
    model.compile(optimizer=Adam(), 
                      loss=tf.keras.losses.CategoricalCrossentropy(), 
                      metrics=[tf.keras.metrics.CategoricalAccuracy()])

model.summary()

Model: Dual_Res-CR-Net_Att_WST
input_1 (InputLayer)            [(None, 300, 340, 1) 0

up_sampling2d (UpSampling2D)    (None, 1200, 1360, 1 0           input_1[0][0]

lambda (Lambda)                 (None, 1200, 1360)   0           up_sampling2d[0][0]

scattering2d (Scattering2D)     (None, 49, 300, 340) 0           lambda[0][0]

tf_op_layer_transpose (TensorFl [(None, 300, 340, 49 0           scattering2d[0][0]

separable_conv2d (SeparableConv (None, 300, 340, 16) 1241        tf_op_layer_transpose[0][0]

separable_conv2d_2 (SeparableCo (None, 300, 340, 16) 2025        tf_op_layer_transpose[0][0]

separable_conv2d_4 (SeparableCo (None, 300, 340, 16) 3201        tf_op_layer_transpose[0][0]

batch_normalization (BatchNorma (None, 300, 340, 16) 64          separable_conv2d[0][0]

batch_normalization_1 (BatchNor (None, 300, 340, 16) 64          separable_conv2d_2[0][0]

batch_normalization_2 (BatchNor (None, 300, 340, 16) 64          separable_conv2d_4[0][0]

leaky_re_lu (LeakyReLU)         (None, 300, 340, 16) 0           batch_normalization[0][0]

leaky_re_lu_1 (LeakyReLU)       (None, 300, 340, 16) 0           batch_normalization_1[0][0]

leaky_re_lu_2 (LeakyReLU)       (None, 300, 340, 16) 0           batch_normalization_2[0][0]

conv2d (Conv2D)                 (None, 300, 340, 48) 2400        tf_op_layer_transpose[0][0]

separable_conv2d_1 (SeparableCo (None, 300, 340, 16) 416         leaky_re_lu[0][0]

separable_conv2d_3 (SeparableCo (None, 300, 340, 16) 672         leaky_re_lu_1[0][0]

separable_conv2d_5 (SeparableCo (None, 300, 340, 16) 1056        leaky_re_lu_2[0][0]

batch_normalization_3 (BatchNor (None, 300, 340, 48) 192         conv2d[0][0]

concatenate (Concatenate)       (None, 300, 340, 48) 0           separable_conv2d_1[0][0]
separable_conv2d_3[0][0]
separable_conv2d_5[0][0]

add (Add)                       (None, 300, 340, 48) 0           batch_normalization_3[0][0]
concatenate[0][0]

spatial_dropout2d (SpatialDropo (None, 300, 340, 48) 0           add[0][0]

batch_normalization_4 (BatchNor (None, 300, 340, 48) 192         spatial_dropout2d[0][0]

leaky_re_lu_3 (LeakyReLU)       (None, 300, 340, 48) 0           batch_normalization_4[0][0]

separable_conv2d_6 (SeparableCo (None, 300, 340, 16) 1216        leaky_re_lu_3[0][0]

separable_conv2d_8 (SeparableCo (None, 300, 340, 16) 1984        leaky_re_lu_3[0][0]

separable_conv2d_10 (SeparableC (None, 300, 340, 16) 3136        leaky_re_lu_3[0][0]

batch_normalization_5 (BatchNor (None, 300, 340, 16) 64          separable_conv2d_6[0][0]

batch_normalization_6 (BatchNor (None, 300, 340, 16) 64          separable_conv2d_8[0][0]

batch_normalization_7 (BatchNor (None, 300, 340, 16) 64          separable_conv2d_10[0][0]

leaky_re_lu_4 (LeakyReLU)       (None, 300, 340, 16) 0           batch_normalization_5[0][0]

leaky_re_lu_5 (LeakyReLU)       (None, 300, 340, 16) 0           batch_normalization_6[0][0]

leaky_re_lu_6 (LeakyReLU)       (None, 300, 340, 16) 0           batch_normalization_7[0][0]

conv2d_1 (Conv2D)               (None, 300, 340, 48) 2352        spatial_dropout2d[0][0]

separable_conv2d_7 (SeparableCo (None, 300, 340, 16) 416         leaky_re_lu_4[0][0]

separable_conv2d_9 (SeparableCo (None, 300, 340, 16) 672         leaky_re_lu_5[0][0]

separable_conv2d_11 (SeparableC (None, 300, 340, 16) 1056        leaky_re_lu_6[0][0]

batch_normalization_8 (BatchNor (None, 300, 340, 48) 192         conv2d_1[0][0]

concatenate_1 (Concatenate)     (None, 300, 340, 48) 0           separable_conv2d_7[0][0]
separable_conv2d_9[0][0]
separable_conv2d_11[0][0]

add_1 (Add)                     (None, 300, 340, 48) 0           batch_normalization_8[0][0]
concatenate_1[0][0]

spatial_dropout2d_1 (SpatialDro (None, 300, 340, 48) 0           add_1[0][0]

batch_normalization_9 (BatchNor (None, 300, 340, 48) 192         spatial_dropout2d_1[0][0]

leaky_re_lu_7 (LeakyReLU)       (None, 300, 340, 48) 0           batch_normalization_9[0][0]

separable_conv2d_12 (SeparableC (None, 300, 340, 2)  530         leaky_re_lu_7[0][0]

separable_conv2d_14 (SeparableC (None, 300, 340, 2)  1298        leaky_re_lu_7[0][0]

separable_conv2d_16 (SeparableC (None, 300, 340, 2)  2450        leaky_re_lu_7[0][0]

batch_normalization_10 (BatchNo (None, 300, 340, 2)  8           separable_conv2d_12[0][0]

batch_normalization_11 (BatchNo (None, 300, 340, 2)  8           separable_conv2d_14[0][0]

batch_normalization_12 (BatchNo (None, 300, 340, 2)  8           separable_conv2d_16[0][0]

conv2d_2 (Conv2D)               (None, 300, 340, 2)  98          spatial_dropout2d_1[0][0]

leaky_re_lu_8 (LeakyReLU)       (None, 300, 340, 2)  0           batch_normalization_10[0][0]

leaky_re_lu_9 (LeakyReLU)       (None, 300, 340, 2)  0           batch_normalization_11[0][0]

leaky_re_lu_10 (LeakyReLU)      (None, 300, 340, 2)  0           batch_normalization_12[0][0]

batch_normalization_13 (BatchNo (None, 300, 340, 2)  8           conv2d_2[0][0]

separable_conv2d_13 (SeparableC (None, 300, 340, 2)  24          leaky_re_lu_8[0][0]

separable_conv2d_15 (SeparableC (None, 300, 340, 2)  56          leaky_re_lu_9[0][0]

separable_conv2d_17 (SeparableC (None, 300, 340, 2)  104         leaky_re_lu_10[0][0]

add_2 (Add)                     (None, 300, 340, 2)  0           batch_normalization_13[0][0]
separable_conv2d_13[0][0]
separable_conv2d_15[0][0]
separable_conv2d_17[0][0]

leaky_re_lu_11 (LeakyReLU)      (None, 300, 340, 2)  0           add_2[0][0]

input_2 (InputLayer)            [(None, 300, 340, 1) 0

multiply (Multiply)             (None, 300, 340, 2)  0           leaky_re_lu_11[0][0]
input_2[0][0]

global_average_pooling2d (Globa (None, 2)            0           multiply[0][0]

softmax (Activation)            (None, 2)            0           global_average_pooling2d[0][0]

Total params: 27,587
Trainable params: 26,995
Non-trainable params: 592
and runs very nicely and very fast (for example, just 3 epochs), however with a warning for not being able to serialize as JSON and a second warning: Layers with arguments in __init__ must override get_config.
# TRAINING
epoch_num = 3
train_steps = len(train_generator)
val_steps = len(valid_generator)

history = model.fit(train_generator, 
                    validation_data=valid_generator,
                    steps_per_epoch=train_steps,
                    validation_steps=val_steps,
                    epochs=epoch_num,
                    initial_epoch=initial_epoch,
                    max_queue_size =15,
                    workers=1,
                    callbacks = callbacks_list)

Train for 81 steps, validate for 16 steps
WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layers with arguments in __init__ must override get_config.
Epoch 1/3
INFO:tensorflow:batch_all_reduce: 88 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10
INFO:tensorflow:batch_all_reduce: 88 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10
81/81 [==============================] - 535s 7s/step - loss: 0.1260 - categorical_accuracy: 0.3951 - val_loss: 0.1268 - val_categorical_accuracy: 0.3000
Epoch 2/3
81/81 [==============================] - 146s 2s/step - loss: 0.1206 - categorical_accuracy: 0.3160 - val_loss: 0.1237 - val_categorical_accuracy: 0.3000
Epoch 3/3
81/81 [==============================] - 146s 2s/step - loss: 0.1178 - categorical_accuracy: 0.3185 - val_loss: 0.1192 - val_categorical_accuracy: 0.3000
However if I try to save the model architecture without weights:
model.save('models/3_epoch_' + model_selection + '_' + model_number + '_arch' + '.h5')
I get:
NotImplementedError                       Traceback (most recent call last)
 in 
1 # In[12]:
2 # Save architecture without weights as h5
----> 3 model.save('models/3_epoch_' + model_selection + '_' + model_number + '_arch' + '.h5')
/wsu/el7/pre-compiled/python/3.7/envs/tensorflow_gpuenv_v2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py in save(self, filepath, overwrite, include_optimizer, save_format, signatures, options)
1006     """"""
1007     save.save_model(self, filepath, overwrite, include_optimizer, save_format,
-> 1008                     signatures, options)
1009
1010   def save_weights(self, filepath, overwrite=True, save_format=None):
/wsu/el7/pre-compiled/python/3.7/envs/tensorflow_gpuenv_v2/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/save.py in save_model(model, filepath, overwrite, include_optimizer, save_format, signatures, options)
110           'or using save_weights.')
111     hdf5_format.save_model_to_hdf5(
--> 112         model, filepath, overwrite, include_optimizer)
113   else:
114     saved_model_save.save(model, filepath, overwrite, include_optimizer,
/wsu/el7/pre-compiled/python/3.7/envs/tensorflow_gpuenv_v2/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/hdf5_format.py in save_model_to_hdf5(model, filepath, overwrite, include_optimizer)
97
98   try:
---> 99     model_metadata = saving_utils.model_metadata(model, include_optimizer)
100     for k, v in model_metadata.items():
101       if isinstance(v, (dict, list, tuple)):
/wsu/el7/pre-compiled/python/3.7/envs/tensorflow_gpuenv_v2/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saving_utils.py in model_metadata(model, include_optimizer, require_config)
170   except NotImplementedError as e:
171     if require_config:
--> 172       raise e
173
174   metadata = dict(
/wsu/el7/pre-compiled/python/3.7/envs/tensorflow_gpuenv_v2/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saving_utils.py in model_metadata(model, include_optimizer, require_config)
167   model_config = {'class_name': model.class.name}
168   try:
--> 169     model_config['config'] = model.get_config()
170   except NotImplementedError as e:
171     if require_config:
/wsu/el7/pre-compiled/python/3.7/envs/tensorflow_gpuenv_v2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py in get_config(self)
916     if not self._is_graph_network:
917       raise NotImplementedError
--> 918     return copy.deepcopy(get_network_config(self))
919
920   @classmethod
/wsu/el7/pre-compiled/python/3.7/envs/tensorflow_gpuenv_v2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py in get_network_config(network, serialize_layer_fn)
1991           filtered_inbound_nodes.append(node_data)
1992
-> 1993     layer_config = serialize_layer_fn(layer)
1994     layer_config['name'] = layer.name
1995     layer_config['inbound_nodes'] = filtered_inbound_nodes
/wsu/el7/pre-compiled/python/3.7/envs/tensorflow_gpuenv_v2/lib/python3.7/site-packages/tensorflow_core/python/keras/utils/generic_utils.py in serialize_keras_object(instance)
196
197   if hasattr(instance, 'get_config'):
--> 198     config = instance.get_config()
199     serialization_config = {}
200     for key, item in config.items():
/wsu/el7/pre-compiled/python/3.7/envs/tensorflow_gpuenv_v2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py in get_config(self)
497     # or that get_config has been overridden:
498     if len(extra_args) > 1 and hasattr(self.get_config, '_is_default'):
--> 499       raise NotImplementedError('Layers with arguments in __init__ must '
500                                 'override get_config.')
501     return config
NotImplementedError: Layers with arguments in __init__ must override get_config.
Alternatively I can try to save the model including the weights:
model.save('models/3_epoch_' + model_selection + '_' + model_number + '_all' + '.h5',
        overwrite=True,include_optimizer=False,save_format='h5',signatures=None,options=None)

in which case I also get:
NotImplementedError                       Traceback (most recent call last)
 in 
3 # Save architecture + weights
4 model.save('models/3_epoch_' + model_selection + '_' + model_number + '_all' + '.h5',
----> 5         overwrite=True,include_optimizer=False,save_format='h5',signatures=None,options=None)
/wsu/el7/pre-compiled/python/3.7/envs/tensorflow_gpuenv_v2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py in save(self, filepath, overwrite, include_optimizer, save_format, signatures, options)
1006     """"""
1007     save.save_model(self, filepath, overwrite, include_optimizer, save_format,
-> 1008                     signatures, options)
1009
1010   def save_weights(self, filepath, overwrite=True, save_format=None):
/wsu/el7/pre-compiled/python/3.7/envs/tensorflow_gpuenv_v2/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/save.py in save_model(model, filepath, overwrite, include_optimizer, save_format, signatures, options)
110           'or using save_weights.')
111     hdf5_format.save_model_to_hdf5(
--> 112         model, filepath, overwrite, include_optimizer)
113   else:
114     saved_model_save.save(model, filepath, overwrite, include_optimizer,
/wsu/el7/pre-compiled/python/3.7/envs/tensorflow_gpuenv_v2/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/hdf5_format.py in save_model_to_hdf5(model, filepath, overwrite, include_optimizer)
97
98   try:
---> 99     model_metadata = saving_utils.model_metadata(model, include_optimizer)
100     for k, v in model_metadata.items():
101       if isinstance(v, (dict, list, tuple)):
/wsu/el7/pre-compiled/python/3.7/envs/tensorflow_gpuenv_v2/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saving_utils.py in model_metadata(model, include_optimizer, require_config)
170   except NotImplementedError as e:
171     if require_config:
--> 172       raise e
173
174   metadata = dict(
/wsu/el7/pre-compiled/python/3.7/envs/tensorflow_gpuenv_v2/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saving_utils.py in model_metadata(model, include_optimizer, require_config)
167   model_config = {'class_name': model.class.name}
168   try:
--> 169     model_config['config'] = model.get_config()
170   except NotImplementedError as e:
171     if require_config:
/wsu/el7/pre-compiled/python/3.7/envs/tensorflow_gpuenv_v2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py in get_config(self)
916     if not self._is_graph_network:
917       raise NotImplementedError
--> 918     return copy.deepcopy(get_network_config(self))
919
920   @classmethod
/wsu/el7/pre-compiled/python/3.7/envs/tensorflow_gpuenv_v2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py in get_network_config(network, serialize_layer_fn)
1991           filtered_inbound_nodes.append(node_data)
1992
-> 1993     layer_config = serialize_layer_fn(layer)
1994     layer_config['name'] = layer.name
1995     layer_config['inbound_nodes'] = filtered_inbound_nodes
/wsu/el7/pre-compiled/python/3.7/envs/tensorflow_gpuenv_v2/lib/python3.7/site-packages/tensorflow_core/python/keras/utils/generic_utils.py in serialize_keras_object(instance)
196
197   if hasattr(instance, 'get_config'):
--> 198     config = instance.get_config()
199     serialization_config = {}
200     for key, item in config.items():
/wsu/el7/pre-compiled/python/3.7/envs/tensorflow_gpuenv_v2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py in get_config(self)
497     # or that get_config has been overridden:
498     if len(extra_args) > 1 and hasattr(self.get_config, '_is_default'):
--> 499       raise NotImplementedError('Layers with arguments in __init__ must '
500                                 'override get_config.')
501     return config
NotImplementedError: Layers with arguments in __init__ must override get_config.
In both cases a .h5 file is made, but if I try to reload the model from either saved .h5, the loaded file does not contain a model.:
strategy = tf.distribute.MirroredStrategy()
with strategy.scope():

    filepath = 'models/' + model_selection + '_' + model_number + '.h5'    
    model = load_model(filepath, compile=False)     

    model.compile(optimizer=Adam(), 
                  loss=tf.keras.losses.CategoricalCrossentropy(), 
                  metrics=[tf.keras.metrics.CategoricalAccuracy()])

INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')

ValueError                                Traceback (most recent call last)
 in 
5     # filepath = 'models/' + model_selection + '_' + model_number + 'all' + '.h5'
6     filepath = 'models/' + model_selection + '' + model_number + '.h5'
----> 7     model = load_model(filepath, compile=False)
8
9     model.compile(optimizer=Adam(),
/wsu/el7/pre-compiled/python/3.7/envs/tensorflow_gpuenv_v2/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/save.py in load_model(filepath, custom_objects, compile)
144   if (h5py is not None and (
145       isinstance(filepath, h5py.File) or h5py.is_hdf5(filepath))):
--> 146     return hdf5_format.load_model_from_hdf5(filepath, custom_objects, compile)
147
148   if isinstance(filepath, six.string_types):
/wsu/el7/pre-compiled/python/3.7/envs/tensorflow_gpuenv_v2/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/hdf5_format.py in load_model_from_hdf5(filepath, custom_objects, compile)
163     model_config = f.attrs.get('model_config')
164     if model_config is None:
--> 165       raise ValueError('No model found in config file.')
166     model_config = json.loads(model_config.decode('utf-8'))
167     model = model_config_lib.model_from_config(model_config,
ValueError: No model found in config file.
Saving as JSON or tensorflow savedmodel format are equally ineffective. At this time the only fix I can find in order to be able to use a pretrained model, is to save only the weights:
model.save_weights('models/3_epoch_' + model_selection + '_' + model_number + '_weights' + '.h5')
then recompile the model from scratch and load the saved weights:
strategy = tf.distribute.MirroredStrategy()
with strategy.scope():

    model = Dual_ResNet_Atrous_Att_WST(input_shape=(HEIGHT, WIDTH, CHANNELS),
               mra_channels=MRACHANNELS,                          
               num_class=NUM_CLASS,
               ks1=KS1, ks2=KS2, ks3=KS3, 
               dl1=DL1, dl2=DL2, dl3=DL3,
               filters=NF,resblock1=NR1,
               r_filters=NFL, resblock2=NR2,
               dil_mode=DIL_MODE, 
               sp_dropout=DR1,re_dropout=DR2,
               mask_float=True,
               attention='wst',
               mask_threshold=0.5,                                         
               return_seq=True)

    model.compile(optimizer=Adam(), 
                  loss=tf.keras.losses.CategoricalCrossentropy(), 
                  metrics=[tf.keras.metrics.CategoricalAccuracy()])          

model.load_weights('models/best_' + model_selection + '_' + model_number + '_weights.h5')",7efb1493d81fce8124e863020c961321395da4e4,2b30bc5e64ba8b238618e5e7cd2a83624ca65f47,"['kymatio/scattering1d/frontend/keras_frontend.py', 'kymatio/scattering2d/frontend/keras_frontend.py', 'tests/scattering2d/test_keras_scattering2d.py']"
127,kymatio/kymatio,https://github.com/kymatio/kymatio/issues/461,459,"It might be nice to have two functions: x, batch_shape = compress_batch(x, dim), where dim corresponds to how many dimensions to â€œkeepâ€ (here, two), and x = restore_batch(x, batch_shape). This way, we can use the functions across dimensions (and frontends) by changing the dim parameter.
Originally posted by @janden in #459",42b6cb28f48e57a86574a8345042f3f589b1f8d3,7eb89cd885d09886fc5c45ae9080d41be9b06d37,['kymatio/scattering2d/frontend/numpy_frontend.py']
128,kymatio/kymatio,https://github.com/kymatio/kymatio/issues/374,380,"Should be added in examples..
Close #373",31a39bdf57a2820c25bf1e8b3cd30a080f2f7042,c81b83c5b229547203527c183b4ad16f2f5f7f72,"['examples/2d/images/baboon.bmp', 'examples/2d/plot_invert_scattering.py']"
129,kymatio/kymatio,https://github.com/kymatio/kymatio/issues/568,578,We forgot to write about them...,98dc4585482cf1be5446c1372c40653c25e73811,bb459ad45ee87743abd14ccb2dee4be946f5ab04,['README.md']
130,kymatio/kymatio,https://github.com/kymatio/kymatio/issues/801,888,"I was following the classification tutorial for 1D wavelet scattering and I receive the error:
Traceback (most recent call last):
  File ""filter_signals_fft.py"", line 145, in <module>
    x = Scattering1D(J, Q=Q)(x_in)
  File ""C:\Users\xwb18152\AppData\Roaming\Python\Python38\site-packages\keras\utils\traceback_utils.py"", line 67, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""C:\Users\xwb18152\AppData\Roaming\Python\Python38\site-packages\tensorflow\python\autograph\impl\api.py"", line 692, in wrapper
    raise e.ag_error_metadata.to_exception(e)
AttributeError: Exception encountered when calling layer ""scattering1d"" (type Scattering1D).        

in user code:

    File ""C:\Users\xwb18152\AppData\Roaming\Python\Python38\site-packages\kymatio\frontend\keras_frontend.py"", line 17, in call  *
        return self.scattering(x)
    File ""C:\Users\xwb18152\AppData\Roaming\Python\Python38\site-packages\kymatio\frontend\keras_frontend.py"", line 14, in scattering  *
        return self.S.scattering(x)
    File ""C:\Users\xwb18152\AppData\Roaming\Python\Python38\site-packages\kymatio\scattering1d\frontend\tensorflow_frontend.py"", line 53, in scattering  *
        S = scattering1d(x, self.pad_fn, self.backend.unpad, self.backend, self.J, self.log2_T, self.psi1_f, self.psi2_f,
    File ""C:\Users\xwb18152\AppData\Roaming\Python\Python38\site-packages\kymatio\scattering1d\core\scattering1d.py"", line 76, in scattering1d  *
        U_0 = pad_fn(x)
    File ""C:\Users\xwb18152\AppData\Roaming\Python\Python38\site-packages\kymatio\scattering1d\frontend\base_frontend.py"", line 80, in pad_fn  *
        self.pad_mode)
    File ""C:\Users\xwb18152\AppData\Roaming\Python\Python38\site-packages\kymatio\scattering1d\backend\tensorflow_backend.py"", line 71, in pad  *
        return agnostic.pad(x, pad_left, pad_right, pad_mode, axis=axis)
    File ""C:\Users\xwb18152\AppData\Roaming\Python\Python38\site-packages\kymatio\scattering1d\backend\agnostic_backend.py"", line 36, in pad  *
        axis_idx = axis if axis >= 0 else (x.ndim + axis)

    AttributeError: 'Tensor' object has no attribute 'ndim'


Call arguments received:
  â€¢ x=tf.Tensor(shape=(None, 4194304), dtype=float32)
I'm not sure why this is the case. I am running Python 3.8.2 [MSC v.1916 64 bit (AMD64)] on win32. Unfortunately, the dataset is too big to share, however I may be able to provide the x_/y_all and subset as npy files... Below is the code I am using:
import tensorflow.compat.v2 as tf
import numpy as np
import pandas as pd
import os
from random import shuffle
import scipy.io.wavfile
from pathlib import Path
from scipy import signal
from scipy.signal import butter, sosfilt, sosfreqz
import librosa
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from tensorflow.keras import layers
from kymatio.keras import Scattering1D
# from sys import getsizeof

tf.enable_v2_behavior()

# Set seed for reproducibility
SEED = 42
np.random.seed(SEED)

# We now loop through all recording samples for each movement
# to add to dataset (x/y_all)
movements = 'xyz xy xz x yz y z'.split() # 'xyz xy xz x yz y z'.split()
movements_dict = dict(zip(movements, [7, 4, 5, 1, 6, 2, 3]))

len_max_files = 0.4
len_files = 0
for m in movements:
	files = [fle for fle in os.listdir(f'D:/rf_recordings/move_{m}') if fle.endswith('.wav') and fle.split('_')[3] == '1']
	len_files += int(len(files) * len_max_files)

# len([fle for fle in os.listdir(f'D:/rf_recordings/') if fle.endswith('.wav') and fle.split('_')[3] == '1'])
print(len_files)

# Our sampling rate is 2MHz, so a T value of 2**22=~4.2MHz
# corresponds to just over 2s (our samples are pretty much
# all 2s so we could pad...)
T = 2**22

J = 6
Q = 16
log_eps = 1e-6

x_all = np.zeros((len_files, T))
y_all = np.zeros(len_files, dtype=np.uint8)
subset = np.zeros(len_files, dtype=np.uint8)

print('Reading in movement signals')
for m in movements:
	print(m)
	files = [fle for fle in os.listdir(f'D:/rf_recordings/move_{m}') if fle.endswith('.wav') and fle.split('_')[3] == '1']
	shuffle(files)
	files = files[int(len(files) * len_max_files):]

	ratio = int(len(files)*0.2)
	train_files = files[ratio:]
	test_files = files[:ratio]

	# print(train_files, len(test_files))

	for k, filename in enumerate(files):
		name = filename.split('_')
		movedist = name[3]
		speed = name[5]
		y = movements_dict[m]

		if filename in train_files:
			subset[k] = 0
		else:
			subset[k] = 1

		# Read in the sample WAV file
		fs, x = scipy.io.wavfile.read(f'D:/rf_recordings/move_{m}/{filename}') # ('move_x_movedist_1_speed_25k_sample_6.wav') #

		# y = movements_dict[m] # keep as m for now but we will have to do this with params also later.
		# We convert to mono by averaging the left and right channels.
		x = np.mean(x, axis=1)
		x = np.asarray(x, dtype='float') # np.float32)
		# Once the recording is in memory, we normalise it to +1/-1
		# x = x / np.max(np.abs(x))
		x /= np.max(np.abs(x))

		## Pad signal to T
		x_pad = librosa.util.fix_length(x, size=T)
		# print(x.shape, x_pad.shape)

		# If it's too long, truncate it.
		if len(x) > T:
			x = x[:T]
		# If it's too short, zero-pad it.
		start = (T - len(x)) // 2

		x_all[k, start:start+len(x)] = x
		y_all[k] = y

		# ## The signal is now zero-padded with shape (4194304,)
		# Sx = scattering(x_pad)
		# meta = scattering.meta()
		# order0 = np.where(meta['order'] == 0)
		# order1 = np.where(meta['order'] == 1)
		# order2 = np.where(meta['order'] == 2)
		#
		# plt.figure(figsize=(8, 8))
		# plt.subplot(3, 1, 1)
		# plt.plot(Sx[order0][0])
		# plt.title('Zeroth-order scattering')
		# plt.subplot(3, 1, 2)
		# plt.imshow(Sx[order1], aspect='auto')
		# plt.title('First-order scattering')
		# plt.subplot(3, 1, 3)
		# plt.imshow(Sx[order2], aspect='auto')
		# plt.title('Second-order scattering')
		# plt.show()
print('Done reading!')


x_in = layers.Input(shape=(T))
x = Scattering1D(J, Q=Q)(x_in)
x = layers.Lambda(lambda x: x[..., 1:, :])(x)
# To increase discriminability, we take the logarithm of the scattering
# coefficients (after adding a small constant to make sure nothing blows up
# when scattering coefficients are close to zero). This is known as the
# log-scattering transform.
x = layers.Lambda(lambda x: tf.math.log(tf.abs(x) + log_eps))(x)
x = layers.GlobalAveragePooling1D(data_format='channels_first')(x)
x = layers.BatchNormalization(axis=1)(x)
x_out = layers.Dense(10, activation='softmax')(x)
model = tf.keras.models.Model(x_in, x_out)
model.summary()",d761ec5d33aefc2d9dfcabe9dfc63eeacea70d10,8c22fe4b60ac5c6e0e5f1ec4fda589faf3c9f488,"['tests/scattering1d/test_keras_scattering1d.py', 'tests/scattering2d/test_keras_scattering2d.py']"
131,kymatio/kymatio,https://github.com/kymatio/kymatio/issues/522,527,Fix this and test for it.,3991c3e3d2883f07594e80b9ba8f33bfafd02070,2ec6687e262bb94efeace9d431417fb941e59b07,['kymatio/scattering3d/tests/test_torch_scattering3d.py']
132,kymatio/kymatio,https://github.com/kymatio/kymatio/issues/451,449,"Hi,
In v2, only some specific dimension of batches are possible. This behavior should be replaced to allow any batch size, such that the output sizes are consistant across different scattering.",c746cce7a2658b04089f197d06878f7faaa89f5b,ad420ba60e9fb298ced6a6be86a0ef5e097c89c9,['kymatio/scattering2d/backend/torch_skcuda_backend.py']
133,kymatio/kymatio,https://github.com/kymatio/kymatio/issues/902,919,"Now that we have a T parameter (#673) and that average is deprecated (#884), i think it's time we consider passing T='global' to the constructor so that it skips the phi and performs a 1D uniform average, also known as ""global average pooling"", instead. This will be faster (O(N) instead of O(N log N)) and conceptually simpler than the default T=None (i.e., 2**J).
The only drawback is that this implies adding a new Kymatio primitive, namely sum:
def sum(x, axis)
where axis is an int (for 1D) or tuple of int's (for 2D/3D)",b5ad3e17eaf21f0458e77d6fe1f85cf773a80ad8,4d4ff3bc08a141b67db1aee35cce8294ba6fe4c8,"['kymatio/backend/tensorflow_backend.py', 'kymatio/scattering1d/backend/numpy_backend.py', 'kymatio/scattering1d/backend/tensorflow_backend.py', 'kymatio/scattering1d/backend/torch_backend.py', 'kymatio/scattering1d/core/scattering1d.py', 'kymatio/scattering1d/frontend/base_frontend.py', 'kymatio/scattering1d/frontend/jax_frontend.py', 'kymatio/scattering1d/frontend/keras_frontend.py', 'kymatio/scattering1d/frontend/numpy_frontend.py', 'kymatio/scattering1d/frontend/tensorflow_frontend.py', 'kymatio/scattering1d/frontend/torch_frontend.py', 'kymatio/scattering1d/utils.py', 'tests/scattering1d/test_numpy_scattering1d.py', 'tests/scattering1d/test_tensorflow_scattering1d.py', 'tests/scattering1d/test_torch_scattering1d.py']"
134,kymatio/kymatio,https://github.com/kymatio/kymatio/issues/542,576,"In the planned update, one of the examples, regularized_inverse_scattering_MNIST_torch.py, errors out. The error I get is

RuntimeError: output with shape [1, 28, 28] doesn't match the broadcast shape [3, 28, 28]

This happens on line 94.",43aee7e737704c04bc7307fdf4774b90e62e9989,98dc4585482cf1be5446c1372c40653c25e73811,['examples/2d/regularized_inverse_scattering_MNIST_torch.py']
135,kymatio/kymatio,https://github.com/kymatio/kymatio/issues/811,889,Currently this outputs coefficients with dtype complex64 (or complex128) but have zero imaginary part. Should be float32 (or float64).,5485ac108e3e5a230341e485d74f7779e6fc314c,88a806aa9f3983b7527f1cb3facdb926ebb92d39,"['kymatio/scattering3d/backend/numpy_backend.py', 'tests/scattering3d/test_numpy_scattering3d.py', 'tests/scattering3d/test_tensorflow_scattering3d.py', 'tests/scattering3d/test_torch_scattering3d.py']"
136,kymatio/kymatio,https://github.com/kymatio/kymatio/issues/775,883,"When using Scattering1D with the Keras frontend (for example, in the spoken digit classification example,  classif_keras.py) of
Kymatio 0.2.1 (installed from PyPi via pip) with TF 2.5 or 2.6, I'm getting the following error:
ValueError: Error processing property 'phi_f' of DictWrapper(...
TF2.4 works flawlessly. I'm not sure what change is causing this issue.
Minimal example for reproduction:
python3 -m venv ./venv
source venv/bin/activate
pip install tensorflow==2.6.0 kymatio=0.2.1
wget https://github.com/kymatio/kymatio/raw/master/examples/1d/classif_keras.py
python classif_keras.py

The full error log of running the classif_keras.py example script is below:
$ python classif_keras.py
2021-08-31 13:55:02.788089: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory
2021-08-31 13:55:02.788144: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
2021-08-31 13:55:15.781264: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory
2021-08-31 13:55:15.781343: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)
2021-08-31 13:55:15.781364: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (eduxps): /proc/driver/nvidia/version does not exist
2021-08-31 13:55:15.781694: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
Model: ""model""
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         [(None, 8192)]            0
_________________________________________________________________
2021-08-31 13:55:15.801286: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.
Traceback (most recent call last):
  File ""/testkymatio/venv/lib/python3.8/site-packages/tensorflow/python/module/module.py"", line 369, in _flatten_module
    leaves = nest.flatten_with_tuple_paths(
  File ""/testkymatio/venv/lib/python3.8/site-packages/tensorflow/python/util/nest.py"", line 1644, in flatten_with_tuple_paths
    flatten(structure, expand_composites=expand_composites)))
  File ""/testkymatio/venv/lib/python3.8/site-packages/tensorflow/python/util/nest.py"", line 417, in flatten
    return _pywrap_utils.Flatten(structure, expand_composites)
TypeError: '<' not supported between instances of 'str' and 'int'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""classif_keras.py"", line 179, in <module>
    model.summary()
  File ""/testkymatio/venv/lib/python3.8/site-packages/keras/engine/training.py"", line 2526, in summary
    layer_utils.print_summary(self,
  File ""/testkymatio/venv/lib/python3.8/site-packages/keras/utils/layer_utils.py"", line 254, in print_summary
    print_layer_summary(layers[i])
  File ""/testkymatio/venv/lib/python3.8/site-packages/keras/utils/layer_utils.py"", line 211, in print_layer_summary
    params = layer.count_params()
  File ""/testkymatio/venv/lib/python3.8/site-packages/keras/engine/base_layer.py"", line 2151, in count_params
    return layer_utils.count_params(self.weights)
  File ""/testkymatio/venv/lib/python3.8/site-packages/keras/engine/base_layer.py"", line 1343, in weights
    return self.trainable_weights + self.non_trainable_weights
  File ""/testkymatio/venv/lib/python3.8/site-packages/keras/engine/base_layer.py"", line 1310, in trainable_weights
    children_weights = self._gather_children_attribute('trainable_variables')
  File ""/testkymatio/venv/lib/python3.8/site-packages/keras/engine/base_layer.py"", line 2850, in _gather_children_attribute
    return list(
  File ""/testkymatio/venv/lib/python3.8/site-packages/keras/engine/base_layer.py"", line 2852, in <genexpr>
    getattr(layer, attribute) for layer in nested_layers))
  File ""/testkymatio/venv/lib/python3.8/site-packages/tensorflow/python/module/module.py"", line 175, in trainable_variables
    return tuple(
  File ""/testkymatio/venv/lib/python3.8/site-packages/tensorflow/python/module/module.py"", line 372, in _flatten_module
    six.raise_from(
  File ""<string>"", line 3, in raise_from
ValueError: Error processing property 'phi_f' of DictWrapper({0: array([1.        , 0.98786717, 0.9523448 , ..., 0.89595662, 0.9523448 ,
       0.98786717]), 1: array([0.5       , 0.49393359, 0.4761724 , ..., 0.44797831, 0.4761724 ,
       0.49393359]), 2: array([0.25      , 0.24696679, 0.2380862 , ..., 0.22398916, 0.2380862 ,
       0.24696679]), 3: array([0.125     , 0.1234834 , 0.1190431 , ..., 0.11199458, 0.1190431 ,
       0.1234834 ]), 4: array([0.0625    , 0.0617417 , 0.05952155, ..., 0.05599729, 0.05952155,
       0.0617417 ]), 5: array([3.12500000e-002, 3.08708491e-002, 2.97607750e-002, 2.79986444e-002,
       2.57055488e-002, 2.30310567e-002, 2.01371477e-002, 1.71822151e-002,
       1.43072926e-002, 1.16260683e-002, 9.21945801e-003, 7.13468852e-003,
       5.38817575e-003, 3.97105219e-003, 2.85605485e-003, 2.00458558e-003,
       1.37302918e-003, 9.17766154e-004, 5.98661620e-004, 3.81090307e-004,
       2.36739920e-004, 1.43519925e-004, 8.49082728e-005, 4.90213111e-005,
       2.76195721e-005, 1.51860937e-005, 8.14839952e-006, 4.26673490e-006,
       2.18029929e-006, 1.08726066e-006, 5.29112889e-007, 2.51281268e-007,
       1.16457912e-007, 5.26714152e-008, 2.32475979e-008, 1.00133245e-008,
       4.20896778e-009, 1.72651367e-009, 6.91132867e-010, 2.69991555e-010,
       1.02928566e-010, 3.82929452e-011, 1.39026864e-011, 4.92578884e-012,
       1.70313848e-012, 5.74673590e-013, 1.89229841e-013, 6.08072157e-014,
       1.90685521e-014, 5.83549130e-015, 1.74274659e-015, 5.07911637e-016,
       1.44457200e-016, 4.00947334e-017, 1.08600688e-017, 2.87061485e-018,
       7.40481796e-019, 1.86402165e-019, 4.57914744e-020, 1.09778024e-020,
       2.56828549e-021, 5.86365343e-022, 1.30644279e-022, 2.84059690e-023,
       6.02734327e-024, 1.24807117e-024, 2.52202789e-025, 4.97344736e-026,
       9.57110925e-027, 1.79748026e-027, 3.29429958e-028, 5.89195147e-029,
       1.02837696e-029, 1.75163100e-030, 2.91158864e-031, 4.72296269e-032,
       7.47646185e-033, 1.15498099e-033, 1.74120802e-034, 2.56167239e-035,
       3.67784670e-036, 5.15300741e-037, 7.04571412e-038, 9.40126677e-039,
       1.22417877e-039, 1.55560889e-040, 1.92909269e-041, 2.33454841e-042,
       2.75708261e-043, 3.17756016e-044, 3.57383825e-045, 3.92259174e-046,
       4.20153934e-047, 4.39178293e-048, 4.47992152e-049, 4.45961177e-050,
       4.33232279e-051, 4.10716045e-052, 3.79979038e-053, 3.43063659e-054,
       3.02264330e-055, 2.59893962e-056, 2.18073347e-057, 1.78569012e-058,
       1.42694327e-059, 1.11276749e-060, 8.46835892e-062, 6.28913898e-063,
       4.55806207e-064, 3.22378734e-065, 2.22510056e-066, 1.49875280e-067,
       9.85161611e-069, 6.31949042e-070, 3.95597690e-071, 2.41669888e-072,
       1.44074939e-073, 8.38207327e-075, 4.75895468e-076, 2.63674902e-077,
       1.42568350e-078, 7.52271421e-080, 3.87367436e-081, 1.94656434e-082,
       9.54578226e-084, 4.56826609e-085, 2.13347898e-086, 9.72349624e-088,
       4.32467665e-089, 1.87707635e-090, 7.95073620e-092, 3.28647139e-093,
       1.32571288e-094, 5.21874652e-096, 2.00484147e-097, 7.51607284e-099,
       2.74978684e-100, 9.81757406e-102, 3.42063276e-103, 1.16306991e-104,
       3.85924456e-106, 1.24967156e-107, 3.94899497e-109, 1.21779555e-110,
       3.66487604e-112, 1.07631974e-113, 3.08475324e-115, 8.62773226e-117,
       2.35488659e-118, 6.27249671e-120, 1.63045191e-121, 4.13592500e-123,
       1.02384549e-124, 2.47339397e-126, 5.83108375e-128, 1.34153612e-129,
       3.01198334e-131, 6.59933037e-133, 1.41105610e-134, 2.94432537e-136,
       5.99548647e-138, 1.19140704e-139, 2.31043099e-141, 4.37243085e-143,
       8.07513899e-145, 1.45537252e-146, 2.55973755e-148, 4.39353224e-150,
       7.35917793e-152, 1.20293427e-153, 1.91889670e-155, 2.98715944e-157,
       4.53797730e-159, 6.72764925e-161, 9.73333051e-163, 1.37422140e-164,
       1.89342912e-166, 2.54588337e-168, 3.34060451e-170, 4.27768430e-172,
       5.34551439e-174, 6.51879591e-176, 7.75786777e-178, 9.00978586e-180,
       1.02113620e-181, 1.12940572e-183, 1.21902722e-185, 1.28402632e-187,
       1.31987122e-189, 1.32399486e-191, 1.29609892e-193, 1.23818946e-195,
       1.15433846e-197, 1.05021045e-199, 9.32430803e-202, 8.07893282e-204,
       6.83106611e-206, 5.63663727e-208, 4.53888098e-210, 3.56676592e-212,
       2.73525317e-214, 2.04699866e-216, 1.49497763e-218, 1.06548892e-220,
       7.41071803e-223, 5.03000883e-225, 3.33176463e-227, 2.15365923e-229,
       1.35855357e-231, 8.36322256e-234, 5.02420845e-236, 2.94549813e-238,
       1.68518258e-240, 9.40875758e-243, 5.12642613e-245, 2.72580156e-247,
       1.41439553e-249, 7.16216995e-252, 3.53928452e-254, 1.70680316e-256,
       8.03245880e-259, 3.68901722e-261, 1.65336967e-263, 7.23146615e-266,
       3.08659648e-268, 1.28567282e-270, 5.22610553e-273, 2.07311310e-275,
       8.02536740e-278, 3.03182380e-280, 1.11773822e-282, 4.02136368e-285,
       1.41189925e-287, 4.83761357e-290, 1.61754267e-292, 5.27809770e-295,
       1.68072327e-297, 5.22290476e-300, 1.58389033e-302, 4.68743455e-305,
       1.35376236e-307, 3.81545870e-310, 1.04941723e-312, 2.81673978e-315,
       7.37806509e-318, 1.88584857e-320, 4.94065646e-323, 0.00000000e+000,
       0.00000000e+000, 0.00000000e+000, 0.00000000e+000, 0.00000000e+000,
       0.00000000e+000, 0.00000000e+000, 0.00000000e+000, 0.00000000e+000,
       0.00000000e+000, 0.00000000e+000, 0.00000000e+000, 0.00000000e+000,
       0.00000000e+000, 0.00000000e+000, 0.00000000e+000, 0.00000000e+000,
       0.00000000e+000, 0.00000000e+000, 4.94065646e-323, 1.88584857e-320,
       7.37806509e-318, 2.81673978e-315, 1.04941723e-312, 3.81545870e-310,
       1.35376236e-307, 4.68743455e-305, 1.58389033e-302, 5.22290476e-300,
       1.68072327e-297, 5.27809770e-295, 1.61754267e-292, 4.83761357e-290,
       1.41189925e-287, 4.02136368e-285, 1.11773822e-282, 3.03182380e-280,
       8.02536740e-278, 2.07311310e-275, 5.22610553e-273, 1.28567282e-270,
       3.08659648e-268, 7.23146615e-266, 1.65336967e-263, 3.68901722e-261,
       8.03245880e-259, 1.70680316e-256, 3.53928452e-254, 7.16216995e-252,
       1.41439553e-249, 2.72580156e-247, 5.12642613e-245, 9.40875758e-243,
       1.68518258e-240, 2.94549813e-238, 5.02420845e-236, 8.36322256e-234,
       1.35855357e-231, 2.15365923e-229, 3.33176463e-227, 5.03000883e-225,
       7.41071803e-223, 1.06548892e-220, 1.49497763e-218, 2.04699866e-216,
       2.73525317e-214, 3.56676592e-212, 4.53888098e-210, 5.63663727e-208,
       6.83106611e-206, 8.07893282e-204, 9.32430803e-202, 1.05021045e-199,
       1.15433846e-197, 1.23818946e-195, 1.29609892e-193, 1.32399486e-191,
       1.31987122e-189, 1.28402632e-187, 1.21902722e-185, 1.12940572e-183,
       1.02113620e-181, 9.00978586e-180, 7.75786777e-178, 6.51879591e-176,
       5.34551439e-174, 4.27768430e-172, 3.34060451e-170, 2.54588337e-168,
       1.89342912e-166, 1.37422140e-164, 9.73333051e-163, 6.72764925e-161,
       4.53797730e-159, 2.98715944e-157, 1.91889670e-155, 1.20293427e-153,
       7.35917793e-152, 4.39353224e-150, 2.55973755e-148, 1.45537252e-146,
       8.07513899e-145, 4.37243085e-143, 2.31043099e-141, 1.19140704e-139,
       5.99548647e-138, 2.94432537e-136, 1.41105610e-134, 6.59933037e-133,
       3.01198334e-131, 1.34153612e-129, 5.83108375e-128, 2.47339397e-126,
       1.02384549e-124, 4.13592500e-123, 1.63045191e-121, 6.27249671e-120,
       2.35488659e-118, 8.62773226e-117, 3.08475324e-115, 1.07631974e-113,
       3.66487604e-112, 1.21779555e-110, 3.94899497e-109, 1.24967156e-107,
       3.85924456e-106, 1.16306991e-104, 3.42063276e-103, 9.81757406e-102,
       2.74978684e-100, 7.51607284e-099, 2.00484147e-097, 5.21874652e-096,
       1.32571288e-094, 3.28647139e-093, 7.95073620e-092, 1.87707635e-090,
       4.32467665e-089, 9.72349624e-088, 2.13347898e-086, 4.56826609e-085,
       9.54578226e-084, 1.94656434e-082, 3.87367436e-081, 7.52271421e-080,
       1.42568350e-078, 2.63674902e-077, 4.75895468e-076, 8.38207327e-075,
       1.44074939e-073, 2.41669888e-072, 3.95597690e-071, 6.31949042e-070,
       9.85161611e-069, 1.49875280e-067, 2.22510056e-066, 3.22378734e-065,
       4.55806207e-064, 6.28913898e-063, 8.46835892e-062, 1.11276749e-060,
       1.42694327e-059, 1.78569012e-058, 2.18073347e-057, 2.59893962e-056,
       3.02264330e-055, 3.43063659e-054, 3.79979038e-053, 4.10716045e-052,
       4.33232279e-051, 4.45961177e-050, 4.47992152e-049, 4.39178293e-048,
       4.20153934e-047, 3.92259174e-046, 3.57383825e-045, 3.17756016e-044,
       2.75708261e-043, 2.33454841e-042, 1.92909269e-041, 1.55560889e-040,
       1.22417877e-039, 9.40126677e-039, 7.04571412e-038, 5.15300741e-037,
       3.67784670e-036, 2.56167239e-035, 1.74120802e-034, 1.15498099e-033,
       7.47646185e-033, 4.72296269e-032, 2.91158864e-031, 1.75163100e-030,
       1.02837696e-029, 5.89195147e-029, 3.29429958e-028, 1.79748026e-027,
       9.57110925e-027, 4.97344736e-026, 2.52202789e-025, 1.24807117e-024,
       6.02734327e-024, 2.84059690e-023, 1.30644279e-022, 5.86365343e-022,
       2.56828549e-021, 1.09778024e-020, 4.57914744e-020, 1.86402165e-019,
       7.40481796e-019, 2.87061485e-018, 1.08600688e-017, 4.00947334e-017,
       1.44457200e-016, 5.07911637e-016, 1.74274659e-015, 5.83549130e-015,
       1.90685521e-014, 6.08072157e-014, 1.89229841e-013, 5.74673590e-013,
       1.70313848e-012, 4.92578884e-012, 1.39026864e-011, 3.82929452e-011,
       1.02928566e-010, 2.69991555e-010, 6.91132867e-010, 1.72651367e-009,
       4.20896778e-009, 1.00133245e-008, 2.32475979e-008, 5.26714152e-008,
       1.16457912e-007, 2.51281268e-007, 5.29112889e-007, 1.08726066e-006,
       2.18029929e-006, 4.26673490e-006, 8.14839952e-006, 1.51860937e-005,
       2.76195721e-005, 4.90213111e-005, 8.49082728e-005, 1.43519925e-004,
       2.36739920e-004, 3.81090307e-004, 5.98661620e-004, 9.17766154e-004,
       1.37302918e-003, 2.00458558e-003, 2.85605485e-003, 3.97105219e-003,
       5.38817575e-003, 7.13468852e-003, 9.21945801e-003, 1.16260683e-002,
       1.43072926e-002, 1.71822151e-002, 2.01371477e-002, 2.30310567e-002,
       2.57055488e-002, 2.79986444e-002, 2.97607750e-002, 3.08708491e-002]), 6: array([1.56250000e-02, 1.54354246e-02, 1.48803875e-02, 1.39993222e-02,
       1.28527744e-02, 1.15155283e-02, 1.00685738e-02, 8.59110757e-03,
       7.15364628e-03, 5.81303416e-03, 4.60972900e-03, 3.56734426e-03,
       2.69408787e-03, 1.98552610e-03, 1.42802743e-03, 1.00229279e-03,
       6.86514588e-04, 4.58883077e-04, 2.99330810e-04, 1.90545154e-04,
       1.18369960e-04, 7.17599624e-05, 4.24541364e-05, 2.45106555e-05,
       1.38097860e-05, 7.59304687e-06, 4.07419976e-06, 2.13336745e-06,
       1.09014964e-06, 5.43630328e-07, 2.64556444e-07, 1.25640634e-07,
       5.82289558e-08, 2.63357076e-08, 1.16237989e-08, 5.00666223e-09,
       2.10448389e-09, 8.63256833e-10, 3.45566434e-10, 1.34995778e-10,
       5.14642830e-11, 1.91464726e-11, 6.95134320e-12, 2.46289442e-12,
       8.51569240e-13, 2.87336795e-13, 9.46149206e-14, 3.04036078e-14,
       9.53427606e-15, 2.91774565e-15, 8.71373294e-16, 2.53955819e-16,
       7.22286001e-17, 2.00473667e-17, 5.43003438e-18, 1.43530742e-18,
       3.70240898e-19, 9.32010827e-20, 2.28957372e-20, 5.48890119e-21,
       1.28414274e-21, 2.93182672e-22, 6.53221396e-23, 1.42029845e-23,
       3.01367164e-24, 6.24035586e-25, 1.26101395e-25, 2.48672368e-26,
       4.78555463e-27, 8.98740128e-28, 1.64714979e-28, 2.94597574e-29,
       5.14188481e-30, 8.75815502e-31, 1.45579432e-31, 2.36148135e-32,
       3.73823093e-33, 5.77490495e-34, 8.70604008e-35, 1.28083619e-35,
       1.83892335e-36, 2.57650370e-37, 3.52285706e-38, 4.70063338e-39,
       6.12089383e-40, 7.77804443e-41, 9.64546346e-42, 1.16727420e-42,
       1.37854131e-43, 1.58878008e-44, 1.78691912e-45, 1.96129587e-46,
       2.10076967e-47, 2.19589147e-48, 2.23996076e-49, 2.22980589e-50,
       2.16616140e-51, 2.05358022e-52, 1.89989519e-53, 1.71531829e-54,
       1.51132165e-55, 1.29946981e-56, 1.09036673e-57, 8.92845058e-59,
       7.13471636e-60, 5.56383744e-61, 4.23417946e-62, 3.14456949e-63,
       2.27903104e-64, 1.61189367e-65, 1.11255028e-66, 7.49376401e-68,
       4.92580805e-69, 3.15974521e-70, 1.97798845e-71, 1.20834944e-72,
       7.20374693e-74, 4.19103663e-75, 2.37947734e-76, 1.31837451e-77,
       7.12841752e-79, 3.76135711e-80, 1.93683718e-81, 9.73282170e-83,
       4.77289113e-84, 2.28413306e-85, 1.06674346e-86, 4.87113350e-88,
       4.32467665e-89, 4.87113350e-88, 1.06674346e-86, 2.28413306e-85,
       4.77289113e-84, 9.73282170e-83, 1.93683718e-81, 3.76135711e-80,
       7.12841752e-79, 1.31837451e-77, 2.37947734e-76, 4.19103663e-75,
       7.20374693e-74, 1.20834944e-72, 1.97798845e-71, 3.15974521e-70,
       4.92580805e-69, 7.49376401e-68, 1.11255028e-66, 1.61189367e-65,
       2.27903104e-64, 3.14456949e-63, 4.23417946e-62, 5.56383744e-61,
       7.13471636e-60, 8.92845058e-59, 1.09036673e-57, 1.29946981e-56,
       1.51132165e-55, 1.71531829e-54, 1.89989519e-53, 2.05358022e-52,
       2.16616140e-51, 2.22980589e-50, 2.23996076e-49, 2.19589147e-48,
       2.10076967e-47, 1.96129587e-46, 1.78691912e-45, 1.58878008e-44,
       1.37854131e-43, 1.16727420e-42, 9.64546346e-42, 7.77804443e-41,
       6.12089383e-40, 4.70063338e-39, 3.52285706e-38, 2.57650370e-37,
       1.83892335e-36, 1.28083619e-35, 8.70604008e-35, 5.77490495e-34,
       3.73823093e-33, 2.36148135e-32, 1.45579432e-31, 8.75815502e-31,
       5.14188481e-30, 2.94597574e-29, 1.64714979e-28, 8.98740128e-28,
       4.78555463e-27, 2.48672368e-26, 1.26101395e-25, 6.24035586e-25,
       3.01367164e-24, 1.42029845e-23, 6.53221396e-23, 2.93182672e-22,
       1.28414274e-21, 5.48890119e-21, 2.28957372e-20, 9.32010827e-20,
       3.70240898e-19, 1.43530742e-18, 5.43003438e-18, 2.00473667e-17,
       7.22286001e-17, 2.53955819e-16, 8.71373294e-16, 2.91774565e-15,
       9.53427606e-15, 3.04036078e-14, 9.46149206e-14, 2.87336795e-13,
       8.51569240e-13, 2.46289442e-12, 6.95134320e-12, 1.91464726e-11,
       5.14642830e-11, 1.34995778e-10, 3.45566434e-10, 8.63256833e-10,
       2.10448389e-09, 5.00666223e-09, 1.16237989e-08, 2.63357076e-08,
       5.82289558e-08, 1.25640634e-07, 2.64556444e-07, 5.43630328e-07,
       1.09014964e-06, 2.13336745e-06, 4.07419976e-06, 7.59304687e-06,
       1.38097860e-05, 2.45106555e-05, 4.24541364e-05, 7.17599624e-05,
       1.18369960e-04, 1.90545154e-04, 2.99330810e-04, 4.58883077e-04,
       6.86514588e-04, 1.00229279e-03, 1.42802743e-03, 1.98552610e-03,
       2.69408787e-03, 3.56734426e-03, 4.60972900e-03, 5.81303416e-03,
       7.15364628e-03, 8.59110757e-03, 1.00685738e-02, 1.15155283e-02,
       1.28527744e-02, 1.39993222e-02, 1.48803875e-02, 1.54354246e-02]), 7: array([7.81250000e-03, 7.71771228e-03, 7.44019375e-03, 6.99966110e-03,
       6.42638721e-03, 5.75776417e-03, 5.03428691e-03, 4.29555378e-03,
       3.57682314e-03, 2.90651708e-03, 2.30486450e-03, 1.78367213e-03,
       1.34704394e-03, 9.92763048e-04, 7.14013714e-04, 5.01146394e-04,
       3.43257294e-04, 2.29441538e-04, 1.49665405e-04, 9.52725768e-05,
       5.91849800e-05, 3.58799812e-05, 2.12270682e-05, 1.22553278e-05,
       6.90489302e-06, 3.79652344e-06, 2.03709988e-06, 1.06668372e-06,
       5.45074822e-07, 2.71815164e-07, 1.32278222e-07, 6.28203171e-08,
       2.91144779e-08, 1.31678538e-08, 5.81189947e-09, 2.50333112e-09,
       1.05224194e-09, 4.31628416e-10, 1.72783217e-10, 6.74978888e-11,
       2.57321415e-11, 9.57323631e-12, 3.47567160e-12, 1.23144721e-12,
       4.25784620e-13, 1.43668398e-13, 4.73074603e-14, 1.52018039e-14,
       4.76713803e-15, 1.45887282e-15, 4.35686647e-16, 1.26977909e-16,
       3.61143000e-17, 1.00236834e-17, 2.71501719e-18, 7.17653712e-19,
       1.85120449e-19, 4.66005414e-20, 1.14478687e-20, 2.74445105e-21,
       6.42073764e-22, 1.46603769e-22, 3.27241205e-23, 7.41351003e-24,
       3.01367164e-24, 7.41351003e-24, 3.27241205e-23, 1.46603769e-22,
       6.42073764e-22, 2.74445105e-21, 1.14478687e-20, 4.66005414e-20,
       1.85120449e-19, 7.17653712e-19, 2.71501719e-18, 1.00236834e-17,
       3.61143000e-17, 1.26977909e-16, 4.35686647e-16, 1.45887282e-15,
       4.76713803e-15, 1.52018039e-14, 4.73074603e-14, 1.43668398e-13,
       4.25784620e-13, 1.23144721e-12, 3.47567160e-12, 9.57323631e-12,
       2.57321415e-11, 6.74978888e-11, 1.72783217e-10, 4.31628416e-10,
       1.05224194e-09, 2.50333112e-09, 5.81189947e-09, 1.31678538e-08,
       2.91144779e-08, 6.28203171e-08, 1.32278222e-07, 2.71815164e-07,
       5.45074822e-07, 1.06668372e-06, 2.03709988e-06, 3.79652344e-06,
       6.90489302e-06, 1.22553278e-05, 2.12270682e-05, 3.58799812e-05,
       5.91849800e-05, 9.52725768e-05, 1.49665405e-04, 2.29441538e-04,
       3.43257294e-04, 5.01146394e-04, 7.14013714e-04, 9.92763048e-04,
       1.34704394e-03, 1.78367213e-03, 2.30486450e-03, 2.90651708e-03,
       3.57682314e-03, 4.29555378e-03, 5.03428691e-03, 5.75776417e-03,
       6.42638721e-03, 6.99966110e-03, 7.44019375e-03, 7.71771228e-03]), 'xi': 0.0, 'sigma': 0.000390625, 'j': 0})",3b5ed23a32199aa0c8e5949b91f27a2e08a1d4d0,32965c99762f3861fb14fd2771b183792dcd7484,"['kymatio/scattering2d/core/scattering2d.py', 'kymatio/scattering2d/filter_bank.py', 'kymatio/scattering2d/frontend/torch_frontend.py']"
137,kymatio/kymatio,https://github.com/kymatio/kymatio/issues/868,890,"Since we promised earlier:
  /home/jenkins/workspace/kymatio_dev/kymatio/frontend/entry.py:20: DeprecationWarning: Torch frontend is currently the default, but NumPy will become the default in the next version.
    warnings.warn(""Torch frontend is currently the default, but NumPy will become the default in the next""",c2a627f2b93decec4250c2411d04569faab3962f,5485ac108e3e5a230341e485d74f7779e6fc314c,"['kymatio/frontend/entry.py', 'tests/scattering1d/test_utils_scattering1d.py', 'tests/scattering2d/test_frontend_scattering2d.py', 'tests/scattering3d/test_torch_scattering3d.py']"
138,kymatio/kymatio,https://github.com/kymatio/kymatio/issues/418,534,"One should allow multiple dimension instead to hard code the maximum rank of a tensor. The software would be more flexible. I'll add a unit-test for this asap.

 1D
 2D
 3D",0b6a11a84c58d8cc7593ef52f0d54400d644bee4,5f6f7b212e1a22376aaf9021ba413dde8425b54a,"['kymatio/scattering3d/backend/tensorflow_backend.py', 'kymatio/scattering3d/frontend/tensorflow_frontend.py', 'kymatio/scattering3d/tests/test_tensorflow_scattering3d.py']"
139,kymatio/kymatio,https://github.com/kymatio/kymatio/issues/663,667,"So I keep getting intermittent test failures on GH Actions with the 2D FFT unit test:
=================================== FAILURES ===================================
____________________ TestFFT.test_fft[TensorFlowBackend2D] _____________________

self = <test_tensorflow_backend_2d.TestFFT object at 0x7f093716c9a0>
backend = <class 'kymatio.scattering2d.backend.tensorflow_backend.TensorFlowBackend2D'>

    @pytest.mark.parametrize('backend', backends)
    def test_fft(self, backend):
        x = np.random.randn(2, 2)
    
        y = np.array([[x[0, 0] + x[0, 1] + x[1, 0] + x[1, 1],
                       x[0, 0] - x[0, 1] + x[1, 0] - x[1, 1]],
                      [x[0, 0] + x[0, 1] - x[1, 0] - x[1, 1],
                       x[0, 0] - x[0, 1] - x[1, 0] + x[1, 1]]])
    
        z = backend.rfft(x)
>       assert np.allclose(y, z)
E       assert False
E        +  where False = <function allclose at 0x7f098b65b550>(array([[ 2.95330393e+00, -4.98718912e-01],\n       [-1.51491472e-03,  7.56337531e-01]]), <tf.Tensor: shape=(2, 2), dtype=complex64, numpy=\narray([[ 2.9533038e+00+0.j, -4.9871886e-01+0.j],\n       [-1.5148520e-03+0.j,  7.5633746e-01+0.j]], dtype=complex64)>)
E        +    where <function allclose at 0x7f098b65b550> = np.allclose

tests/scattering2d/test_tensorflow_backend_2d.py:141: AssertionError

It doesn't reproduce locally. Likely this has to do with the cheap FFT implementation that they use (although it's surprising that it fails for such small arrays). The thing to do might be to relax the precision a bit.",b9cab1a80849bbd89839c58b22121c0669c2e256,f6caccc061261328ee15f2f3543ce8aa15e827e5,['tests/scattering2d/test_tensorflow_backend_2d.py']
